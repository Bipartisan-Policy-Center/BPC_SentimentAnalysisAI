
AI4People's Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations 
ABSTRACT 
This article reports the findings of AI4People, an Atomium –EISMD initiative designedtolay the foundations for a “Good AI Society”. We introduce thecore opportunities and risks of AI for society; present asynthesis offive ethicalprinciples that shouldundergird itsdevelopment andadoption; and offer 20 concrete recommendations –to assess, to develop, to incentivise, and to support good AI –which in some cases may be undertaken directly by national orsupranational policy makers, while inothersmay beledby otherstakeholders. If adopted, these recommendations would serve asa firm foundation for the establishment of aGood AI Society. 


TABLE OF CONTENTS 
Executive Summary ............................................................................................................................................. 4 
1. 
Introduction ....................................................................................................................................................... 5 

2. 
The Opportunities and Risks of AI for Society ........................................................................................ 6 


2.1 Who we can become: enabling human self-realisation, without devaluing human abilities ................ 7 
2.2 What we can do: enhancing human agency, without removing human responsibility .......................... 8 
2.3 What we can achieve: increasing societal capabilities, without reducing human control ..................... 9 
2.4How we caninteract: cultivating societal cohesion, without eroding human self-determination ......9 
3. The Dual Advantage of an Ethical Approach to AI ................................................................................. 11 
4.A Unified Framework of Principles for AI in Society ...........................................................................12 
4.1Beneficence: promoting well-being, preserving dignity, and sustaining the planet ...............................13 
4.2Non-maleficence: privacy, security and “capability caution” ..........................................................................14 
4.3Autonomy: the power to decide (whether to decide) ..................................................................................... 15 
4.4Justice: promoting prosperity and preserving solidarity ..................................................................................16 
4.5Explicability: enabling the other principles through intelligibility and accountability ..........................17 
5. Recommendations for a Good AI Society ................................................................................................ 19 
5.1Preamble .......................................................................................................................................................................... 19 
5.2Action Points .................................................................................................................................................................. 20 
5.2.1Assessment .................................................................................................................................................................. 19 
5.2.2Development ............................................................................................................................................................... 20 
5.2.3Incentivisation ............................................................................................................................................................ 23 
5.2.4Support ......................................................................................................................................................................... 24 
References ............................................................................................................................................................ 27 
Authors 
Luciano Floridi1,2, Josh Cowls1,2, Monica Beltrametti3, Raja Chatila4,5, Patrice Chazerand6, Virginia Dignum7, 8, Christoph Luetge9, Robert Madelin10, Ugo Pagallo11, Francesca Rossi12,13, Burkhard Schafer14, Peggy Valcke15,16, and Effy Vayena17. 
1Oxford Internet Institute, University of Oxford, Oxford, United Kingdom. 2The Alan Turing Institute, London, United Kingdom. 3Naver Corporation, Grenoble, France. 4French National Center of Scientific Research, France. 5Institute of Intelligent Systems and Robotics at Pierre and Marie Curie University, Paris, France. 6Digital Europe, Brussels, Belgium. 7University of Umeå, Umeå, Sweden. 8Delft Design for Values Institute, Delft University of Technology, Delft, the Netherlands. 9TUM School of Governance, Technical University of Munich, Munich, Germany. 10Centre for Technology and Global Affairs, University of Oxford, Oxford, United Kingdom. 11Department of Law, University of Turin, Turin, Italy. 12IBM Research, United States. 13University of Padova, Padova, Italy. 14University of Edinburgh Law School, Edinburgh, United Kingdom. 15Centre for IT & IP Law, Catholic University of Leuven, Flanders, Belgium. 16Bocconi University, Milan, Italy. 17Bioethics, Health Ethics and Policy Lab, ETH Zurich, Zurich, Switzerland. 



EXECUTIVE SUMMARY 
This White Paper reports the findings of AI4People, anAtomium –EISMD initiative designedtolay thefoundations for a“GoodAISociety”through thecreation ofan ethicalframework. Thisdocumentwas produced by theScientificCommitteeof AI4People. 
The opportunities and risks of AI for Society 
Establishinganethicalframework for AIinsocietyrequires anexplanation ofthe opportunities and risks that the design and useof the technology presents. We identify four ways in which, at a high level, AI technology may have a positive impact on society, ifitisdesignedandusedappropriately. Eachofthesefour opportunitieshasa corresponding risk, which may result from itsoveruse ormisuse.There isalsoan overarching risk that AI might be underused, relative toits potential positive impact, creating an opportunity cost. An ethical framework for AI must be designed to maximise these opportunities and minimise the related risks. 
A unified framework of principles for AI 
Several multistakeholder groups have created statementsofethicalprinciples which should guide the development and adoption of AI. Rather than repeat the sameprocess here, we insteadpresent acomparative analysis ofseveral ofthesesetsofprinciples. Each principle expressed in each of the documents we analyse is encapsulated by oneof five overarching principles. Four of these –beneficence, nonmaleficence, autonomy, and justice–are established principles of medical ethics, but afifth–explicability –is also required, to capture the novel ethical challenges posed by AI. 

Twenty recommendations for a Good AI Society 
We offer 20concrete recommendations tailored totheEuropean contextwhich, if adopted,would facilitatethedevelopment andadoptionofAIthatmaximisesits opportunities,minimisesitsrisks,andrespects thecore ethicalprinciples identified. Eachrecommendation takes oneof four forms: to assess,todevelop,toincentivise,orto support good AI. These recommendations may in somecasesbe undertaken directly by national or supranational policy makers, and in others may be led by other stakeholders. Taken together withtheopportunities,risksand ethicalprinciples we identify, the recommendations constitute the final element of anethical framework for agood AI society. 

1 



INTRODUCTION 
AI is notanother utility that needs toberegulated onceit is mature. It isapowerful force, a new form of smart agency, which is already reshaping our lives, our interactions, and our environments. AI4People was setuptohelpsteerthispowerful force towards thegoodofsociety, everyone in it, and the environments we share. This White Paper is the outcome of the collaborative effort by theAI4People Scientific Committee—comprising 12 experts and chaired by Luciano Floridi1—to propose a series of recommendations for the development of a Good AI Society. 
The White Paper synthesises three things: the opportunities and associated risks that AI technologies offer for fostering human dignity and promoting human flourishing; the principles that should undergird theadoption of AI; and twenty specific recommendations that, if adopted, will enable all stakeholders to seize the opportunities, to avoid or at least minimise and counterbalance the risks, to respect the principles, and hence to develop a Good AI Society. 
The White Paper is structured around four more sections after this introduction. Section2statesthecore opportunitiesfor promoting humandignityandhuman flourishing offered by AI, together with their corresponding risks.2Section 3 offers a brief, high-level view of the advantages for organisations of taking anethical approach tothedevelopment anduseofAI.Section4formulates 5ethicalprinciples for AI, buildingonexistinganalyses, which shouldundergird theethicaladoptionofAIin societyatlarge.Finally, Section5offers 20recommendations for thepurposeof developing a Good AI Society in Europe. 
Since thelaunchofAI4People inFebruary 2018, theScientificCommitteehas acted collaboratively to develop the recommendations in the final section of this paper. Through this work, we hope to have contributed to the foundation of a Good AI Society we can all share. 
1 Besides LucianoFloridi, the members of the Scientific Committee are: Monica Beltrametti, Raja Chatila, Patrice Chazerand, Virginia Dignum, Christoph Luetge, Robert Madelin, Ugo Pagallo, Francesca Rossi, Burkhard Schafer, Peggy Valcke, and Effy Vayena. Josh Cowls is the rapporteur. Thomas Burri contributed to an earlier draft.2 The analysis in this and the following two sections is also available in Cowls and Floridi (2018). Further analysis andmore information on the methodology employed will be presented in Cowls and Floridi (Forthcoming). 

2 

THE OPPORTUNITIES AND RISKS OF AI FOR SOCIETY 
That AI will have amajor impact onsociety is nolonger in question. Current debate turnsinsteadonhow far this impact will be positive ornegative, for whom, in which ways, in which places, and onwhat timescale. Put another way, we cansafely dispense with the question of whether AI will have animpact; the pertinent questions now are by whom,how,where, andwhen this positive or negative impact will be felt. 
Inorder toframethesequestionsinamore substantive andpracticalway, we introduce here what we consider the four chief opportunities for society that AI offers. Theyare four because they address the four fundamental points in the understanding of human dignity and flourishing: who we can become (autonomous self-realisation); what we can do (human agency); what we can achieve (individual and societal capabilities); and how we can interact with each other and the world (societal cohesion). In each case, AI can beused tofoster humannature anditspotentialities,thuscreating opportunities; underused, thus creating opportunity costs; or overused andmisused, thus creating risks. As theterminology indicates, the assumption is that the use of AI is synonymous with good innovation and positive applications of this technology. However, fear, ignorance, misplaced concernsorexcessive reaction may lead asociety to underuse AI technologies below their full potential, for what might be broadly described asthewrong reasons. This may cause significant opportunity costs. It might include, for example, heavy-handed ormisconceived regulation, underinvestment, orapublic backlash akin to that faced by genetically modified crops (Imperial College, 2017). As aresult, the benefits offered by AI technologies may not be fully realised by society. 
These dangers arise largely from unintended consequences and relate typically to good intentions goneawry. However, we mustalso consider the risks associated with inadvertent overuse orwilfulmisuse ofAItechnologies,grounded, for example, in misalignedincentives, greed, adversarial geopolitics,ormaliciousintent.Everything from emailscamstofull-scale cyber-warfare may be accelerated orintensified by the malicioususeof AI technologies (Taddeo, 2017). And newevilsmay be made possible (King et. al, 2018). The possibility of social progress represented by the aforementioned opportunities above must beweighed against the risk that malicious manipulation will be enabled orenhanced by AI. Yet abroad risk is that AI may be underused out of fear ofoveruse ormisuse. We summarise these risks in Figure A below, and offer amore detailed explanation in the text that follows. 


opportunity cost of underusing AI. 
2.1Who we canbecome:enabling humanself-realisation, without devaluing human abilities 
AImay enable self-realisation, by which we meanthe ability for people toflourish in terms of their own characteristics, interests, potential abilities orskills, aspirations, and life projects. Much asinventions, suchasthewashing machine, liberatedpeople – particularly women –from the drudgery of domestic work, the “smart” automation of other mundane aspects of life may free up yet more time for cultural, intellectual and social pursuits, andmore interesting and rewarding work. More AImay easily mean more human life spentmore intelligently. The risk in this caseis not the obsolescence ofsome old skills and the emergence of newones per se, but the pace at which this is happeningand the unequal distributions of the costs and benefits that result. A very fast devaluation of old skills and hence aquick disruption of the job market and the nature of employment canbeseenat the level of both the individual and society. At the level of theindividual, jobs are oftenintimately linked topersonal identity, self-esteem, and socialrole orstanding, all factors that may be adversely affected by redundancy, even putting to oneside the potential for severe economic harm. Furthermore, at the level of society, the deskilling in sensitive, skill-intensive domains, such as health care diagnosis oraviation, may create dangerous vulnerabilities in the event of AI malfunction oran adversarial attack. Fostering the development of AI in support of new abilities and skills, while anticipating and mitigating its impact onoldoneswillrequire both close study andpotentially radicalideas,suchastheproposal for someform of“universal basic income”, which is growing in popularity and experimental use. In the end, we need some intergenerational solidarity between thosedisadvantaged today andthoseadvantaged tomorrow, toensure thatthe disruptive transitionbetween the present and the future will be as fair as possible, for everyone. 


2.2What we cando:enhancinghumanagency, withoutremoving human responsibility 
AIisproviding agrowing reservoir of“smart agency”. Putattheserviceofhuman intelligence, such aresource can hugely enhance human agency. We can do more, better, and faster, thanks to the support provided by AI. In this sense of “Augmented Intelligence”, AI could be compared to the impact that engines have hadonourlives. The larger the number of people who will enjoy the opportunities and benefits of such areservoir of smart agency “on tap”, thebetter our societies will be. Responsibility is therefore essential, inviewofwhat sortofAIwe develop, how we useit,andwhether we share with everyone its advantages and benefits. Obviously, the corresponding risk is the absence of suchresponsibility. Thismay happen not just because we have thewrong socio-political framework, but also because of a“black box” mentality, according to which AI systems for decision-making are seenasbeing beyond human understanding, and hence control. These concerns apply not only to high-profile cases, such as deaths caused by autonomous vehicles, but also tomore commonplace but still significant uses, such asin automated decisions about parole or creditworthiness. 
Yet therelationship between the degree and quality of agency that people enjoy and how much agency we delegate to autonomous systems is not zero-sum, either pragmatically or ethically. 
Infact,ifdeveloped thoughtfully, AIoffers theopportunityofimproving and multiplying thepossibilitiesfor humanagency. Considerexamples of“distributed morality”inhuman-to-human systems suchaspeer-to-peer lending (Floridi, 2013). Human agencymay be ultimately supported, refined and expanded by the embedding of “facilitating frameworks”, designed to improve the likelihood of morally good outcomes, inthesetoffunctionsthatwe delegatetoAIsystems. AIsystems could,ifdesigned effectively, amplify and strengthen shared moral systems. 

2.3What we canachieve: increasing societal capabilities, without 
reducing human control 
Artificialintelligence offers myriad opportunitiesfor improving andaugmentingthe capabilities of individuals and society at large. Whether by preventing and curing diseases oroptimising transportation and logistics, the useof AI technologies presents countless possibilities for reinventing society by radically enhancing what humans are collectively capable of. More AImay support better coordination, and hence more ambitious goals. Human intelligence augmented by AI could find new solutions to old and new problems, from afairer ormore efficient distribution of resources toamore sustainable approach to consumption. Precisely because such technologies have the potential to be so powerful and disruptive, they also introduce proportionate risks. 
Increasingly, we may notneedtobe either ‘in oronthe loop’ (that is, aspart of the process orat least in control of it), if we candelegateourtasks to AI. However, if we rely ontheuseof AI technologies toaugmentourown abilities in the wrong way, we may delegate important tasks and above all decisions toautonomoussystems that shouldremain at least partly subject to human supervision and choice. This in turn may reduce ourability to monitor the performance of these systems (by nolonger being ‘on the loop’ either) orpreventing orredressing errors orharms that arise (‘post loop’). It is also possible that these potential harms may accumulate and become entrenched, as more andmore functionsare delegated to artificial systems. It is therefore imperative to strike abalance between pursuing the ambitious opportunities offered by AI to improve human life and what we canachieve, ontheonehand, and, onthe other hand, ensuring that we remain in control of these major developments and their effects. 


2.4How we can interact: cultivating societal cohesion, without eroding human self-determination 
From climatechangeandantimicrobial resistance tonuclearproliferation and fundamentalism,globalproblems increasingly have highdegrees ofcoordination complexity, meaning that they canbe tackled successfully only if all stakeholders co-design and co-own the solutions and cooperate to bring them about. AI, with its data-intensive, algorithmic-driven solutions,canhugely help to deal with such coordination complexity, supporting more societal cohesion andcollaboration. For example, efforts to tackle climate change have exposed the challenge of creating acohesive response, both within societies and between them. The scale of this challenge is such that we may soon need to decide between engineering the climate directly and designing societal frameworks to encourage a drastic cut in harmful emissions. This latter option might be undergirded by analgorithmic system tocultivate societalcohesion.Such asystem would notbe imposed from the outside; it would be the result of a self-imposed choice, not unlike our choice of notbuying chocolate if we had earlier chosen to be onadiet,orsetting up an alarm clock to wake up. “Selfnudging” to behave in socially preferable ways is the best form of nudging, and theonly onethat preserves autonomy. It is the outcome of human decisions and choices, but it canrely onAI solutions to be implemented and facilitated. Yet the risk is that AI systems may erode human selfdetermination, astheymay lead to unplanned and unwelcome changes in human behaviours to accommodate the routines that make automation work and people’s lives easier. AI’s predictive power and relentless nudging,even if unintentional, should be at the service of human selfdetermination and foster societal cohesion, not undermining of human dignity or human flourishing. 

Taken together, these four opportunities, and theircorresponding challenges, paint amixed picture about the impact of AI onsociety and the people in it. Accepting the presence of trade-offs, seizing the opportunities while working toanticipate,avoid, or minimise the risks head-on will improve the prospect for AI technologies topromote human dignity and flourishing. Having outlined the potential benefits to individuals and society at large of anethically engaged approach to AI, in the next section we highlight the “dual advantage” to organisations of taking such an approach. 

3 


THE DUAL ADVANTAGE OF AN ETHICAL APPROACH TO AI 
Ensuringsocially preferable outcomesofAIrelies onresolving thetensionbetween incorporating the benefits and mitigating the potential harms of AI, in short, simultaneously avoiding the misuse and underuse of these technologies. In this context, the value of an ethical approach to AI technologies comesinto starker relief. Compliance with the law is merely necessary (the leas that is required), but significantly insufficient (not the most thancanbe done) (Floridi, 2018). With ananalogy, it is the difference between playing according tothe rules, and playing well, sothatonemay win the game. Adopting an ethical approach to AI confers what we define here asa“dual advantage”. Ononeside, ethics enables organisations to take advantage of the social value that AI enables. This is the advantage of being able to identify and leverage newopportunities that are socially acceptable or preferable. On the other side, ethics enables organisations to anticipate and avoid oratleastminimise costly mistakes. Thisistheadvantage ofprevention and mitigationofcoursesofactionthatturnouttobesocially unacceptable andhence rejected, even when legally unquestionable. This also lowers theopportunitycostsof choices not made or options not grabbed for fear of mistakes. 
Ethics’ dual advantage canonly function in anenvironment of public trustand clearresponsibilities more broadly. Public acceptance and adoption of AI technologies will occur only if the benefits are seen as meaningful and risks as potential, yet preventable, minimisable,oratleastsomething against which onecanbeprotected, through risk management (e.g. insurance) orredressing. Theseattitudeswilldependinturnon public engagement with the development of AI technologies, openness about how they operate, and understandable, widely accessible mechanisms of regulation and redress. In thisway, anethical approach to AI canalso be seenasanearly warning system against riskswhich might endanger entire organisations. The clear value to any organisation of the dual advantage of an ethical approach to AI amply justifies the expense of engagement, openness, and contestability that such an approach requires. 

4 

A UNIFIED FRAMEWORK OF PRINCIPLES FOR AI IN SOCIETY 
AI4People isnotthefirstinitiative toconsidertheethicalimplications ofAI.Many organisations have already produced statementsof the values orprinciples that should guide the development and deployment of AI in society. Rather than conduct asimilar, potentially redundant exercise here, we strive to move the dialogue forward, constructively, from principles to proposed policies, best practices, and concrete recommendations for newstrategies.Such recommendations are notoffered inavacuum. Butratherthan generating yet anotherseriesofprinciples toserve asanethicalfoundation for our recommendations, we offer a synthesis of existing sets of principles produced by various reputable, multi-stakeholder organisationsandinitiatives. Afullerexplanation ofthe scope, selection and method of assessing these sets of principles is available in Cowls and Floridi (Forthcoming). Here, we focus on the commonalities and noteworthy differences observable across these sets of principles, in view of the 20 recommendations offered in the rest of the paper. The documents we assessed are: 
1. 
the Asilomar AI Principles, developed under the auspices of the Future of Life Institute, in collaboration with attendees of the high-level Asilomar conference of January 2017 (hereafter “Asilomar”; Asilomar AI Principles, 2017); 

2. 
the Montreal Declaration for Responsible AI, developed under the auspices of the University of Montreal, following the Forum onthe Socially Responsible Development ofAIofNovember 2017(hereafter “Montreal”; Montreal Declaration, 2017);3 

3. 
the General Principles offered in the second version of Ethically Aligned Design: A Vision for Prioritizing Human Well-being with Autonomous and Intelligent Systems. Thiscrowd-sourced globaltreatise received contributionsfrom 250global thoughtleaderstodevelop principles andrecommendations for theethical development anddesignofautonomousandintelligentsystems, andwas published in December 2017 (hereafter “IEEE”; IEEE, 2017);4 

4. 
the Ethical Principles offered in the Statement onArtificial Intelligence, Robotics and ‘Autonomous’ Systems,published by the European Commission’s European Group onEthics in Science and New Technologies, in March 2018 (hereafter “EGE”; EGE, 2018); 


3 The Montreal Declaration is currently open for comments as part of a redrafting exercise. The principles we refer tohere are those which were publicly announced as of 1st May, 2018.4 The third version of Ethically Aligned Design will be released in 2019 following wider public consultation. 

5. 
the “five overarching principles for an AI code” offered in paragraph 417 of the UK House of Lords Artificial Intelligence Committee’s report, AI in the UK: ready, willing and able?, published in April 2018 (hereafter “AIUK”; House of Lords, 2018); and 

6. 
the Tenets of the Partnership on AI, amultistakeholder organisation consisting of academics, researchers, civil society organisations, companies building and utilisingAItechnology, andothergroups (hereafter “thePartnership”; Partnership on AI, 2018). 


Taken together, they yield 47principles.5 Overall, we findanimpressive and reassuring degree of coherence andoverlap between the six sets of principles. This can mostclearly beshown by comparing thesetsofprinciples withthesetoffour core principles commonly usedinbioethics:beneficence,non-maleficence, autonomy, and justice. The comparison should not be surprising. 
Ofallareas ofapplied ethics,bioethicsistheonethatmostclosely resembles digital ethics in dealing ecologically with new forms of agents, patients, and environments (Floridi, 2013). The four bioethical principles adapt surprisingly well to the fresh ethical challenges posed by artificial intelligence. But they are notexhaustive. On the basis of the following comparative analysis, we argue that onemore, newprinciple is needed in addition: explicability, understood as incorporatingboth intelligibility and accountability. 
4.1Beneficence: promoting well-being, preserving dignity, andsustaining 
the planet 
Of the four core bioethics principles, beneficence is perhaps the easiest to observe across the six sets of principles we synthesise here. The principle of creating AI technology that is beneficial tohumanity is expressed in different ways, but it typically features at the top of each list of principles. 
Montreal and IEEE principles both usetheterm“well-being”: for Montreal, “the development of AI should ultimately promote thewell-being of all sentient creatures”; while IEEE states the need to “prioritize human well-being asanoutcome in all system designs”. AIUK and Asilomar both characterise this principle asthe“common good”: AI 
5 Of the six documents, the Asilomar Principles offer the largest number of principles with arguably the broadest scope. The 23principles are organisedunderthree headings,“research issues”, “ethics andvalues”, and“longer-term issues”. We have omitted consideration of the five “research issues” here astheyare related specifically to the practicalities of AI development, particularly in the narrower contextof academia and industry. Similarly, the Partnership’s eight Tenets consist of both intra­organisational objectives and wider principles for the development and useof AI. We include only the wider principles (the first, sixth, and seventh tenets). 

should “be developed for thecommongood and the benefit of humanity”, according to AIUK. The Partnership describes the intention to“ensure that AI technologies benefit and empower asmany people aspossible”;while the EGE emphasises the principle of both“humandignity”and “sustainability”. Its principle of“sustainability” represents perhapsthewidestofallinterpretations ofbeneficence, arguing that“AI technology must be in line with …ensur[ing] the basic preconditions for life on our planet, continued prospering for mankindandthepreservation ofagoodenvironment for future generations”. Taken together, the prominence of these principles of beneficence firmly underlines the central importance of promoting the well-being of people and the planet. 
4.2 Non-maleficence: privacy, security and “capability caution” 
Though“do only good” (beneficence) and “do noharm” (non-maleficence) seem logically equivalent, inboth the context of bioethics and of the ethics of AI they represent distinct principles, each requiring explication. While they encourage well-being, the sharing of benefits and the advancement of the public good, each of the six sets of principles also cautions against the many potentially negative consequences ofoverusing ormisusing AI technologies. Of particular concernis the prevention of infringements onpersonal privacy, which islistedasaprinciple infive of the six sets, and aspartofthe“humanrights” principles inthe IEEE document. Ineach case, privacy is characterised asbeing intimately linked to individuals’ access to, and control over, how personal data is used. 
Yet theinfringement of privacy is not the only danger to be avoided in the adoption of AI. Several of the documents also emphasise the importance of avoiding the misuse of AI technologies in other ways. The Asilomar Principles are quite specific on this point, citing the threats of an AI arms race and of the recursive self-improvement of AI, as well as the need for “caution” around “upper limits on future AI capabilities”. The Partnership similarly assertsthe importance of AI operating “within secure constraints”. The IEEE documentmeanwhile cites the need to“avoid misuse”, while the Montreal Declaration argues that those developing AI “should assumetheirresponsibility by working against the risks arising from their technological innovations”, echoed by the EGE’s similar need for responsibility. 
From thesevarious warnings, itisnotentirely clearwhether itisthepeople developing AI, or the technology itself, which should be encouraged not to do harm –in otherwords,whether it is Frankenstein orhismonsteragainstwhose maleficence we should be guarding. Confused also is the question of intent: promoting non-maleficence canbeseentoincorporate theprevention ofbothaccidental(what we above call “overuse”) and deliberate (what we call “misuse”) harms arising. In terms of the principle of non-maleficence, this need not be an either/or question: the point is simply to prevent harmsarising,whether from theintentofhumansortheunpredicted behaviour of machines(including theunintentionalnudgingofhumanbehaviour inundesirable ways). Yet theseunderlying questions ofagency, intentandcontrol becomeknottier when we consider the next principle. 


4.3 Autonomy: the power to decide (whether to decide) 
Anotherclassictenetofbioethicsistheprinciple ofautonomy: theideathat individuals have a right to make decisions for themselves about the treatment they do or notreceive. Inamedicalcontext,thisprinciple ofautonomy ismostoftenimpaired when patients lack the mental capacity tomake decisions in their own best interests; autonomy is thus surrendered involuntarily. With AI, the situation becomes rather more complex: when we adopt AI and its smart agency, we willingly cede some of our decision-making power to machines. Thus, affirming the principle of autonomy in the context of AImeans striking a balance between the decision-making power we retain for ourselves and that which we delegate to artificial agents. 
The principle of autonomy is explicitly stated in four of the six documents. The Montreal Declaration articulates the need for abalance between human- and machine-led decision making, stating that “the development of AI should promote the autonomy of all human beings and control… the autonomy of computer systems” (italics added). The EGE argues that autonomoussystems “mustnotimpair [the] freedom of human beingstosettheirown standards and normsand be able tolive according tothem”, while AIUK adopts the narrower stance that “the autonomous power to hurt, destroy or deceive human beings should never bevested in AI”. The Asilomar document similarly supports the principle of autonomy, insofar as “humans should choose how and whether to delegate decisions to AI systems, to accomplish human-chosen objectives”. 
These documents expressa similar sentiment in slightly different ways, echoing the distinction drawn above between beneficence andnon-maleficence: not only should the autonomy ofhumansbepromoted, butalsotheautonomy ofmachinesshouldbe restricted and madeintrinsically reversible, shouldhumanautonomy needtobere­established (consider the caseofapilot able to turn off the automatic pilot and regain full control of the airplane). Taken together, the central point is to protect the intrinsic value of human choice –at least for significant decisions –and, as a corollary, to contain the risk of delegating too much to machines. Therefore, what seems most important here is what we might call “meta-autonomy”, or a “decide-to-delegate” model: humans should always retain the power todecidewhich decisions totake, exercising the freedom to choose where necessary, and ceding it in cases where overriding reasons, such as efficacy, may outweigh the loss of control over decision-making. As anticipated, any delegation should remain overridable in principle (deciding to decide again). 

The decision to make ordelegate decisions does not take place in avacuum. Nor isthiscapacitytodecide(to decide,andtodecideagain) distributedequally across society. The consequences of this potential disparity in autonomy are addressed in the final of the four principles inspired by bioethics. 


4.4 Justice: promoting prosperity and preserving solidarity 
The last of the four classic bioethics principles is justice, which is typically invoked in relation to the distribution of resources, such as new and experimental treatment options orsimply thegeneralavailability ofconventional healthcare. Again,thisbioethics principle finds clear echoes across the principles for AI that we analyse. The importance of“justice” isexplicitly citedintheMontreal Declaration,which arguesthat“the development of AI should promote justice and seek to eliminate all types of discrimination”, while the Asilomar Principles include the need for both“shared benefit” and “shared prosperity” from AI. Under its principle named “Justice, equity and solidarity”, the EGE argues that AI should “contribute to global justice and equal access to the benefits” of AI technologies. It also warns against the risk of bias in datasets used to train AI systems, and–uniqueamong thedocuments –argues for the need to defend against threats to “solidarity”, including“systems ofmutualassistancesuchasinsocialinsuranceand healthcare”. 
The emphasis onthe protection of social support systems may reflect geopolitics, insofarasthe EGE is aEuropean body. The AIUK report argues that citizens should be able to “flourish mentally, emotionally and economically alongside artificial intelligence”. The Partnership, meanwhile, adopts amore cautious framing, pledging to“respect the interests of all parties that may be impacted by AI advances”. Aswiththeotherprinciples already discussed, theseinterpretations ofwhat justice meansasanethical principle in the context of AI are broadly similar, yet contain subtle distinctions. Across the documents, justice variously relates to 
a)
 using AI to correct past wrongs such as eliminating unfair discrimination; 

b) 
ensuring that the use of AI creates benefits that are shared (or at least shareable); and 

c) 
preventing the creationof new harms, such as the undermining of existing social structures. 



Notable alsoare thedifferent ways inwhich thepositionofAI,vis-à-vis people, is characterised in relation to justice. In Asilomar andEGE respectively, it is AI technologies themselves that “should benefit and empower as many people as possible” and “contribute to global justice”, whereas in Montreal, it is “the development of AI” that “should promote justice” (italics added). In AIUK, meanwhile, people should flourish merely “alongside” AI.Ourpurposehere isnottosplit semantichairs.Thediverse ways inwhich the relationship between people and AI is described in these documents hints atbroader confusion over AI as a man-made reservoir of “smart agency”. 
Putsimply, andtoresume ourbioethicsanalogy, are we (humans) thepatient, receiving the “treatment”of AI, the doctor prescribing it? Or both? It seemsthatwe mustresolve this question before seekingtoanswer thenextquestion of whether the treatment will even work. This is the core justification for our identification within these documents of a new principle, one that is not drawn from bioethics. 
4.5Explicability: enabling theotherprinciples through intelligibilityand accountability 
The short answer to the question of whether “we” are the patient orthe doctor is that actually we could be either –dependingonthe circumstances and onwho “we” are in oureveryday life. The situation is inherently unequal: asmall fraction of humanity is currently engaged in the design and development of a set of technologies that are already transforming the everyday lives of just about everyone else. This stark reality is not lost onthe authors whose documents we analyse. In all, reference is made tothe need to understand andhold to account the decision-making processes of AI. 
Thisprinciple isexpressed usingdifferent terms:“transparency” inAsilomar; “accountability” in EGE; both “transparency” and “accountability” in IEEE; “intelligibility” in AIUK; and as “understandable and interpretable” for the Partnership. Though described in different ways, each of these principles captures something seemingly novel about AI: that its workings are often invisible orunintelligible to all but (at best) the most expert observers. 

Theadditionofthisprinciple, which we synthesiseas“explicability” bothinthe epistemologicalsenseof“intelligibility” (as ananswer tothequestion“how doesit work?”) andin the ethical senseof“accountability” (as ananswer to the question: “who is responsible for the way it works?”), is therefore the crucial missing piece of the jigsaw when we seek to apply the framework of bioethics to the ethics of AI. It complements the other four principles: for AI to be beneficent and non-maleficent, we must be able to understand the good orharm it is actually doing to society, and in which ways; for AI to promote and not constrain human autonomy, our “decision about who should decide” must be informed by knowledge of how AI would act instead of us; and for AI to be just, we mustensure that the technology –or, more accurately, the people and organisations developing and deploying it –are held accountable in the event of anegative outcome, which would require inturnsomeunderstanding ofwhy thisoutcomearose. More broadly, we mustnegotiate thetermsoftherelationship between ourselves andthis transformative technology, ongrounds that are readily understandable to the proverbial person “on the street”. 
Taken together, we argue that these five principles capture the meaning of each of the 47 principles contained in the six high-profile, expert-driven documents, forming an ethical framework withinwhich we offer ourrecommendations below. This framework of principles is shown in Figure B. 


5 



RECOMMENDATIONS FOR A GOOD AI SOCIETY 
This section introduces the Recommendations for aGood AI Society. It consists of two parts:aPreamble, and 20 Action Points. There are four kinds of Action Points: to assess, to develop,toincentivise andtosupport.Somerecommendations may beundertaken directly, by nationalorEuropean policymakers, incollaborationwithstakeholders where appropriate. For others, policy makers may play anenablingrole for efforts undertaken or led by third parties. 
5.1 Preamble 
We believe that, in order to create aGood AI Society, the ethical principles identified in the previous section should be embedded in the default practices of AI. In particular, AI should be designed and developed in ways that decrease inequality and further social empowerment, with respect for human autonomy, and increase benefits that are shared by all, equitably. It isespecially important thatAIbeexplicable, asexplicability isa critical tool to build public trust in, and understanding of, the technology. 
We also believe that creating a Good AI Society requires a multistakeholder approach, which is the most effective way to ensure that AI will serve the needs of society, by enabling developers, users and rule-makers to all be on board and collaborating from the outset. 
Different cultural frameworks inform attitudes to newtechnology. This document represents aEuropean approach, which ismeanttobecomplementary toother approaches. We are committed to the development of AI technology in a way that secures people’s trust,serves the public interest, andstrengthens shared social responsibility. 
Finally, thissetofrecommendations should be seenasa“living document”. The Action Points are designed to be dynamic, requiring not simply single policies or one-off investments, but rather, continuous, ongoing efforts for their effects to be sustained. 

5.2 


ACTION POINTS 
5.2.1 Assessment 
1. 
Assess the capacity of existing institutions, such asnational civil courts, toredress themistakes madeorharmsinflicted by AIsystems.This assessmentshouldevaluate thepresence ofsustainable,majority-agreed foundations for liabilityfrom thedesign stage onwards inorder toreduce negligence and conflicts (see also Recommendation 5).6 

2. 
Assess which tasksanddecision-making functionalitiesshould not be delegated to AI systems, through the use of participatory mechanisms to ensure alignment withsocietalvalues and understanding ofpublicopinion. This assessment shouldtake intoaccount existing legislationand besupportedby ongoing dialogue between all stakeholders (including government, industry, and civil society) todebate how AI will impact society opinion(inconcert with Recommendation 17). 

3. 
Assess whether current regulations are sufficiently grounded in ethics to provide alegislative framework that cankeep pace with technological 


developments.Thismay include aframework of key principles that would be applicable to urgent and/or unanticipated problems. 

5.2.2 Development 
4. 
Develop aframework toenhance the explicability of AI systems which make socially significant decisions.Central to this framework is the ability for individuals to obtain a factual, direct, and clear explanation of the decision-makingprocess, especially intheevent ofunwanted consequences.Thisis likely to require the development of frameworks specific to different industries, andprofessional associationsshouldbeinvolved inthisprocess, alongside experts in science, business, law, and ethics. 

5. 
Develop appropriate legal procedures and improve the IT infrastructure of the justice system to permit the scrutiny of algorithmic decisions in court.This is likely to include the creation of a framework for AI explainability asindicated in Recommendation 4,specific to the legal system. Examples of appropriate procedures may includetheapplicable disclosure ofsensitive commercial information inIPlitigation,and–where disclosure poses 


6 Determining accountability and responsibility may usefully borrow from lawyers in Ancient Rome who would go bythis formula ‘cuius commoda eius et incommoda’ (‘the person who derives an advantage from a situation must alsobear the inconvenience’). A good 2,200 years old principle that has a well-established tradition and elaboration couldproperly set the starting level of abstraction in this field. 
commercial information inIPlitigation,and–where disclosure poses unacceptable risks, for instanceto national security –the configuration of AI systems to adopt technical solutions by default, such aszeroknowledge proofs in order to evaluate their trustworthiness. 
6. Develop auditingmechanismsfor AIsystemstoidentify unwanted consequences, such asunfair bias, and (for instance, in cooperation with the insurance sector) asolidarity mechanism todealwith severe risksin AI-intensive sectors.Those risks could be mitigated by multistakeholder mechanisms upstream. Pre-digital experience indicates that, insome cases, it may take a couple of decades before society catches up with technology by way of rebalancing rights and protectionadequately to restore trust. The earlier that users and governments become involved – as made possible by ICT – the shorter this lag will be. 
7. Develop a redressprocessor mechanism toremedy or compensate for a wrong or grievance caused by AI.To foster publictrustinAI, society needs awidely accessibleand reliable mechanism of redress for harms inflicted, costs incurred, or other grievances caused by the technology. Such a mechanism will necessarily involve a clear and comprehensive allocation of accountability to humans and/or organisations. Lessons could be learnt from the aerospace industry, for example, which has a proven system of handling unwanted consequences thoroughly and seriously. The development of this process mustfollow from theassessmentofexisting capacity outlined in Recommendation 1.Ifalackofcapacityisidentified, additional institutional solutions should be developed at national and/or EU levels, to enable people to seek redress. Such solutions may include: 

an“AIombudsperson” toensure theauditingofallegedly unfairor inequitable uses of AI; 

a
guided process for registering acomplaint akin to making a Freedom of Information request; and 


the
development ofliabilityinsurancemechanisms,which would be required as an obligatory accompaniment of specific classes of AI offerings in EU and other markets. This would ensure that the relative reliability of AI-powered artefacts,especially inrobotics, ismirrored ininsurance pricing and therefore in the market prices of competing products.7 


Whichever solutionsare developed, these are likely torely onthe framework for intelligibility proposed in Recommendation 4. 
7 Of course, to the extent that AI systems are ‘products’, general tort law still applies in the sameway to AI asit applies in any instance involving defective products or services that injure users or do not perform as claimed or expected. 

8. 
9. 
10. 
11. 
Develop agreed-uponmetrics for the trustworthiness of AI products and services, to be undertaken either by anew organisation, orby asuitable existing organisation. These metrics would serve as the basis for a system that enables theuser-driven benchmarking ofall marketed AI offerings. In this way, anindexfor trustworthy AIcanbe developed and signalled, in additiontoaproduct’s price.This“trustcomparisonindex”for AIwould improve publicunderstanding andengendercompetitiveness around the development of safer, more socially beneficial AI (e.g., “IwantgreatAI.org”). In the longer term, such asystem could form the basis for abroader system of certificationfor deservingproducts andservices,administered by the organisationnotedhere, and/or by theoversight agencyproposed in Recommendation 9.The organisation could also support the development of codes of conduct (see Recommendation 18). Furthermore, those who own or operate inputs toAI systems and profit from it could be tasked with funding and/or helpingtodevelop AI literacy programs for consumers, in their own best interest. 
Develop anew EUoversight agencyresponsible for theprotection of public welfare throughthescientific evaluation andsupervision ofAI products, software, systems orservices.Thismay be similar, for example, totheEuropean Medicines Agency. Relatedly, a“post-release” monitoring system for AIs similar to, for example, the oneavailable for drugs should be 
developed,  with reporting  duties for  some stakeholders  and easy reporting  
mechanisms for other users.  
Develop a European observatory  for  AI. The mission of the observatory  
would  be to watch  developments, provide  a forum  to nurture  debate and  

consensus,provide arepository for AIliterature andsoftware (including concepts and links to available literature), and issue step-by-step recommendation and guidelines for action. 
Develop legal instruments and contractual templates to lay the foundation 
for asmooth and rewarding human-machine collaboration in the work 
environment. Shaping the narrative onthe ‘Future of Work’ is instrumental towinning “hearts and minds”. In keeping with ‘A Europe that protects’, the idea of “inclusive innovation” andtosmooth thetransition tonewkinds of jobs,aEuropean AI Adjustment Fund could be setup along the lines of the European Globalisation Adjustment Fund. 


5.2.3 Incentivisation 
12. 
Incentivise financially, attheEUlevel, thedevelopment anduseofAI technologies withintheEUthatare socially preferable (not merely acceptable) andenvironmentally friendly (not merely sustainable but favourable totheenvironment).Thiswillincludetheelaborationof methodologies that canhelpassesswhether AI projects are socially preferable and environmentally friendly. In this vein, adopting a ‘challenge approach’ (see DARPA challenges) may encourage creativity and promote competition in the development of specific AI solutions that are ethically sound and in the interest of the common good. 

13. 
Incentivise financially asustained,increased andcoherent European research effort,tailored tothe specific features of AI asascientific field of investigation. This should involve a clear mission to advance AI for social good, toserve asauniquecounterbalancetoAItrends withlessfocus onsocial opportunities. 

14. 
Incentivise financially cross-disciplinary andcross-sectoral cooperation anddebateconcerningtheintersections between technology, social issues, legal studies, and ethics.Debates about technological challenges may lag behind the actual technical progress, but if they are strategically informed by adiverse, multistakeholder group, they may steer and support technological innovation inthe right direction. Ethics should help seize opportunities and cope with challenges, not only describe them. It is essential in this respect that diversity infuses the design and development of AI, in termsof gender, class, ethnicity, discipline andotherpertinentdimensions,inorder toincrease inclusivity, toleration, and the richness of ideas and perspectives. 


15. Incentivise financially theinclusion ofethical, legal andsocial considerations inAIresearch projects. Inparallel, incentivise regular reviews of legislation to test the extent to which it fosters socially positive innovation.Taken together, thesetwo measures willhelpensure thatAI technology has ethics at its heart and that policy is oriented towards innovation. 
16. Incentivise financially the development and useof lawfully de-regulated specialzoneswithin the EU for the empirical testing and development of AI systems.Thesezonesmay take the form ofa“living lab” (or Tokku), buildingontheexperience ofexisting“testhighways” (or Teststrecken).In 

additiontoaligning innovation more closely withsociety’s preferred level of risk, sandbox experiments such asthese contribute to handson education and the promotion of accountability and acceptability at anearly stage. “Protection by design” is intrinsic to this kind of framework. 
17. Incentivise financially research about public perception and understanding of AI and its applications, and the implementation ofstructured public 
consultation mechanisms to design policies and rules related to AI.This may includethedirect elicitationofpublic opinion viatraditionalresearch methods, such asopinion polls and focus groups, aswell asmore experimental approaches, suchasproviding simulatedexamples oftheethicaldilemmas introduced by AI systems, orexperiments in social science labs. This research agenda should not serve merely to measure public opinion, but should also lead to the co-creation of policies, standards, best practices, and rules as a result. 

5.2.4 Support 
18. 
Support the development of self-regulatory codes of conduct for data and AI related professions, with specific ethical duties.This would be along the lines of other socially sensitive professions, such as medical doctors or lawyers, i.e., with the attendant certification of ‘ethical AI’ through trust-labels to make sure that people understand the merits of ethical AI and will therefore demand itfrom providers. Current attentionmanipulationtechniquesmay be constrained through these self-regulating instruments. 

19. 
Support the capacity of corporate boards of directors to take responsibility for the ethical implications of companies’ AI technologies.For example, thismay includeimproved trainingfor existing boards andthepotential development of anethics committee with internalauditing powers. This could be developed within the existing structure of both one-tier and two-tier board systems, and/or in conjunction with the development of amandatory form of “corporate ethical reviewboard” to be adopted by organisations developing or using AI systems, to evaluate initial projects and their deployment with respect to fundamental principles. 

20. 
Support thecreation ofeducational curricula andpublic awareness activities around thesocietal, legal, andethical impactofArtificial 



Intelligence. This may include: 

curricula for schools, supporting the inclusion of computer science among the basic disciplines to be taught; initiatives and qualification programmes inbusinessesdealingwithAI technology, to educate employees on the societal, legal, and ethical impact of working alongside AI; aEuropean-level recommendation to include ethics and human rights in the degrees of data and AI scientists and other scientific and engineering curricula dealing with computational and AI systems; thedevelopment ofsimilarprogrammes for thepublicatlarge,witha specialfocus onthoseinvolved ateachstageofmanagementofthe technology, including civil servants, politicians and journalists; engagement with wider initiatives such as the ITU AI for Good events and NGOs working on the UN Sustainable Development Goals. 

CONCLUSION 
Europe, and the world at large, face the emergence of a technology that holds much exciting promise for many aspects of human life, and yet seems to pose major threats as well. This White Paper – and especially the Recommendations in the previous section 
– seek to nudge the tiller in the direction of ethically and socially preferable outcomes from the development, design and deployment of AI technologies. Building on our 
identification of both the core opportunities and the risks of AI for society as well as the setoffive ethicalprinciples we synthesisedtoguideitsadoption,we formulated 20 
Action Points in the spirit of collaboration and in the interest of creating concrete and constructive responses to the most pressing social challenges posed by AI. 
With the rapid pace of technological change, it can be tempting to view the political process in the liberal democracies of today as old-fashioned, out-of-step, and no longer up to the task of preserving the values and promoting the interests of society and everyone init.We disagree. With the Recommendations we offer here, includingthe creation of centres, agencies, curricula, and other infrastructure, we have made the case for an ambitious, inclusive, equitable programme of policy making and technological innovation, which we believe will contribute to securing the benefits and mitigating the risks of AI, for all people, and for the world we share. 
Acknowledgements 
This White Paper would not have been possible without the generous support of Atomium 
– European Institute for Science, Media and Democracy. We are particularly grateful to Michelangelo Baracchi Bonvicini, Atomium’s President, to Guido Romeo, its Editor in 
Chief, and the staff of Atomium for their help, and to all the partners of the AI4People project and members of its Forum (http://www.eismd.eu/ai4people) for their feedback. 
The authors of this White Paper are the only persons responsible for its contents and any remaining mistakes. 
With the contribution of: 


References 
Asilomar AI Principles (2017). Principles developed in conjunction with the 2017 Asilomar conference [Benevolent AI 2017]. Retrieved September 18, 2018, from https://futureoflife.org/ai-principles Cowls, J. and Floridi, L. (2018). Prolegomena to a White Paper on Recommendations for the Ethics of AI (June 19, 2018). Available at SSRN: https://ssrn.com/abstract=3198732. Cowls, J. and Floridi, L. (Forthcoming). The Utility of a Principled Approach to AI Ethics. European Group on Ethics in Science and New Technologies (2018, March). Statement on Artificial Intelligence, Robotics and ‘Autonomous’ Systems. Retrieved September 18, 2018, from https://ec.europa.eu/info/news/ethics-artificial-intelligence-statement-ege-released-2018-apr-24_en. Imperial College London (2017, Oct, 11). Written Submission to House of Lords Select Committee on Artificial Intelligence [AIC0214]. Retrieved September 18, 2018, from http://bit.ly/2yleuET The IEEE Initiative on Ethics of Autonomous and Intelligent Systems (2017). Ethically Aligned Design, v2. Retrieved September 18, 2018, from https://ethicsinaction.ieee.org Floridi, L. (2018). Soft Ethics and the Governance of the Digital. Philos. Technol. 2018, 1-8. Floridi, L. (2013). The Ethics of Information. Oxford, Oxford University Press. House of Lords Artificial Intelligence Committee (2018, April, 16). AI in the UK: ready, willing and able? Retrieved September 18, 2018, from https://publications.parliament.uk/pa/ld201719/ldselect/ldai/100/10002.htm King, T., Aggarwal, N., Taddeo, M., and Floridi, L (2018, May, 22), Artificial Intelligence Crime: An Interdisciplinary Analysis of Foreseeable Threats and Solutions. Available at SSRN: https://ssrn.com/abstract=3183238 Montreal Declaration for a Responsible Development of Artificial Intelligence (2017, November, 3). Announced at the conclusion of the Forum on the Socially Responsible Development of AI. Retrieved September 18, 2018, from https://www.montrealdeclaration-responsibleai.com/the-declaration. Partnership on AI (2018). Tenets. Retrieved September 18, 2018, from https://www.partnershiponai.org/tenets/ Taddeo, M. (2017). The limits of deterrence theory in cyberspace. Philos. Technol. 2017, 1–17. 


Atomium-European Institute forScience, Media and Democracy(EISMD),convenesleadingEuropean universities, media, businesses,governments and policymakers to increase theexchange of information and interdisciplinary collaboration, to develop innovative collaborative initiatives and to encourage frontier thinking about science, media and democracy. 
Atomium-EISMDwas launched publicly by the former PresidentofFrance Valéry Giscard d’Estaing, MichelangeloBaracchiBonviciniandbytheleaders of the institutions engaged during the first conference onthe 27 November 2009 attheEuropeanParliamentinBrussels. 

Atomium - European Institute for Science, Media and Democracy 
24, Boulevard Louis Schmidt, Brussels, Belgium 
T. +32 (0)2 8887010  |F. +32 (0)2 8887011 
secretariat@eismd.eu  |www.eismd.eu www.AI4People.org 


