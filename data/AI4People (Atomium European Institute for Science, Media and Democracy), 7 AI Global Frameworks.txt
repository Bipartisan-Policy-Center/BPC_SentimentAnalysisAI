
AI4People s 7 AI Global Frameworks 
AI IS NOT MERELY ANOTHER UTILITY THAT NEEDS TO BE REGULATED ONLY ONCE IT IS MATURE. 
IT IS A POWERFUL FORCE THAT IS RESHAPING OUR LIVES, OUR INTERACTIONS, AND OUR ENVIRONMENTS. 
Luciano Floridi 
2018 Chairman, Scientific Committee AI4People, Professor of Philosophy and Ethics of Information and Director of the Digital Ethics Lab at Oxford University. 



AI4People s 7 AI Global Frameworks 
Following itspastwork onAIethics(withthe 
 AI4People s Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations ) andonAIgovernance(with the AI4People Report on Good AI Governance: 14 Priority Actions, a 
S.M.A.R.T. Model of Governance, and a Regulatory Toolbox ), in 2020AI4People has identified seven strategicsectors(Automotive, Banking & Finance, Energy, Healthcare, Insurance,LegalService Industry, Media & Technology)for the deployment of ethical AI, appointing 7 different committeesto analyze how can trustworthy AI be implemented in these sectors: the AI4People s 7 AI Global Frameworks are the result of this effort. 



TABLE OF CONTENTS 
1. Automotive 8 
a. Abstract 9 b. Aim & Scope of this paper 9 c. The Guidelines 10 d. Conclusion 32 e. Ai4people Practical Recommendations for The Automotive Sector 32 f. References 38 

2. Banking & Finance 42 
a. Executive Summary 43 b. Overview of ai and its role in banking and finance 45 c. Analysis and Recommendations 48 d. Final recommendations 59 e. References 62 

3. Energy 66 
a. Introduction 67 b. How the Seven Key Requirements Impact the Energy Sector 69 c. What the Energy Sector Must Do to be Compliant with the Seven Key Requirements 86 d. Conclusion, Practical Recommendation and Obligations 92 e. References 94 

4. Healthcare 98 
a. Introduction 99 b. AI and Healthcare 100 c. Risk, Danger and Hazard 101 d. Case Studies 104 e. General Discussion and Conclusions 114 f. References 117 

5. Insurance 121 
a. Executive summary 122 b. Introduction 129 
c. The impact of ai for the insurance sector 129    Insurance sector overview and key stakeholder segments (value-chain) 
d. Use-case analysis regarding the 7 key requirements for trustworthy AI 133 
e. Recommendations for the insurance sector 162 f. References 169 



TABLE OF CONTENTS 
6. Legal Services Industry 171 
a. Introduction: scope and remit of the report 172 b. Foundational Principles for Responsible use of AI in law 178 c. Principles for Responsible Development of AI in Law 184 d. Principles for Responsible Employment of AI in Law 191 e. Summary of Principles 205 f. Bibliography 209 
7. Media & Technology 212 a. Abstract 213 b. Introduction 214 
c. Conceptual Framework for AI in Media and Technology Sector 215 
d. European AI Governance for MTS 220 
e. Research Questions 222 f. Conclusion 247 g. Acknowledgements 249 


COMMITTEES MEMBERS 
Atomium EISMD wishes to thank the following Chairs and Committee's members for their participation and contributions: 
Automotive:Christoph L tge1; Franziska Poszler2; Aida Joaquin Acosta3; David Danks4; Gail Gottehrer5; Nicolae Lucian Mihet6; Aisha Naseer7; Banking & Finance: Nir Vulkan8; Aisha Naseer7; FrankMcGroarty9; Giulia Del Gamba10; John Cooke11; Lampros Stergioulas12; Paul Jorion13; Raffaella Donini14; Energy: Nicolae Lucian Mihet6; Afzal S. Siddiqui15; Fausto Pedro Garc a M rquez16; R n n Kennedy17; Sergio Saponara18; Healthcare: Raja Chatila19; Stephen Cory Robinson20; Donald Combs21; Paula Boddington22; Herv  Chneiweiss23; Eugenio Guglielmelli24; Danny van Roijen25; Jos Dumortier26; Leonardo Calini27; Insurance: Frank McGroar ty9; Gianvito Lanzolla28; Nir Vulkan8; Paul Jorion13; Patrice Chazerand29; Rui Manuel Melo Da Silva Ferreira30; Tilman Hengevoss31; Xenia Ziouvelou32; Legal Services Industry: Burkhard Schafer33; Cornelia Kutterer34; Elisabeth Staudegger35; Evdoxia Nerantzi36; Jacob Slosser37; Jamie J. Baker38; Mireille Hildebrandt39; R n n Kennedy17; Media & Technology:Jo Pierson40; Stephen Cory Ro binson20; Paula Boddington22; Patrice Chazerand29; Aphra Kerr41; Stefania Milan42; Fons Verbeek43; Cornelia Kutterer34; Evdoxia 
Nerantzi36; Elizabeth Crossick44; Norberto Andrade45; Janne Elvelid46. 
1.	 ChairmanAutomotiveCommittee,AI4People;DirectoroftheTUMInstituteforEthicsinArtificialIntelligenceatTechnical 
University of Munich, Germany 
2. 
Research Associate & PhD Student, Technical University of Munich, Germany 

3. 
Head of Unit at Ministry of Transport and Infrastructure, Madrid, Spain 

4. 
L.L. Thurstone Professor of Philosophy and Psychology Chief Ethicist, Block Center for Technology and Society, Carnegie Mellon University, USA 

5.	 
LawOfficeofGailGottehrerLLCALawFirmFocusedonEmergingTechnologies 

6. 
Chairman Energy Committee, AI4People; Professor in Energy Technology, Faculty of Engineering, Oestfold University College, Norway 

7. 
AI Ethics Research Manager at Fujitsu Laboratories of Europe 

8. 
Chairman Banking & Finance Committee, AI4People; Associate Professor of Business Economics at Sa d Business School, University of Oxford, UK 

9. 
Chairman Insurance Committee, AI4People; Professor of Computational Finance and Investment Analytics; Director of Centre for Digital Finance at Southampton Business School, UK 

10. 
Digital and Innovation Policy Advisor at Intesa Sanpaolo 

11. 
Chairman of the Liberalisation of Trade in Services Committee at TheCityUK 

12. 
Professor in Business Analytics at the University of Surrey, UK 

13. 
Associate Professor of Ethics, Universit  Catholique de Lille, France 

14. 
Senior Manager, European Digital and Innovation policies at Intesa Sanpaolo 

15. 
Professor of Energy Economics in the Department of Statistical Science, UCL, UK 

16. 
Full Professor at Castilla-La Mancha University, Spain 

17. 
Lecturer in Law, School of Law, National University of Ireland Galway, Irelands 

18. 
Professor of Electronics, Department of Information Engineering, Pisa University, Italy 

19. 
Chairman Healthcare Committee, AI4People; Professor and Director of the Institute of Intelligent Systems and Robotics (ISIR)at Pierre and Marie Curie University in Paris, France 

20. 
Senior Lecturer/Assistant Professor in Communication Design at Linking University, Norrkoping, Sweden 

21. 
Vice President & Dean of the School of Health Professions, Eastern Virginia Medical School, USA 

22. 
Senior Research Fellow, New College of the Humanities London, UK 

23. 
Directeur de Recherche au CNRS, Paris, France 

24. 
Senior Advisor on Publications for IEEE RAS Professor of Bioengineering Prorector for Research Founder, Research Unit of Biomedical Robotics and Biomicrosystems Universit  Campus Bio-Medico di Roma 

25. 
Digital Health Director at COCIR 

26. 
Honorary Professor of ICT Law at the University of Leuven, Belgium 

27.	 
PolicyManager,EuropeanGovernmentAffairsatMicrosoft 

28. 
Professor and Dean at Cass Business School - City, University of London, UK 

29. 
Director at DIGITALEUROPE 

30.	 
ChiefDataGovernanceOfficer,ZurichInsuranceGroup(ZIG) 

31.	 
HeadPublicAffairsEMEARegionatZurichInsuranceGroup(ZIG) 

32.	 
InnovationOfficerandResearchScientist,InstituteofInformaticsandTelecommunications,NationalNationalCentrefor ScientificResearchDemokritos,&MemberoftheScientificCommitteeonDataPolicyandArtificialIntelligence, 


National Council for Research and Innovation (NCRI), Greece 
33. 
Chairman Legal Services Industry Committee, AI4People; Professor of Computational Legal Theory; Director, SCRIPT Centre for IT and IP Law, University of Edinburgh, Scotland 

34.	 
SeniorDirector,RuleofLaw&ResponsibleTech,EuropeanGovernmentAffairsatMicrosoft 

35. 
Professor at Universit t Graz, Austria 

36.	 
PolicyManager,EuropeanGovernmentAffairsatMicrosoft 

37. 
Carlsberg Foundation Postdoctoral Fellow at University of Copenhagen, Denmark 

38. 
Associate Dean and Director of the Law Library; Professor of Law at Texas Tech University School of Law, USA 

39. 
Research Professor on 'Interfacing Law and Technology' at Vrije Universiteit Brussel, Belgium 

40. 
Chairman Media & Technology Committee, AI4People; Professor at imec-SMIT, Department of Media & Communication Studies, Vrije Universiteit Brussel, Belgium 

41. 
Professor of Sociology at Maynooth University and Maynooth lead of the ADAPT Centre for Digital Media Technology, Ireland 

42. 
Associate Professor of New Media and Digital Culture, University of Amsterdam 

43. 
Full Professor in Bio-Imaging and Bio-Informatics, Leiden Insitute of Advanced Computer Science 

44. 
Head of Government Relations at RELX 

45. 
Global Policy Lead for Digital and AI Ethics at Facebook 

46.	 
PolicyManagerEUAffairsatFacebook 



AI4P 


IN BRIEF 
AI4People isamulti-stakeholder forum, bringing together all actorsinterested in shaping the social impactofnewapplications ofAI, including the European Commission, theEuropean Parliament, civil society organisations, industry and the media. 
LaunchedinFebruary 2018withathree year roadmap, thegoalofAI4People istocreate a commonpublic space for laying outthe founding principles, policies and practices onwhich to build a  good AI society . For this to succeedwe need to agree onhow best to nurture human dignity, foster human flourishing and take care ofabetterworld. It isnotjustamatteroflegal acceptability, itis really a matter of ethical preferability. 
1 


AUTOMOTIVE 
AI4People-Ethical guidelines for the automotive sector: Fundamental requirements & practical recommendations for industry and policymakers 


ABSTRACT 
This paper presents thework of the AI4People-Automotive Committee established to advisemore concretely onspecific ethical issues that arise from autonomousvehicles (AVs). Practicalrecommendations for theautomotive sectorare provided across the topicareas: human agency and oversight, technical robustness and safety, privacy and data governance, transparency, diversity, non-discrimination and fairness, societal and environmental wellbeing aswell asaccountability. By doing so, this paper distinguishes between policyrecommendations that aim toassist policymakers in setting acceptable standards and industry recommendations that formulate guidelines for companies across their value chain. In the future, the automotive sector may rely on these recommendations to determine relevant next steps and to ensure that AVs comply with ethical principles. 
Keywords: Autonomous driving, Self-driving cars, Autonomous vehicle ethics, Governance, Regulation, Ethics of AI, AI4People, Transparency, Override, Fundamental Rights 
AIM & SCOPE OF THIS PAPER 
Inthepast decade, many policy documents have discussedethicalissuesand potential future directions related to new emerging technologies such asartificial intelligence (AI) orautonomoussystems. Thispaperpresents thework oftheAI4People-Automotive Committee1 established to advise more concretely onspecific ethical issues that arise from autonomous vehicles (AVs). The committee consisted of industry experts and researchers from the fields of ethics, law, philosophy, engineering, technology and policy. The aim of this paper is to provide the automotive sector, including both companies and public entities such as regulators, with concrete and practical guidelines to comply with ethical principles within the AI systems of AVs. Therefore, this paper could serve as a checklist for policymakers and companies aswell asabasis for developing acertification of ethics, an ecosystem of trust  (EuropeanCommission, 2020b) and ultimately a Good AI Society  (Floridi et al., 2018) in the automotive sector. These guidelines are intended to provide a clearer vision and moral compass onhow to proceed and what to consider when developing AVs, rather thanadditional barriers to innovation. The automotive sector is defined here inthe broadest termspossibletoencompass awiderange of companies involved inthe development of vehicles, including private cars,trucks,bussesand shuttles.Sea,airandmilitary-type applications have beenexcluded due to their functional and ethical specificity. This paper will focus onthe ethics of the AI-based tools that are used in automotive technology, rather thanon the ethics of vehicles in general. 
1 All co-authors of this paper constitute the AI4People-Automotive Committee. 
This paper distinguishes between high-level guidelines for policymakers ( policy recommendations ) and concrete actionable recommendations for companies ( industry recommendations ). However, the line between the two cannot always be drawn clearly which alsohighlightstheimportanceofco-regulation (i.e. theinteractionoflegal regulation andself-regulation by companies) (Pagallo etal.,2019).Thepolicy recommendations are designed to focus attentiononpressing policy issues and assist in setting acceptable standards. Thus, the policy recommendations ultimately influence the industryrecommendations. Responsible targetsfor theexecution ofthepolicy recommendations are: policymakers, legislators, ethics standards boards and commissions suchasthe United Nations Economic Commissionfor Europe (UNECE). The industry recommendations formulate guidelinesfor companies across theirentire value chain (especially duringresearch &development, production &operationsandservice). Therefore, original equipment manufacturer (OEM) / car manufacturers are the primary responsible targets for those recommendations.   Before turning to the principles and guidelines, we note three points of consensus among the authors: (1) aresponsible balancing of risks orestimated harm should be permitted at any time for AVs; (2) a large-scale introduction of full-mode AVs (level 4 and higher) onto streets is unlikely in the short run, sowe must consider amore incremental, step by-step approach; and (3) policymakers face significant challenges now, and so there are significant pressures to quickly develop a clear regulatory framework. 
THE GUIDELINES 
Fundamental rights underlying the guidelines 
Particular fundamentalrightsare thebasisfor theproposed seven requirements that were originally derived by the High-level Expert Group on Artificial Intelligence (2019) 
(i.e.humanagencyandoversight; technicalrobustness andsafety; privacy anddata governance; transparency; diversity, non-discriminationandfairness;societaland environmental wellbeing; accountability) and recommendations in this paper. In addition togeneral humandignity, key fundamentalrights(United Nations, 1948)that policymakers and companies in the automotive sector should recognize are: Right to self-determination and liberty which draws attention to human agency in self-driving cars (i.e. importance of override options) (see Guideline 1). Right to life and security which entails ensuring technical robustness and safety of operating self-driving vehicles; onabroader level, this includes securing societal and environmental wellbeing (see Guideline 2 and 6).Right to protection of personal data drawing attentiontodataownership, data governance and privacy of personal data that is generated during the operation of self-drivingcars(see Guideline3).Right to equality and non-discrimination requiring the avoidance of unfair bias in operating vehicles aswell asthe accessibility of benefits for every individual in society (see Guideline 5). Right to explanation which, in the field of autonomousdriving,demandstransparency andcommunicationoftheunderlying functionality, which canbe achieved through accountability measures suchasaudits andloggingmechanisms(see Guideline3and7).Certainly, incompatibilitiesand trade-offs between fundamental rights can emerge; for example, life and security can be in tension with the right to self-determination. On the onehand, AVs are expected to improve traffic flow and decrease fatalities that are due to human error. On the other hand, automated driving systems reduce the driver s autonomy, perhaps to the point of beingamere passenger. Inthisregard, theEthics CommissiononAutomated and ConnectedDriving (BMVI, 2017) formulated thefollowing guideline:  Inafree society, the way in whichtechnology is statutorily fleshed out is such that abalance is struck between maximum personal freedom of choice in a general regime of development and the freedom of others and their safety  (L tge, 2017, p. 550). In conflict situations, policymakers andlegislatorsshoulddecidewhich fundamentalrightsare tobe prioritized. 
Policy recommendations: 
  
Relevant fundamental rights to be considered in the field of autonomous driving are: humandignity, righttoself-determination andliberty, righttolife and security, righttoprotection ofpersonaldata,righttoequalityandnon discrimination as well as the right to explanation. 

  
It mustberealized that there will be notechnologies orpolicies that maximize all fundamental rights for everybody simultaneously. There will always be trade-offs.Therefore, policymakers and legislators should decide which fundamental rights are to be prioritizedin particular situations. 

  
Indoingso,policymakers andlegislatorsshouldcooperatewithmultiple stakeholders toobtainnecessaryinformation for executing anevaluation and subsequent agreement on compromises and prioritization. 


1. Human agency and oversight   including monitoring, training, human-machine interfaces and external control of vehicle data 
Afew guidelineshave already beendeveloped thathighlighttheimportanceof maintaining personal autonomy in AVs, including possible requirements for a stop  or  override  button(European Commission, 2020b; L tge, 2017). At thesametime, autonomy requires informed anddeliberatecontrol, andsooverrides (and other measures) shouldnotnecessarily beuniversal. Inparticular, admissibilityofhuman override should be conditional on two aspects: 
(1)
 The level of automation of the AV2 

  
for levels up toand including 3, there should be anoverride function that can be executed at any time. 

  
for level 4, there should be anoverride function that canbeexecuted only when not impacting or undermining the safety mechanisms of the AV(e.g., one helpful factorto satisfy this requirement might be toimplement overrides with atime lag).Therationalefor thisisthat,ifindividualswere allowed tointervene immediately at any point, the inherent logic and longer-term plan completion of the technically functionalAV is disrupted which may lead to increased risks for all parties involved.  

  
for level 5, it is not necessary to include anoverride function, asitwould take away many oftheoriginaladvantages suchasinclusive accessibility (e.g., by excluding elderly, disabled individuals, youth orindividualswho do notholda drivinglicense), safety (e.g., humanstakingcontrol may beoutofpractice), trust (e.g., giving drivers the impression that the system could fail), and comfort (e.g., limiting opportunities for new and more comfortable mobility options and designs)3 



(2)
 The state and behavior of the driver 


  when the driver s abilities are impaired (e.g., due to alcohol consumption), the availability of an override function should be limited and preceded by a request for confirmation 
Nevertheless, recent examples of AVs involved in crashes draw attention to the failing assumptionofresponsibility by individuals.Theunderlying problems relate to overconfidence in, oroverreliance on, the AI system ascompanies do notadequately warn drivers and/or drivers violate the guidelines provided by the companies.  
2 The levels refer to the taxonomy developed by the SAE International (2018) for six levels of driving automation, ranging from no driving automation (level 0) to full driving automation (level 5). 3 The override function does not need to be similar to the way we are driving today such as taking over using a steering wheel or a paddle. On thecontrary, the control can be a function provided through some interfaces that do not take away the original advantages of AVs such as inclusive accessibility. 
Tesla Highway Accident:In2018,aTesla s Model Xcarcrashedintoacurb, collided with two othervehicles and caught fire while in Autopilot mode. The Tesla driver diedfrom blunt-force traumainjuries.TheU.S. National Transportation Safety Board (2020) determined that the probable causeof the crash was related to system limitations of the Tesla Autopilot, aswell asthe driver s overreliance onthe system andlackofresponse (due todistractionlikely from acellphone game application). Tesla s positionwas that it tries to ensure and monitor driver engagement inorder to prevent driver overreliance, and that its policies advise Tesla owners that inanSAE-defined Level 2 partial driving automation system (which it considers its vehicles to be), it is the driver s responsibility to be prepared to intervene at all times. Nevertheless, drivers continue to be overly reliant on Autopilot and appear to believe thatwhen in Autopilot, the vehicle is fully autonomous. This raises questions about theeffectiveness ofTesla s disclosures ofthecapabilitiesofthevehicle when in Autopilot. This case highlights that appropriate agency requirements must go beyond giving the driver the option touseastop-button andinclude providing the driver with sufficient information and training to know when to press that button. 
Therefore, companies must clearly distinguish and make apparent whether adriverless system isbeingusedorwhether adriver remains accountablefor driving (L tge, 2017). In order to realize effective human agency and clarity over personal responsibility, our approach concerning AVs is threefold: 
1.Companiesshouldputinplace technicalsafeguards tohelp Monitoring, training 
drivers remain fully aware andready to take over the driving and an external human-machine 
when the AV expects them to.AVs shouldmonitor drivers 
interaction is needed 
andhelpdrivers remain awake andattentive. For example, 
to improve one s 
current driving monitoring systems using camera-based facial 
ability to act with
recognition technology determine the driver s level of vigilance 
intention. 
and trigger alerts tothe driver when signs of distraction are detected(Research &Markets, 2019).Othermonitoring systems are related totheamountoftorque in the steering wheel. For example, Tesla (2020) lockstheactivation oftheautopilotmodeifthedriver seems inattentive (e.g., insufficient torque is applied or warnings are repeatedly ignored). Theupcomingregulationonautomatedlanekeeping systems willobligatecar manufacturers to introduce driver availability recognition systems and clarify the criteria that assesswhether adriver is deemed to be unavailable (e.g., eye closure) (UNECE, 2020b). UNECE also considers that  [a]utomated/autonomous vehicles shouldinclude driver engagement monitoring incaseswhere drivers couldbe involved (e.g., take-over requests)  (UNECE, 2019, p. 3). It isimportantthat handovers bealignedwiththelevel ofautomation:Asthelevel ofautomation increases, drivers engagemore in other activities such aswatching avideo,which decreases humancapabilitytotake over control (Merat etal.,2014).Thus, handovers should conform tohuman capabilitiesby, for example, obviating  the need for anabrupt handover of control to the driver (emergency)  (L tge, 2017, 
p.556).There iscurrently noagreement onwhat constitutesacomfortable transition time, and sowe donotpropose auniversal prescription onthis point. Inthemeantime, companies shouldprovide documentationthatjustifiestheir particular handover window. A possible starting point for determining a reasonable transition time might be that AVs, asthey drive, could learn about the capabilities ofdrivers from aggregated trafficdataandadjustthevehicle s parameters accordingly (respecting a safe minimum time response). 
2.Companiesshouldtrain drivers onthecapabilitiesandlimitationsofAVs (European Commission, 2020b), sothat individuals canmake informed decisions and do notover rely onthevehicle s capabilities (see also UNECE, 2019). This training should be tailored todifferent demographic groups, given recent studies that show demographic differences in interactions with AVs (Manser et al., 2019). Training programs should cover topics such asthe  [system s] functional intent, operationalparameters,system capabilitiesandlimitations,engagement/ disengagement methods, HMI, emergency fallbackscenarios, operational design domainparameters(i.e.,limitations), andmechanismsthatcouldalter[the system s] behavior while in service  (NHTSA, 2017, p. 15). Drivers should also be trainedonthe purpose of using anAVs, the degree of automation, and conditions for potential system failures (Manser et al., 2019). 
3.The importance of human autonomy applies not only to drivers but also to humans outside the vehicle such aspedestrians. Therefore, companies should ensure that these latter individuals can also exercise their autonomy. For example, AVs should have mechanismstoshow pedestrians that they have beenrecognized and reveal theAV s motionintentions,perhapswithLEDstripstoconvey perception information (e.g., displaying cool colors for far away obstacles and warm colors for nearobstaclesintheenvironment) (Florentine etal., 2016). Theseexternal human-machine interfaces facilitatehumanagencyfor pedestrians,asthey enable them to feel less anxious about the technology and have more information tomove freely and safely. However, furtherresearch is required to determine the most useful interfaces (Rouchitsas & Alm, 2019). 
Additionally, external oversight mechanisms need tobe put in place to control for adequatehumanagency. Therefore, althoughinternaloverriding functionsmay not always orimmediately be available for (drivers in) AVs, general oversight should be possibleatall times. Live and total oversight is both impracticable and unwarranted (L tge, 2017). However, under certain circumstances, such as following a fatal accident, and depending onthe legal and regulatory framework in place in the country where theaccidentoccurred, itmay beappropriate todesignateanorganizationineach jurisdiction that is permitted to retrospectively look at the code and data within the AV to determine the cause of the accident (for more information see Guideline 7). 
Industry recommendations: 
  
There should be aconditionaloverride option allowing the control to be handed back to the driver. The admissibility of an overridefunction depends on the level of automation of the AV (up tolevel 3: atany time; level 4: corresponding to safety mechanisms of anAV; level 5: notrequired) aswell asonthestateand behavior of the driver (e.g., impaired ability). 

  
AVs should continuously assessand monitor the driver s attentiveness and ability to intervene.Before operation, the AV could pose control questions to the driver (e.g., did you ingestany drugs oralcohol?); during operation, the AV could use sensorsand biometric technology todoso.The upcoming UN Regulation on Automated Lane KeepingSystems canserve asabaseline for carmanufacturers to develop appropriate driver attentiveness recognition systems. 

  
Handover shouldcorrespond tothedriver s capabilities.Therefore, AVs could learnaboutdrivers  capabilitiesandresponse timesduring operation from aggregated dataandadjustthevehicle s parametersaccordingly (respecting a safe minimum time response). 

  
Companies should provide documentation that justifies their particular handover window.  

  
Training programs shouldbetailored todifferent demographic groups and exhibitminimumelementsthatshouldberegarded inatrainingcurriculum (e.g., limitations and capabilities of AVs) based on findings of recent studies. 

  
AVs should offer a training mode , for the first kilometers to train drivers onthe AV s functioning. 

  
Externalhuman-machine interfacesshouldclearly communicateaboutthe vehicle s motion intentionand awareness of other traffic participants to humans outside the vehicle.      


Policy recommendations: 
  
Policymakers shouldfinalizewhat constitutes acceptable and legitimateoverride functions and define applicable situations for activation. 

  
Policymakers shoulddeterminestandards for drivers  monitoring,training requirements, handover routines and external human-machine interfaces.These standards should be as global as possible. 

  
Policymakers in each jurisdiction should consider designating anorganization in each jurisdictionthat is allowed tolookatthe code and data within the AVin the event of a fatal accident involving an AV or a corresponding legal proceeding. 



2. Technical robustness and safety   including resilience to attack and security, fall back plan and general safety, accuracy and reliability 
A prime requirement of AVs should be safety, both in ordinary operations and if subject to adversarial attack (L tge, 2017). 
There are many differingformsofpotentialthreatsto AVs, and so governmental entities such as the ENISA(2019) or UNECE (2020a) have created holistic summaries andcategorizationsofrelevant dangersandvulnerabilities.Firstly, there are threats thatdonotsolely apply toAVs butalsotoconventional vehicles suchastechnical malfunctions and outages including sensor and other failures (ENISA, 2019). Secondly, there are threats thatare particularly important for AVs andcanbe subsumed under the term  cybersecurity . Potential cybersecurity threats include the following: 
  
hijacking such as unauthorized information disclosure or extraction of copyrighted or proprietary software from vehicle systems (product piracy) (UNECE, 2020a) 

  
abuse suchasattacksonback-end servers thatstopsthevehicle s functioning (e.g., disruptionsofcommunicationandexternalconnectivity) orthreats regarding thevehicle s update procedures (e.g., preventing the rollout of critical software updates) (UNECE, 2020a) 

  
passive behavioral attacks suchasindividualsintentionally interfering with AVs. For example, human drivers might tend to drive more aggressively around AVs orjaywalking may increase becauseitisknown thatAVs respect thesafety distance. 


There are several categorizationsofthreats thatrelate tothedatastored in vehicles on an associated server and to the information exchanged during communication between thevehicle and the server. These threats canimpact the safe operation of the vehicle, alter the software operation, and generate data breaches, though many of these threats are not specific to AVs but also can be found in current vehicles. 
It isessentialtodevelop mechanismstotest an AV s While oversight is cybersecurity management system before operation.TheEU more about retrospect, Cybersecurity Act aimstoestablishageneralcertification safety is more about 
prospect. 
framework for ICT digital products, services, andprocesses that 
allows the creation oftailored andrisk-based EUcertificationschemes (ECCG, 2020). Similarly, the UN is preparing aregulation onuniform provisions concerning theapproval ofvehicles withregard tocybersecurityandoftheircybersecurity management systems. For example, the draft regulation (as of March 2020) proposes aninternational approval mark ortheverificationof amanufacturer s compliance by anapproval authority (UNECE, 2020a). Inthefuture, suchclearregulations and standardized testswillbenecessarysothatallcompaniesare informed about,and comply with,theuniversal requirements for cybersecuritymanagementsystems. Governments should  promote mutualrecognition systems and certification schemes that are built upon international standards [...] to facilitate international harmonization onprivacy and security  (Joaquin Acosta, 2019, p. 215). SAE J3061, acomprehensive cybersecurity implementation guidelinefor theautomotive industry, canserve asa starting point (SAE International, 2016). 
Eurocybcar   Cybersecurity test for cars: Vehicles canbe considered computers onwheels. They contain systems such asABS, airbags, Bluetooth, eCall and remote control keys which make thevehicle susceptibletocyberattacks.Therefore, Eurocybcar developed the first European testing program for verifying the level of cybersecurity of (autonomous) vehicles. The test is twofold: first, it assesses the level ofprotectionagainst cyberattacks thatavehicle has; second, itevaluates how a cyberattackwould affect the integrity of the car s system and the physical security and privacy of its passengers. As soonasacarpasses the Eurocybcar test, it receives the  Cybersecure Car  seal, with a rating of one to five (Eurocybcar, 2019). 
Additional tothreats, measures needtobedeveloped thatassess the general functionality of an AV.The Ethics Commission on Automated and Connected Driving suggeststhat [t]he public sectorisresponsible for guaranteeingthesafety ofthe automatedandconnected systems introduced andlicensedinthepublicstreet environment. Driving systems thus need officiallicensingand monitoring  (L tge, 2017, p. 550). For example, akind of T V, i.e.atechnical inspection agency, for AVs could be developed. Relevant factorstobe assessed here are accuracy, reliability and fallback options of AVs. Intermsofaccuracy and reliability,it could be tested to what extent the AV s underlying  AI meets, or exceeds, the performance of a competent &careful humandriver , refrains from engagingin careless, dangerous orreckless driving behavior  aswell astowhat extent it  remains aware, willing and able to avoid collisions at all times  (ADA, 2020). SAE International published a more detailed and elaborate list of driving safety performance assessment metrics such asminimum safe distance factors orproper responses (Wishart et al., 2020). Furthermore, safeguards against technical failures andoutagesneedtobeestablished.TheIEEEP7009 standard for fail-safe design of autonomous and semi-autonomous systems could serve asabaselinefor developers. Thestandard provides clear procedures for measuring, testing,andcertifyingasystem s abilitytofailsafely aswell asinstructionsfor improvement in the case of unsatisfactory performance (IEEE, 2019). 
Intermsofgeneral safety and fallback plans,  [i]n emergency situations, the vehicle must autonomously, i.e., without human assistance, enter into a  safe condition   (L tge,2017, p.556).Thisconditionhasbeenspecifiedby proposing theterms  minimal risk condition  and  minimum risk maneuver . TheMinimal risk condition is  [a] condition to which auseroranADSmay bring avehicle after performing the DDT [dynamic driving task] fallback in order toreduce the risk of acrashwhen a given trip cannot orshould not be completed  (SAE International, 2018, p. 11). The  Minimum risk maneuver meansaprocedure aimedatminimizing risks in traffic, which is automatically performed by the system  (Leonhardt, 2018, p. 12). Causes for theexecution of such amaneuver could be detection that the driver is inactive and not reacting to transition demands, or reaching system failure / boundaries when the driver is not responding to transition demands. In such situations, potential maneuvers could entail  further lane keeping for acertain time, enlarging gap to other road users, [ ] slowing down tostandstill (BMVI, 2015, p.4).What constitutesanappropriate maneuver depends on (1) the operation condition of the vehicle (e.g., technical failures that hinder the AV to perform a fallback), (2) the prevailing environmental conditions (e.g., density of traffic) and (3) regulatory boundary conditions (Leonhardt, 2018). AlthoughSAEJ3016(Leonhardt, 2018) makes significantprogress regarding the nature ofasafe orminimal risk condition, the definition of such conditions aswell as the particular circumstances in which such conditions should be activated (e.g., incidents that leave the driver incapacitated such asastroke) need to be further determined and harmonized. 
Overall, experimenting with newAVs and testingtheir technical robustness and safety should follow a stepwise approach: For example,  the levels of testing that should beconductedbefore testing on open roads,including,for example, theuseof simulation, hardware-in-the-loop testing shouldbeidentifiedandstandardized (European Commission, 2020a, p. 29). Recognizing thechallengesofphysical test strategies for AVs (lengthof time they take tocomplete, high number of hours of drive timerequired), ESTECOhasdeveloped awhite-box /scenario-basedverification system to investigate the performance of ADAS/AD functions across different sensors, algorithms, actuation andscenarios (ESTECO, 2020). Systems like thesecanactas helpful antecedents to actual testing on open roads. 
Industry recommendations: 
  
Theprime requirement of AVs should be safety. 

  
In addition to threats that relate to conventional vehicles, manufacturers of AVs should particularly focus oncybersecurity threats.In doing so, companies need tocomply with regulations for cybersecurity management systems.SAE J3061 could serve as a guidelineto design cybersecurity into AVs throughout the entire development life cycle process. 

  
In terms of general functionality and safety, vehicles need to passanofficial test that assures the system s accuracy, reliability and adequacy of its fallback options. TheSAE DrivingSafety Performance Assessment Metricsand the IEEE P7009 standardcould serve as a baseline to design fail-safe mechanisms of autonomous and semi-autonomous systems. 


Policy recommendations: 
  
Regulations need to be developed that reflect consensusonthe method by which to grantapproval to a vehicle s cybersecurity management system. 

  
Policymakers need to work with industry experts to develop astandardized test for thegeneral functionality and safetyof AVs toassure the system s accuracy, reliability and adequacy of its fallback options. This testcouldserve asabasis for the approval of AVs for sale to consumers.  

  
Policymakers needtocollaboratewithindustry expertstodetermineand harmonizethedefinitionofa safe condition  / minimalriskcondition , the corresponding  minimum risk maneuvers , and the circumstances in which such maneuvers should be executed. In doing so, SAE J3016could serve as a baseline. 



3. Privacy and data governance   including respect for privacy, transparency and communication, and access to data 
Conventional vehicles collect data through event data recorders (that record technical information about avehicle s operation involved in crashes) and on-board diagnostic information (to accessinformation aboutdriver behavior, emissionmeasures or diagnose performance issues). With newtechnological options, connected vehicles and AVs willmake transportationsafer andmore convenient. However, many features dependonthecollectionandprocessing ofever more datainorder tofunction effectively (Future of Privacy Forum, 2017). 
Therefore, it is essential to specify the type and scope of data that AVs are permitted to collect. Three types of data can be distinguished that warrant special attention: 
  
geolocation data (e.g., for activating route navigation), which couldreveal the passenger s location and life habits of individuals (EDPB, 2020) 

  
biometric data (e.g., for userrecognition ortracking of driver s attention), which could be used to enable unauthorized accesstoavehicle and enable accesstoa driver s profile settings and preferences (EDPB, 2020). The collection of this type of data applies not only to drivers but also to individuals outside the vehicle such as pedestrians. 

  
driver behavior data,which couldreveal unlawful behavior, including traffic violations such as speeding (EDPB, 2020) 


Some of this data will be collected automatically, andsomewillrequire consent from the vehicle owner or driver in order to activate and use certain functions. Careful consideration needs tobe given tothe collection of data from inside the vehicle that relates to things other than the operation of the vehicle. Additionally, individual s rights shouldbeconsidered atgroup level (e.g., drivers versus pedestrians) (European Commission, 2020a). For example, data (especially, biometric data) of external parties suchasindividualswalking onthe street should warrant special protection. Although theEuropean Commission(2020a) hasadditionally problematized datacollection when AVs pass through particular locations suchasprivate and non-public settings, we suggest that collectingdata in private spaces should not in general be restricted in order to guarantee anAV s functionality. The focus should instead be onthe mode of data collection and sharing. 
Overall, servicesthatcollectandshare datashouldcomply withallapplicable laws, andbeaccompaniedby astrict privacy and data governance policy that includes, but is not limited to, the following (Future of Privacy Forum, 2017): 
1. 
Transparency and communication:Manufacturers needtoprovide clearand concise privacy policies to the vehicle owners that describe data collection and use. These policies must be readily understood by vehicle owners. These policies could, for example, be displayed in the purchase agreement, user manual or in the interface of an app. This is also in line with the General Data Protection Regulation (GDPR) statingthatcontrollers must,before personaldatais obtained, provide thedata subjectswithinformation necessarytoensure transparent processing aboutthe existence of automated decision-making. 

2. 
Affirmativeandexplicitconsent: The driver s educated and affirmative consent isrequired before certainsensitive data is collected orused. This requirement is particularly critical for marketing uses, or if the data will be shared with unaffiliated 


third parties.ThisisinlinewiththeguidelineoftheEthics Commissionon Automated andConnected Drivingaboutthepermissibilitytousedatathatis generatedby AVs for otherbusinessmodels,which statesthat lastly  [i]t is the vehicle keepers and vehicle userswho decide whether their vehicle data that are generated are to be forwarded and used  (L tge, 2017, p. 555). Additionally, even in the absence of laws requiring it, usersshould always have the right toopt-out or request that particular data not be collected, unless those data are critical for the AV safe system s operation.  
3.Limited and useful sharing with third parties:There The anonymization shouldonly be limitedcircumstances where manufacturers issue is a pivotal point to be
are allowed toshare avehicle s datawithexternal parties. 
highlighted, because it
Under appropriate conditionsandwiththeappropriate 
distinguishes privacy 
safeguards, data that guarantees safe operation of the vehicle 
from surveillance. 
andothertrafficparticipants aswell asdatathatprovides benefits to overall society and is of public interest should be shared. For example, AVs could provide information tothe local department of transportation about a potholeontheroad, sothat infrastructure inspections and maintenance resources canbebetterallocated(inconsiderationofafairandunbiaseddistributionof resources) andtrafficinformation canbeshared toimprove trafficflow and promote safety. Accordingly, the European Commission has issued aregulation requiring public orprivate road operatorsandserviceproviders toshare and exchange relevant road safety-related trafficdatasuchastheobservation ofa temporary slippery road or exceptional weather conditions (European Commission, 2013). Personally identifiable information must always be given the highest levels of protection. If data must be shared with third parties due to the above mentioned reasons, theyshould be anonymizedanddeidentifiedbeforebeingtransmitted (EDPB, 2020). For example, the EU Data Task Force partnered with TomTom to improve road safety by sharing anonymized vehicle and infrastructure data between countries and manufacturers. For example, this will allow the detection of dangerous road conditionssuchasslipperyroads andissuewarnings toothertraffic participants. TheEUDataTask Force (DTF) willuseadecentraliseddata collaborationarchitecture toshare vehicle-generated data [ ]. The datasets will then be taken by TomTom, processed further, and delivered back to other vehicles androad authorities via its live Traffic services  (Europawire, 2019). In line with Article 3(c) of Directive 2010/40/EU, data and procedures for the provision of road safety-related traffic information should be free of charge to users (European Commission, 2013). However, past studies showed individuals cansometimes be identified using anonymized data (Techcrunch, 2006; Archie et al., 2018), and so companies must ensure that the shared data does not permit re-identification (e.g., byminimizing collected data or usingdifferentialprivacytechniques). 
4.Compliance with pertinent data protection standards and regulations:All data collection and processing obviously must respect relevant regulations (EDPB, 2020), such as the GDPR that applies to the processing of personal individual data, aswell astheePrivacy directive for information accessonthe terminal equipment of a user (EDPB, 2020; European Commission, 2020b). The IEEE P7002 standard specifies how to manage privacy issues for systems that collect personal data, e.g., by providing a guideline for a privacy impact assessment (IEEE, 2019). 
Industry recommendations: 
  
Manufacturers shouldfollow astrictprivacy anddatagovernance policythat includestransparencyandcommunicationtousers,requesting affirmative consentandallowing limitedsharingofdatawiththird parties(including governments). In doing so, companies should comply withapplicable standards and regulations such as the GDPR, the ePrivacy directive or the IEEE P7002. 

  
Before transmitting personal information from anAV to third parties, steps must be taken to ensure that it cannot be traced back to an individual. 

  
Manufacturers should implement data protocols defining who canhave accessto data under which conditions. 


Policy recommendations: 
  
Before receiving AV data, policymakers need to make clear what types of AV data theyare seekingand how that data will enable them to improve public safety or someother legitimate public purpose (e.g., improve infrastructure, traffic flow and law compliance). 

  
At the EU level, building on Article 3(c) of Directive 2010/40/EU, consideration should be given to expanding the list of events and relevant traffic information that should be communicated free of charge. 



4. Transparency   as a key mechanism to realize all other requirements 
Intheautomotive sector, we contend thattransparency isnota 
Transparency is 
freestanding desideratum, but rather akey mechanism torealize the 
more like a mean other principles orrequirements. Transparency plays amajorrole for to an end   it is a achieving the principle of privacy and data governance, requiring that key mechanism to manufacturers provide vehicle owners with information regarding data realize the other six collectionpracticesand intended uses(for more informationsee requirements. Guideline 3). Similarly, tosatisfy the principle of accountability, the implementationof explicit transparency measures suchaslogging mechanisms orblack boxes are essential (for more informationsee Guideline 7). The IEEE P7001 ( Transparency of Autonomous Systems ) standard can serve as a baseline to address these issues. 

5. Diversity, non-discrimination and fairness   including the avoidance of unfair bias, responsible balancing and accessibility 
Generally and regarding the operations of AVs, no distinction between individuals should be allowed and fair treatment of all humans should be enacted. This is clearly stated in the Universal Declaration of Human Rights:  [e]veryone is entitled to all the rights [ ] without distinction of any kind, such asrace, colour, sex, language, religion, politicalorotheropinion,nationalorsocialorigin, property, birthorotherstatus  (United Nations, 1948, p. 2; Kriebitz & L tge, 2020). In the field of AI (e.g., AVs), this obligationbecomesever more importantasimplicit biasesanddiscriminationmay unintentionally, andwithouttransparency, beincorporated intoalgorithms.Studies show thatsystems canhave differential performance for people ofdifferent ethnic groups, which consequently canresult inthembeingharmed.For example, astudy from theGeorgiaInstitute ofTechnology illustrateshow state-of-the-art AIobject detection systems are lesslikely to detect pedestrians with darker skin color than those with lighter skin (Wilson, Hoffman & Morgenstern, 2019). Another study from the US National Institutefor Transportation andCommunities investigated thedriving behavior through crosswalks that  revealed that black pedestrians were passed by twice as many cars and experienced wait times that were 32% longer than white pedestrians . If such driving data is fed into amachine-learningalgorithm, the system may discover this discriminatory pattern and adapt it into its functioning (Forbes, 2020). 
In order to ensure non-discriminatory programming and functioning, the systems need to be trained and tested for unfair bias.Companies should test their algorithms for bias and discriminationand demonstrate that certain fairness standards are met (Vox, 2019). Laws could be enacted, for example, that mandate that facial recognition software used by public entities and companies must be independently tested for bias (Secretary of State Washington, 2020). The IEEE P7003 standard for algorithmic bias considerations sets out instructions for eliminating bias when developing algorithms: it provides developers of algorithms for autonomous systems with protocols and includes criteriafor selectingvalidation datasets(IEEE, 2019).Thetraining should be differentdependingonthelocationwhere the system operates: when atechnology is launched into the market, companies could localize it using location specific data. Companies could ensure that their developmentteamsaresufficientlydiverse to guard against intentional and implicit bias being incorporated into their algorithms and technologies (Vox, 2019). 
The Moral Machine Experiment: The Moral Machine Experiment by Awad et al. (2018) is anonline experimental platform designed to explore the moral dilemmas facedby AVs. Thepresented scenariosare oftenvariations ofthetrolley problem asking the participant asanoutside observer to choose between undesirable option suchaskillingtwo passengersorfive pedestrians. The data indicates someglobal tendencies: people support minimizing loss of life and protecting children, favoring the fit and wealthy, and sacrificing people who are old, overweight, orhomeless. The results alsoshowed broad differences inrelative preferences when comparing participants indifferent countries (e.g., the preference for sparing younger people rather than older ones is much higher for countries in the Southern cluster compared to the Eastern cluster). The implication is that  developing global, socially acceptable principles for machine learning  (Awad etal., 2018, p. 59) should be approached with great caution (Kochupillai, L tge & Poszler, forthcoming). The findings from the study indicates that it is more effective todraw attentiontothe prohibition of discriminatory decision-making. 
Past literature hasextensively debated dilemma situations (e.g., unforeseen and unavoidable accidents) with reference to the famous trolley cases.The ideal is to avoid such situations inwhich accidents are unavoidable in the first place; for example, the lateral position of AVs onalanecanbe adjusted totunethe risk posed toall other traffic participants (e.g., how muchroom should be given toabicyclist?). Therefore, we arguetomove away from thedebatearound dilemmasituations.Instead,a responsible balancing of risk or estimated harm should be permitted for AVs at all times. This balancing decision should not be based on personal features of individuals suchasageorgender (Lin, 2016; L tge, 2017). Instead, astheseverity ofinjury increases in proportion to the kinetic energy, estimated harm could be quantified and balancedby more objective factorssuchasthetypeorspeedofparticulartraffic participants and the impact angle under which acollisionwould occur(Gei linger et al., 2020). Taking into accountthe type of road userswould grant vulnerable traffic participants (e.g., pedestrians orcyclists) the samelevel of protection asotherroad users (European Commission, 2020a). Overall, the consideration of these factors could helpachieve a [g]eneral programming toreduce thenumberofpersonal injuries  (L tge, 2017, p. 552). 
Besides the unbiased vehicle s internal functioning, AVs should be human-centric (European Commission, 2020a). In particular, AVs should be equally usable for and accessible to all individuals,which requires anon-discriminatory design.For example, age orthe presence ofadisability is notalways considered by automotive companies, leading topotential issues of discrimination. Therefore, levels of differing abilities need tobe acknowledged (e.g., ayoung individual may have quicker reflexes for executing requests thanelderly people) andthesystems needtobeadapted accordingly for different users, sothateveryone canbenefit from thisnewtechnology (for more information see also Guideline 1). 
Industry recommendations: 
  
Companiesshouldtesttheirvehicle s AIsystemsfor unfairperformance differences across skintone, gender, ageandothercharacteristics.TheIEEE P7003 standardcan serve as a baseline to address and eliminate issues of bias in the creation of algorithms. 

  
When atechnologyislaunchedintothemarket, companies shouldlocalizeit using data and train the model using multiple diverse data sets that are location specific. 

  
TheAIdeveloping teamshould be as inclusive as possibleto include the broadest group possible in terms of demographics such as ethnicity. 

  
A responsible balancing ofrisksandpotentialharmtoreduce thenumberof personal injuries shouldbepermittedfor AVs withoutdiscriminating against personal characteristics. Instead, factors underlying the balancing could include the typeorspeed of particular traffic participantsand the impact angleunder which a collision would occur. 

  
ThepersonalizationofAVs shouldbeaccessibleby designandasinclusive as possible (e.g., disabilitiesincluded). Before anAV is released ontothestreets, companies should demonstrate their plans and actions that ensure customizing optionsto their vehicles (e.g., possibility to take away seats or include a ramp for entering the vehicle with a wheelchair).       


Policy recommendations: 
  
Consideration should be given to having ethics standards boards test and assess 

  
Consideration should be given torequiring carmakers to explain the procedures they have put in place to make their designs accessible and avoid biasesbefore granting them authorization to sell their vehicles to the public. 



6. Societal and environmental wellbeing   including sustainability and environmental friendliness and social impact 
Intermsof societal and environmental wellbeing, the Sustainable Development Goalsadopted by all United Nations Member States canserve asareference point. Goal 3 (to  [e]nsure healthy lives and promote well-being for all at all ages ) and goal 11(aiming to [m]ake citiesandhumansettlementsinclusive, safe, resilient and 
.that the systemsfor AVs areworking properly, fairly and in an unbiased manner 
sustainable )are particularly relevant tothistopic (United Nations, 2015, p. 14). Companiesandpolicymakers in the automotive sectorshouldfocus onmeeting the following objectives: 
1. 
Increased public health and mobility:AVs canimprove society s healthby avoiding fatalities that are due to human error (Bartneck et al., 2019). This is in line with Vision Zero, which states that eventually noonewill and shall be killed orseriously injured within the road transport system (Ministry of Transport and Communications, 1997). Theintroduction ofAVs couldoffer greater mobility solutions for amajor part of society that is mobility-impaired, whether the elderly, young (without adrivinglicense) orthosewho were otherwise unable todrive (BCG, 2017; WEF, 2018). This could positively affect mental health (e.g., due to feeling lessdependentonothers) andcreate amore inclusive society (Lim & Taeihagh, 2018). Thesebenefits,however, canonly berealized ifsafety and diversity standards are adhered to (for more information see Guideline 2 and 5). 

2. 
Bettertrafficflow:AVs couldreduce congestionanddelays (e.g., during peak hours) and improve traffic flows and efficiency, especially when combined with shared mobility options. For example, using atraffic simulation model for Boston, itwas found thatthesimulationsthathadincludedAV technology yielded less congestion,shortertravel timesandmore streetspaceand(BCG, 2017; WEF, 2018). These benefits stem mostly from AVs  connectivity to external communication networks so that data canbe managed and distributed in real time enabling methods suchasplatooning (Lim & Taeihagh, 2018). However, if not managed properly, it could also increase traffic flow and generate inefficiencies of uncoordinated traffic (Joaquin Acosta, 2018a). Proactive measures suchasadoptingafitting physical and digital infrastructure, could improve the existing traffic situation by atleast 15-20% (Inframix, 2020).   

3. 
Decreased carbon emissions:Widespread adoptionofAVs couldreduce environmental degradationthrough reduced emissions and energy consumption (BCG, 2017). This is especially trueif unnecessary acceleration and braking is reduced (Lim & Taeihagh, 2018). A concrete action point for companies would be to design AI systems that reduce vehicles  CO2 emissions. For example, companies could offer by default aneco-driving mode with aspeedaverage that minimizes emissions and avoids unnecessary acceleration orbraking. Many of the benefits relating tothereductionof carbon emission canbe achieved by combining AVs withotherdisruptive technologiessuchastheelectrificationofvehicles (BCG, 2020). In addition, promoting AV shared mobility could  lessen the environmental impact of passenger vehicles by decreasing the number of vehicles ontheroad  (Joaquin Acosta, 2018a, p. 3). A concrete action point for policymakers would be tofacilitateresearch and development for solutionstocombineAVs with other disruptive technologies (e.g., electrification, shared mobility). 


While these potential benefits are substantial, there is also significant uncertainty about thenet impact of introducing AVs. Many measures of benefits focus onimprovements pervehicle-mile traveled (VMT).However, theincreased mobilityandconvenience benefitswillpotentially leadtosignificantincreases inVMTs, potentially leading to increased total pollution,congestion, and soforth, despite the per-VMT gains (Geary & Danks, 2019). Thus, as technology continually develops, companies and policymakers in the automotive sectorshouldfollow a stepwise implementation process to ensure thatintroductionofAVsprovidesnetbenefits.Moreover, thisimplementation process must be combined with a simultaneous adaption of infrastructure (physical and digital).  Needed structural improvements include dedicated lanes to separate AVs from other traffic, and sensorstoenable self-driving carstocommunicate with their environment  (BCG, 2020). Otherwise, if AVs entertraffic in anuncoordinated way and without afitting infrastructure, traffic flow and other benefits may be degraded. Several projects of the EU Horizon 2020 programhave been focusing on this challenge (e.g., CoEXist or Inframix) (European Commission, 2019). 
Inframix: The Inframix project aims at developing aroad infrastructure for mixed vehicle traffic flows. Therefore, physical and digital elements of the road infrastructure need to be designed, upgraded and adapted to prepare for the stepwise introduction of automated vehicles without jeopardizing safety, quality of service and efficiency. This includes the design of novel visual signs and electronic signals that inform about theroad operator s control commands and are readable and understandable by both automated and conventional vehicles. Further objectives of the project are to develop hybrid-testing systems by merging infrastructure elements and vehicles on real roads withavirtualtrafficenvironment aswell astocreate aRoad Infrastructure Classification Scheme that assess the level of  automation-appropriateness  (Inframix, 2020). 
City planners, road operators and local authoritiesshould use the findings of such projects tomake informed decisions onwhere toroll outnewmobility models and how toupdate their road network accordingly. Collaboration with multiple private-sectorleaders and national agencies is key tofostering innovation and progress:  the successof AMoD [autonomous mobilityondemand] will depend to alarge extent on establishingclosepartnerships among mobility providers, infrastructure companies, and city authorities  (BCG, 2020). 
Industry recommendations: 
  
When developing theirproducts, automotive companiesshouldconsider integrating and providing benefits of increased public health and mobility, better traffic flow and decreased carbon emission. 

  
Manufacturers shouldoffer by default aneco-driving modewithaspeedaverage thatavoids unnecessaryaccelerationorbraking andthusreduces carbon emissions. 

  
When developing AVs, carmanufacturers should try to integrate other disruptive technologies such as electrification and shared mobility. 


Policy recommendations: 
  
Policymakers shouldfollow astepwise implementation processandconcentrate onmixed traffic scenarios. Policymakers shouldpromote the integration of AVs in existing transport systems instead of competitionbetween them, for example, by prioritizing research and development of AV solutions for public and shared mobility. 

  
A simultaneous adaptionof physical and digital infrastructureis essential (e.g., lanes that separate AVs from other traffic). 

  
In doing so, collaborationwith multiple actorssuchasprivate-sector leaders and national agencies is key to fostering innovation and progress (e.g., make useof projects investigating differing mobility models). 



7. Accountability   including auditability, measures of transparency, reporting of negative impact, and redress 
The attribution of liabilityand responsibilities for AVs is a challenging issue.  The first step towards the creation of aculture ofresponsibility is the study, deliberation andagreementonthedifferentresponsibilitiesofdifferentstakeholders  (European Commission, 2020a, p. 56). In caseof accidents, the AV itself cannotbe held morally accountablesince it is notconsidered amoral agent (Gogoll & M ller, 2017).Responsible partieswillinsteadbemanufacturers, componentsuppliers, technology companies, infrastructure providers orcarholders/drivers. Therefore, policymakers should clarify the concept of aproducer aswell asreview regulations on product liability (e.g., European Commission, 2018). This will, of course, vary dependingonthemotorvehicle laws in place indifferent countries. When adapting existingregulations toAVs, regulators may have tochoose between different liability regimes for different situations and levels of automation. For example, strict liability conceptsmay meanthat for AVs, the manufacturer canbe held liable if the automated mode is switched on, whereas, if not, the driver is considered liable. On the other hand, onecould argue that liability should move gradually from oneactor to the next (e.g., from thecarmanufacturer to the driver) depending onthe driver s level of autonomy andsoloaction.For guidance,regulators couldlooktoLaw Labs (Joaquin Acosta, 2018b). Law Labsare aproposed concept toexperimentwithdifferent regulatory approaches for agiven innovation (e.g., AVs), similartohow regulatory sandboxes experiment with innovations in controlled environments (operating under temporary regulatory exemptions). For example, trafficrulescouldberevised anditcouldbe investigated under whichcircumstances AVs are allowed to not to comply with a traffic rule (European Commission, 2020a). 
Inorder toprovide clarityaboutthecausesofaccidents, companies inthe automotive sector may want to consider the following issues: 
Regularly conduct internal and external audits.Intermsof internal audits, manufacturers shouldexecute continuous optimization and tests.This is in line with the guidelinesoftheEthics CommissiononAutomated andConnectedDriving,which statethat [l]iabilityfor damagecausedby activated automateddrivingsystems is governed by the sameprinciples asin other product liability [ :] manufacturers or operatorsare obligedtocontinuously optimizetheirsystems andalsotoobserve systems they have already delivered  (L tge, 2017, p. 553). In doing so, companies should conduct a risk assessment by listing factors that may increase risk and uncertainty regarding avehicle s operationandby proactively implementing appropriate countermeasures. Risksmay stem from thevehicle s technology (e.g., technical failure to transmit sensor data), the actions of other traffic participants (e.g., disobeying traffic law suchasjaywalking), external circumstances (e.g., the stateof the road, weather conditions) or the vehicle s driving behavior (e.g., speed). 
Uber Crash with jaywalker:In2018,aself-driving vehicle owned by Uber Technologies Inc. struck and killed apedestrianwho was walking her bicycle across a road at night in Arizona. The underlying reasons for the accident included software flaws, such asthe inability to recognize jaywalkers, which contributed to the failure tocalculate that the vehicle could potentially collide with the pedestrian until only 
1.2secondsbefore impact,atwhich pointitwas toolatetobrake (National Transportation Safety Board, 2019). These types of crashes highlight the importance of prior risk assessment (such as the potential of other participants to disobey traffic rules) and subsequent redress mechanisms. 
In addition to adhering to internal standards and audit requirements, external test centers could perform conformity assessments and grant certifications since  independent assessment will increase trust and ensure objectivity  (European Commission, 2020b, 
p.25). Common auditareas for acertification system are  similar to the AI4People requirements  autonomy and control, fairness, transparency, reliability, security and data protection. Expected benefits of an AI certification would be trust in the application, orientation for customersand developers, fulfillment of normssuchascybersecurity and data security and comparable market equality (IAIS, 2019). 
Implement explicit measures of transparency. These transparency measures should pertain tothe development aswell asthe operation of AVs. Before operation, during the development phase, companies should retain records and data including data sets used for training(e.g., selection process) and document the programming and training methodologies. This is particularly important if authorities seek to review the underlying logic of a system and inspect relevant documentation (European Commission, 2020b). Also, during operations, relevant information shouldberecorded through logging mechanisms and black boxes integrated into AVs (L tge, 2017). Regarding the black box, companies could consider an event data recorder and data storage system for AVs that record data of  the system status, occurrence of malfunctions, degradations orfailures inaway that canbe used to establish the causeofany crash and to identify thestatusof the automated/autonomous driving system and the statusof the driver  (UNECE, 2019, pp. 3-4). These measures will ensure that the functioning and actions ofAVs are explainable inretrospect. Overall,  [i]nternational standardization ofthe [ ] documentation (logging) is tobe sought in order toensure the compatibility of theloggingordocumentationobligationsasautomotive anddigital technologies increasingly cross national borders  (L tge, 2017,p. 555). For example, the upcoming regulation on automated lane keeping systems will determine what events are recorded by datastoragesystems for AVs (e.g., emergencymaneuvers, failures) (UNECE, 2020b). IEEE P7001 provides such astandard for the transparency and accountability of autonomous systems sothat the reasons why atechnology makes certain decisions canbe determined (IEEE, 2019). Similarly, SAE J3197 aims togovern data element definition,toprovide aminimum data element setandcommondata output formats for an automated driving system data logger (SAE International, 2020). 
Finally, external communication and reporting of performance and negative impact shouldberegularly required for companies intheautomotive sector. Manufacturers andregulators should anticipate that individuals will want explanations when an AV s system did not perform as expected and intended. This is similarly stated in the ethical guidelines for trustworthy AI:  Whenever anAI system has asignificant impactonpeople s lives, it should be possible to demand asuitable explanation of the AI system s decision-making process  (High-level Expert Group on Artificial Intelligence, 2019, p. 18). In California, for example, Transportation Network Companies (TNCs) suchasLyft have toprovide the California Public Utilities Commission with reports regarding zero tolerance complaints, violations and collisions of their vehicles onan annual basis (California Public Utilities Commission, 2020). Further information to disclose may be the tradeoffs made within algorithms, the number of past accidents put intocontext(e.g., relative number of accidents during testdrives compared tototal number of test drives) as well as safety measures initiated to counteract these accidents. Thereby data, algorithmic and AI literacy is improved (European Commission, 2020a). 
Industry recommendations: 
  
Manufacturers shouldcontinuously conductinternal audits(e.g., assessing potentialriskstothesafe operationofAVs) andsubsequentlyoptimizetheir systems. 

  
Manufacturers shouldbetransparent aboutthescopeandprocess oftheir internal audits and risk assessments(e.g., space of conditions that are checked for). 

  
Theinternalauditsshouldbecomplemented withregular externalauditsby independent test centers. 

  
Manufacturers shoulddevelop specific measures of transparency.This includes storingrecords and data of the underlying system logic(e.g., used training data sets) as well as logging mechanisms and black boxes(e.g., an event data recorder and data storage system) that document the actions of / in AVs during operation. The upcoming UN Regulation on Automated Lane Keeping Systemscan serve as a baseline for vehicle manufacturers to develop appropriate data storage systems for AVs. SAE J3197 and the IEEE P7001 standardcanserve asabaselineto address requirements for transparency andaccountabilityofautonomous systems. 

  
Companiesshouldtransparently communicateandreport performance and negative impactsof AVs (e.g., number of collisions, tradeoffs withinalgorithms). 


Policy recommendations: 
  
Regulators should adapt laws andregulations concerning AVs and liability asthe technology continues to develop.Regulators should clarifywhere responsibility lies in certain situations and ensure thatprivacy and cybersecurity damages are taken into account. 

  
Policymakers should consider establishing test centersthatregularly request that companies perform conformity assessmentsandprovide certifications. 





CONCLUSION 
This paper provides practical recommendations for the automotive sector to deal with ethical issues regarding: human agency and oversight, technical robustness and safety, privacy and data governance, transparency, diversity, non-discrimination and fairness, societal and environmental wellbeing aswell asaccountability. By doing so, this paper distinguishes between policy and industry recommendations in suggesting first steps to betaken by bothpolicymakers andcompanies.Thefollowing listsummarizesall recommendations. In the future, we encourage stakeholders in the automotive sector to rely ontheserecommendations todeterminerelevant actions and toensure that AVs comply with ethical principles.  
AI4PEOPLE PRACTICAL 
RECOMMENDATIONS 
FOR THE AUTOMOTIVE SECTOR 

Underlying Fundamental Rights 
Policy recommendations: 
  
Relevant fundamental rights to be considered in the field of autonomous driving are: humandignity, righttoself-determination andliberty, righttolife and security, righttoprotection ofpersonaldata,righttoequalityandnon discrimination as well as the right to explanation. 

  
It mustberealized that there will be notechnologies orpolicies that maximize all fundamental rights for everybody simultaneously. There will always be trade-offs.Therefore, policymakers and legislators should decide which fundamental rights are to be prioritizedin particular situations. 

  
Indoingso,policymakers andlegislatorsshouldcooperatewithmultiple stakeholderstoobtainnecessaryinformation for executing anevaluation and subsequent agreement on compromises and prioritization. 


1. Human agency and oversight 
Industry recommendations: 
  
There should be a conditional override optionallowing the control to be handed back to the driver. The admissibility of an overridefunction depends on the level of automation of the AV(up tolevel 3: atany time; level 4: corresponding to safety mechanisms of anAV; level 5: notrequired) aswell asonthestateand behavior of the driver (e.g., impaired ability). 

  
AVs should continuously assess and monitor the driver s attentiveness and ability to intervene.Before operation, the AV could pose control questionsto the driver (e.g., did you ingestany drugs oralcohol?);during operation, the AV could use sensorsand biometric technologytodoso.The upcoming UN Regulation on Automated Lane KeepingSystemscanserve asabaseline for carmanufacturers to develop appropriate driver attentiveness recognition systems. 

  
Handover shouldcorrespond tothe driver s capabilities.Therefore, AVs could learnaboutdrivers  capabilitiesandresponse timesduring operation from aggregated dataandadjustthevehicle s parametersaccordingly (respecting a safe minimum time response). 

  
Companies should provide documentationthat justifies their particular handover window. 

  
Training programs shouldbetailored todifferent demographic groups and exhibitminimumelementsthatshouldberegarded inatrainingcurriculum (e.g., limitations and capabilities of AVs) based on findings of recent studies. 

  
AVs shouldoffer a  training mode , for the first kilometers to train drivers on the AV s functioning. 

  
Externalhuman-machine interfacesshouldclearly communicateaboutthe vehicle s motion intentionand awareness of other traffic participants to humans outside the vehicle. 


Policy recommendations: 
  
Policymakers should finalize what constitutes acceptable and legitimate override functions and define applicable situations for activation. 

  
Policymakers shoulddeterminestandards for drivers  monitoring,training requirements, handover routines and external human-machine interfaces.These standards should be as global as possible. 

  
Policymakers in each jurisdiction should consider designating anorganization in each jurisdiction that is allowed tolookatthe code and data within the AVin the event of a fatal accident involving an AV or a corresponding legal proceeding. 



2. Technical robustness and safety 
Industry recommendations: 
  
Theprime requirement of AVs should be safety. 

  
In addition to threats that relate to conventional vehicles, manufacturers of AVs should particularly focus oncybersecurity threats.In doing so, companies need tocomply with regulations for cybersecurity management systems.SAE J3061 could serve as a guidelineto design cybersecurity into AVs throughout the entire development life cycle process. 

  
In terms of general functionality and safety, vehicles need to passanofficial test that assures the system s accuracy, reliability and adequacy of its fallback options. TheSAE DrivingSafety Performance Assessment Metricsand the IEEE P7009 standardcould serve as a baseline to design fail-safe mechanisms of autonomous and semi-autonomous systems. 


Policy recommendations: 
  
Regulations need to be developed that reflect consensus on the method by which to grantapproval to a vehicle s cybersecurity management system. 

  
Policymakers need to work with industry experts to develop astandardized test for thegeneral functionality and safetyof AVs toassure the system s accuracy, reliability and adequacy of its fallback options. This testcouldserve asabasis for the approval of AVs for sale to consumers.  

  
Policymakers needtocollaboratewithindustryexpertstodetermineand harmonizethedefinitionofa safe condition  / minimalriskcondition ,the corresponding  minimum risk maneuvers ,and thecircumstances in which such maneuvers should be executed. In doing so, SAE J3016could serve as a baseline. 



3. Privacy and data governance 
Industry recommendations: 
  
Manufacturers shouldfollow astrict privacy and data governance policythat includestransparencyandcommunicationtousers,requesting affirmative consentandallowing limitedsharingofdatawiththird parties(including governments). In doing so, companies should comply withapplicable standards and regulations such as the GDPR, the ePrivacy directive or the IEEE P7002. 

  
Before transmitting personal information from anAV to third parties, steps must be taken to ensure that it cannot be traced back to an individual. 


    Manufacturers should implement data protocols defining who canhave access to data under which conditions. 
Policy recommendations: 
  
Before receiving AV data, policymakers needtomake clearwhat types of AV datatheyare seekingand how thatdatawillenablethemtoimprove public safety orsome otherlegitimate public purpose(e.g., improve infrastructure, traffic flow and law compliance). 

  
At the EU level, building on Article 3(c) of Directive 2010/40/EU, consideration should be given to expanding the list of events and relevant traffic information that should be communicated free of charge. 



5. Diversity, non-discrimination and fairness 
Industry recommendations: 
  
Companies shouldtesttheirvehicle s AIsystemsfor unfairperformance differences across skintone, gender, ageandothercharacteristics.TheIEEE P7003 standardcan serve as a baseline to address and eliminate issues of bias in the creation of algorithms. 

  
When atechnologyislaunchedintothemarket, companies shouldlocalizeit using data and train the model using multiple diverse data sets that are location specific. 

  
TheAI developing teamshould be as inclusive as possibleto include the broadest group possible in terms of demographics such as ethnicity. 

  
Aresponsible balancingof risks and potential harmtoreduce the number of personal injuries shouldbepermittedfor AVs withoutdiscriminating against personal characteristics. Instead, factors underlying the balancing could include the typeorspeed of particular traffic participantsandthe impact angleunder which a collision would occur. 

  
Thepersonalizationof AVs should be accessible by designandasinclusive as possible(e.g., disabilitiesincluded). Before anAV is released ontothestreets, companies should demonstrate their plans and actions that ensure customizing optionsto their vehicles (e.g., possibility to take away seats or include a ramp for entering the vehicle with a wheelchair).       


Policy recommendations: 
  
Consideration should be given to having ethics standards boards test and assess 

  
Consideration should be given torequiring carmakers to explain the procedures they have put in place to make their designs accessible and avoid biasesbefore granting them authorization to sell their vehicles to the public. 


.that the systemsfor AVs are working properly, fairly and in an unbiased manner 

6. Societal and environmental wellbeing 
Industry recommendations: 
  
Whendeveloping theirproducts, automotive companiesshouldconsider integrating and providing benefits of increased public health and mobility, better traffic flow and decreased carbon emission. 

  
Manufacturers should offer by default an eco-driving modewith a speed average thatavoids unnecessaryaccelerationorbraking andthusreduces carbon emissions. 

  
Whendeveloping AVs, car manufacturers should try to integrate other disruptive technologies such as electrification and shared mobility. 


Policy recommendations: 
  
Policymakers shouldfollow astepwise implementation processand concentrate onmixed traffic scenarios. Policymakers shouldpromote the integration of AVs in existing transport systems instead of competitionbetween them, for example, by prioritizing research and development of AV solutions for public and shared mobility. 

  
A simultaneous adaptionof physical and digital infrastructureis essential (e.g., lanes that separate AVs from other traffic). 

  
In doing so,collaboration with multiple actorssuch as private-sector leaders and national agencies is key to fostering innovation and progress (e.g., make useof projects investigating differing mobility models). 



7. Accountability 
Industry recommendations: 
  
Manufacturers shouldcontinuously conductinternal audits(e.g., assessing potentialriskstothesafe operationofAVs) andsubsequentlyoptimizetheir systems. 

  
Manufacturers shouldbetransparent aboutthescopeandprocess oftheir internal audits and risk assessments(e.g., space of conditions that are checked for). 

  
Theinternalauditsshouldbecomplemented withregular externalauditsby independent test centers. 

  
Manufacturers shoulddevelop specific measures of transparency.This includes storingrecords and data of the underlying system logic(e.g., used training data sets) as well as logging mechanisms and black boxes(e.g., an event data recorder and data storage system) that document the actions of / in AVs during operation. The upcoming UN Regulation on Automated Lane Keeping Systemscan serve as 

a baseline for vehicle manufacturers to develop appropriate data storage systems for AVs. SAE J3197and the IEEE P7001 standardcanserve asabaselineto address requirements for transparency andaccountabilityofautonomous systems. 

  
Companies shouldtransparently communicateandreport performance and negative impactsof AVs (e.g., number of collisions, tradeoffs withinalgorithms). 


Policy recommendations: 
  
Regulators should adapt laws andregulations concerning AVs and liability as the technology continues to develop.Regulators should clarifywhere responsibility lies in certain situations and ensure thatprivacy and cybersecurity damages are taken into account. 

  
Policymakers should consider establishing test centersthat regularly request that companies perform conformity assessmentsandprovide certifications. 



References 
Archie, M., Gershon, S., Katcoff, A., & Zeng, A. (2018). De-anonymization of Netflix reviews using Amazon reviews. Retrieved from https://courses.csail.mit.edu/6.857/2018/project/Archie-Gershon Katchoff-Zeng-Netflix.pdf 
Autonomous Drivers Alliance (ADA) (2020). ADA Turing test. Retrieved from: https://ada.ngo/ ada-turing-test 
Awad, E., Dsouza, S., Kim, R., Schulz, J., Henrich, J., Shariff, A., Bonnefon, J.-F., & Rahwan, I. (2018). The moral machine experiment.Nature,563(7729), 59-64. 
Bartneck, C., L tge, C., Wagner, A., & Welsh, S. (2019). An introduction to ethics in robotics and AI. Cham, Switzerland: Springer. 
Boston Consulting Group (BCG) (2017). Making autonomous vehicles a reality: Lessons from Boston and beyond. Retrieved from https://www.bcg.com/de-de/publications/2017/automotive making-autonomous-vehicles-a-reality.aspx 
Boston Consulting Group (BCG) (2020). Can self-driving cars stop the urban mobility meltdown?. Retrieved from https://www.bcg.com/de-de/publications/2020/how-autonomous-vehicles-can benefit-urban-mobility 
California Public Utilities Commission (2020). Required reports TNCs must provide the CPUC. Retrieved from https://www.cpuc.ca.gov/General.aspx?id=3989 
ESTECO (2020). Driving change for autonomous vehicles. Retrieved from https://mcusercontent. com/e18919a10879a5f50c06081a5/files/fb6d6b93-86c0-4bec-9bb0-0161a0629e09/WhitePaper_ ADAS.pdf?utm_source=mailchimp&utm_campaign=0300efc2e1f0&utm_medium=page 
Eurocybcar (2019). Cybersecurity test for cars. Retrieved from https://eurocybcar.com/en/ 
Europawire (2019). TomTom part of EU Data Task Force s proof of concept to make roads in EU safer. Retrieved from https://news.europawire.eu/tomtom-part-of-eu-data-task-forces-proof-of concept-to-make-roads-in-eu-safer-20943847/eu-press-release/2019/06/05/11/18/39/73674/ 
European Commission (2013). REGULATION (EU) No 886/2013. Official Journal of the European Union. Retrieved from https://eur-lex.europa.eu/legal-content/EN/TXT/ PDF/?uri=CELEX:32013R0886&from=EN 
European Commission (2018). Liability for emerging digital technologies. Retrieved from https:// eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:52018SC0137&from=en 
European Commission (2019). Automated road transport   On the way to connected and automated mobility. Retrieved from https://ec.europa.eu/inea/sites/inea/files/art_brochure-2019.pdf 
European Commission (2020a). Ethics of connected and automated vehicles. Retrieved from https:// ec.europa.eu/info/sites/info/files/research_and_innovation/ethics_of_connected_and_automated_ vehicles_report.pdf 
European Commission (2020b). On artificial intelligence   A European approach to excellence and trust. Retrieved from https://ec.europa.eu/info/sites/info/files/commission-white-paper-artificial intelligence-feb2020_en.pdf 
European Cybersecurity Certification Group (ECCG) (2020). The EU cybersecurity certification framework. Retrieved from https://ec.europa.eu/digital-single-market/en/eu-cybersecurity certification-framework 
European Data Protection Board (EDPB) (2020). Guidelines 1/2020 on processing personal data in the context of connected vehicles and mobility related applications. Retrieved from https://edpb. europa.eu/sites/edpb/files/consultation/edpb_guidelines_202001_connectedvehicles.pdf 
European Union Agency for Cybersecurity (ENISA) (2019). ENISA good practices for security of mart cars. Retrieved from https://www.enisa.europa.eu/publications/smart-cars/at_download/ fullReport 
Federal Ministry of Transport and Digital Infrastructure (BMVI) (2015). Minimum risk manoeuvres. Retrieved from https://wiki.unece.org/download/attachments/27459841/ACSF-04 07%20%20%28D%29%20-%20ACSF-MRM.pdf?api=v2 
Federal Ministry of Transport and Digital Infrastructure (BMVI) (2017). Ethics Commission automated and connected driving. Retrieved from https://www.bmvi.de/SharedDocs/EN/publications/ report-ethics-commission.pdf?__blob=publicationFile 
Florentine, E., Ang, M. A., Pendleton, S. D., Andersen, H., & Ang Jr, M. H. (2016). Pedestrian notification methods in autonomous vehicles for multi-class mobility-on-demand service. In Proceedings of the Fourth International Conference on Human Agent Interaction (pp. 387-392). New York, NY: Association for Computing Machinery. 
Floridi, L., Cowls, J., Beltrametti, M., Chatila, R., Chazerand, P., Dignum, V., L tge, C., Madelin, R., Pagallo, U., Rossi, F., Schafer, B., Valcke, P. & Vayena, E. (2018). AI4People   An ethical framework for a good AI society: Opportunities, risks, principles, and recommendations. Minds and Machines, 28(4), 689-707. 
Forbes (2020). Overcoming racial bias in AI systems and startlingly even in AI self-driving cars. Retrieved from https://www.forbes.com/sites/lanceeliot/2020/01/04/overcoming-racial-bias-in-ai systems-and-startlingly-even-in-ai-self-driving-cars/#2b1cc433723b 
Fraunhofer Institute for Intelligent Analysis and Information Systems (IAIS) (2019). Trustworthy use of artificial intelligence: Priorities from a philosophical, ethical, legal, and technological viewpoint as a basis for certification of artificial intelligence. Retrieved from https://www.iais.fraunhofer.de/ content/dam/iais/KINRW/Whitepaper_Thrustworthy_AI.pdf 
Future of Privacy Forum (2017). Data and the connected car. Retrieved from https://fpf.org/wp content/uploads/2017/06/2017_0627-FPF-Connected-Car-Infographic-Version-1.0.pdf 
Geary, T., & Danks, D. (2019). Balancing the benefits of autonomous vehicles. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society (pp. 181-186). New York, NY: Association for Computing Machinery. 
Gei linger, M., Poszler, F., Betz, J., L tge, C., & Lienkamp, M. (2020). Autonomous driving ethics: From Trolley problem to ethics of risk. Working paper. 
Gogoll, J., & M ller, J. F. (2017). Autonomous cars: In favor of a mandatory ethics setting. Science and engineering ethics, 23(3), 681-700. 
High-level Expert Group on Artificial Intelligence (2019). Ethics guidelines for Trustworthy AI. Retrieved from https://ec.europa.eu/futurium/en/ai-alliance-consultation/guidelines#Top 
IEEE (2019). Ethically aligned design   A vision for prioritizing human well-being with autonomous and intelligent systems. Retrieved from https://standards.ieee.org/content/dam/ieee standards/standards/web/documents/other/ead1e.pdf 
Inframix (2020). Expected impact   a step by step introduction of automation. Retrieved from https://www.inframix.eu/expected-impact/ 
Joaquin Acosta, A. (2019). IoT international regulator challenges: The European approach. In C. H. Cwik, C. A. Suarez, & L. L. Thomson (Eds.), The internet of things (Iot): Legal issues, policy, and practical strategies (pp. 191-215). Chicago, IL: American Bar Association. 
Joaquin Acosta, A. (2018a). Autonomous vehicles: A smart move? 24 essentials of a SWOT analysis policymakers need to consider. Berkman Klein Center for Internet and Society at Harvard University. Retrieved from https://cyber.harvard.edu/sites/default/files/2018-07/2018-07_AVs02_0.pdf 
Joaquin Acosta, A. (2018b). Autonomous vehicles: 3 practical tools to help regulators develop better laws and policies. Berkman Klein Center for Internet and Society at Harvard University. Retrieved from https://cyber.harvard.edu/sites/default/files/2018-07/2018-07_AVs04_1.pdf 
Kochupillai, M., L tge, C., & Poszler, F. (forthcoming).  Programming away human rights and responsibilities? The Moral Machine Experiment and the need for a more 'Humane' AVs Future. NanoEthics. 
Kriebitz, A., & L tge, C. (2020). Artificial intelligence and human rights: A business ethical assessment. Business and Human Rights Journal, 5(1), 84-104. 
Leonhardt, T. (2018). Minimal risk maneuver. Retrieved from https://www.ko-haf.de/fileadmin/ user_upload/media/abschlusspraesentation/12_Ko-HAF_Minimal-Risk-Maneuver.pdf 
Lim, H. S. M., & Taeihagh, A. (2018). Autonomous vehicles for smart and sustainable cities: An in-depth exploration of privacy and cybersecurity implications.Energies,11(5), 1062. 
Lin, P. (2016). Why ethics matters for autonomous cars. In M. Maurer, J. C. Gerdes, B. Lenz, & H. Winner (Eds.) Autonomous driving (pp. 69-85). Berlin, Heidelberg, Germany: Springer. 
L tge, C. (2017). The German ethics code for automated and connected driving.Philosophy & Technology,30(4), 547-558. 
Manser, M. P., Noble, A. M., Machiani, S. G., Shortz, A., Klauer, S. G., Higgins, L., & Ahmadi, A. (2019). Driver training research and guidelines for automated vehicle technology. Retrieved from https://vtechworks.lib.vt.edu/bitstream/handle/10919/95178/01-004_Final%20Research%20Report_ Final.pdf?sequence=1 
Merat, N., Jamson, A. H., Lai, F. C., Daly, M., & Carsten, O. M. (2014). Transition to manual: Driver behaviour when resuming control from a highly automated vehicle. Transportation research part F: traffic psychology and behaviour, 27(Part B), 274-282. 
Ministry of Transport and Communications (1997). En route to a society with safe road traffic. Retrieved from https://trid.trb.org/View/512093 
National Transportation Safety Board (2019). Vehicle automation report, Tempe, AZ, HWY18MH010. Retrieved from https://www.documentcloud.org/documents/6540547-629713.html 
National Transportation Safety Board (2020). Collision between a sport utility vehicle operating with partial driving automation and a crash attenuator. Retrieved from https://www.ntsb.gov/ investigations/accidentreports/pages/har2001.aspx 
NHTSA (2017). Automated driving systems 2.0. Retrieved from https://www.nhtsa.gov/sites/nhtsa. dot.gov/files/documents/13069a-ads2.0_090617_v9a_tag.pdf 
Pagallo, U., Aurucci, P., Casanovas, P., Chatila, R., Chazerand, P., Dignum, V., L tge, C., Madelin, R., Schafer, B., & Valcke, P. (2019). AI4People on good AI governance: 14 priority actions, a SMART model of governance, and a regulatory toolbox. Retrieved from: https://ssrn.com/abstract=3486508 
Research and Markets (2019). Analysis of driver monitoring systems, 2020 Edition. Retrieved from https://www.researchandmarkets.com/reports/4893877/analysis-of-driver-monitoring-systems 2020?utm_source=dynamic&utm_medium=GNOM&utm_code=c968tk&utm_campaign=1338270+ +2020+Analysis+of+the+Global+Driver+Monitoring+Systems+Market&utm_exec=cari18gnomd 
Rouchitsas, A., & Alm, H. (2019). External human machine interfaces for autonomous vehicle-to pedestrian communication: A review of empirical work. Frontiers in psychology, 10(2757), 1-12. 
SAE International (2018). J3016   Taxonomy and definitions for terms related to driving automation systems for on-road motor vehicles. Retrieved from https://saemobilus.sae.org/content/ J3016_201806 
SAE International (2020). J3197   Automated driving system data logger. Retrieved from https:// www.sae.org/standards/content/j3197_202004/ 
SAE International   Vehicle Cybersecurity Systems Engineering Committee (2016). SAE J3061: Cybersecurity guidebook for cyber-physical vehicle systems. Retrieved from https://www.sae.org/ standards/content/j3061_201601/ 
Secretary of State Washington (2020). Certification of enrollment   engrossed substitute senate bill 6280. Retrieved from http://lawfilesext.leg.wa.gov/biennium/2019-20/Pdf/Bills/Senate%20Passed%20 Legislature/6280-S.PL.pdf?q=20200409103455 
Techcrunch (2006). AOL proudly releases massive amounts of private data. Retrieved from https:// techcrunch.com/2006/08/06/aol-proudly-releases-massive-amounts-of-user-search-data/ 
Tesla (2020). Using autopilot and full self-driving capability. Retrieved from https://www.tesla.com/ support/autopilot 
United Nations (1948). Universal Declaration of Human Rights. Retrieved from https://www.ohchr. org/EN/UDHR/Documents/UDHR_Translations/eng.pdf 
United Nations (2015). Transforming our world: The 2030 agenda for sustainable development. Retrieved from https://www.un.org/ga/search/view_doc.asp?symbol=A/RES/70/1&Lang=E 
United Nations Economic Commission for Europe (UNECE) (2019). Revised framework document on automated/autonomous vehicles. Retrieved from https://www.unece.org/fileadmin/DAM/trans/ doc/2020/wp29/ECE-TRANS-WP29-2019-34-Rev2e.pdf 
United Nations Economic Commission for Europe (UNECE) (2020a). Draft new UN regulation on uniform provisions concerning the approval of vehicles with regard to cyber security and of their cybersecurity management systems. Retrieved from https://www.unece.org/fileadmin/DAM/trans/ doc/2020/wp29grva/GRVA-06-19r1e.pdf 
United Nations Economic Commission for Europe (UNECE) (2020b). UN regulation onautomated lane keeping systems is milestone for safe introduction of .automated vehicles in traffic. Retrieved from https://www.unece.org/info/media/presscurrent-press-h/transport/2020/un-regulation-on automated-lane-keeping-systems-is-milestone-for-safe-introduction-of-automated-vehicles-in-traffic/ doc.html 
Vox (2019). A new study finds a potential risk with self-driving cars: Failure to detect dark-skinned pedestrians. Retrieved from https://www.vox.com/future-perfect/2019/3/5/18251924/self-driving car-racial-bias-study-autonomous-vehicle-dark-skin 
Wilson, B., Hoffman, J., & Morgenstern, J. (2019). Predictive inequity in object detection. arXiv preprint arXiv:1902.11097. 
Wishart, J., Como, S., Elli, M., Russo, B., Weast, J., Altekar, N., James, E., & Chen, Y. (2020). Driving safety performance assessment metrics for ADS-equipped vehicles. SAE International. Retrieved from https://www.researchgate.net/profile/Jeffrey_Wishart/publication/340652968_Driving_Safety_ Performance_Assessment_Metrics_for_ADS-Equipped_Vehicles/links/5eb0a39e92851cb2677403ba/ Driving-Safety-Performance-Assessment-Metrics-for-ADS-Equipped-Vehicles.pdf 
World Economic Forum (WEF) (2018). Reshaping urban mobility with autonomous vehicles   Lessons from the City of Boston. Retrieved from http://www3.weforum.org/docs/WEF_Reshaping_ Urban_Mobility_with_Autonomous_Vehicles_2018.pdf 
2 



BANKING & FINANCE 
Authors 
Nir Vulkan 
Chairman Banking & Finance Committee, AI4People; Associate Professor of Business Economics at Sa d Business School, University of Oxford, UK 

Aisha Naseer 
AI Ethics Research Manager at Fujitsu Laboratories of Europe 

Frank McGroarty 
Chairman Insurance Committee, AI4People; Professor of Computational Finance and Investment Analytics; Director of Centre for Digital Finance at Southampton Business School, UK 

Giulia Del Gamba 
Digital and Innovation Policy Advisor at Intesa Sanpaolo 

John Cooke 
Chairman of the Liberalisation of Trade in Services Committee at TheCityUK 

Lampros Stergioulas 
Professor in Business Analytics at the University of Surrey, UK 

Paul Jorion 
Associate Professor of Ethics, Universit  Catholique de Lille, France 
RaffaellaDonini 
Senior Manager, European Digital and Innovation policies at Intesa Sanpaolo 



1. Executive Summary 
AItechnologyhashadanenormousimpactontheBanking&Finance sector (B&F from hereon in). Moreover, thistrend is only likely tocontinue. Already most credit check, KYC and AML decisions are now made by algorithms. Credit scores and therefore credit worthiness tests are also hugely faster and more accurate when carried outby algorithms trained onlargedatasetsbased onpastdecisionsandoutcomes (Khandanietal., 2010). Similarly, investing and trading continue tobe disrupted by AI-in some markets more trade happens as results of orders put in by algorithms than by humans.Theso-called"FinTech Revolution" which hasdisruptedandforced a rethink of existing paradigms within B&F, is strongly assisted by AI technologies (Noya, 2019); for example, many firms useAItocombat fraud (Stripe, 2020; Amazon Web Services, 2020; Chatfield, 2017; Thanendran, 2018). However, themassadoption of AIover the past ten years has also brought up important concerns.When using these technologies, institutions now have to ask themselves fundamental questions  are the algorithmswe are using fair and transparent? Are ourcustomers adequately protected from risk? Is ourusers  data safe? Are these approaches sustainable? And crucially, is using AI worth it? 
Thiscommitteebelieves thatAIhasthepotentialtoeffect agreat amountof positive change within this important sector. More specifically, we believe AIcanhelp address three of the mostimportant challenges faced by finance nowadays: financial inclusion, financial literacy and financial wellbeing. Additionally, if AI is used responsibly, it can lead to higher revenue growth, cost efficiencies and a better customer experience. Conversely, poor useof AI may lead individuals to be less engaged with their finances, may propagate further discrimination and may foster exclusion. At the level of afirm, AI may be hugely cost inefficient or it may negatively impact their operational resilience if it is not deployed correctly, safely and efficiently. 
We notethatfinanceisalready themostregulated area ofindustry. Inmany sectors of financial services, regulatory objectives are now set by global standard-setters 
(e.g. the Basel Committee), leaving precise implementation to supervisors in individual jurisdictions.Thismeansthatmuchregulation (whether prudentialregulation or market conduct regulation) is already highly developed and potentially all-embracing. Thus,we believe that thedevelopment of wholly newregulation pertaining to the use ofAIwithinfinancial services willbeunlikely. Indeed, there islikely tobeexisting regulation, which in principle is technologically neutral. Instead, we believe the crucial question is how existingregulation (or the skills of regulators and supervisors) should be adapted to cater for the enhanced role of AI in previously existing modes of delivery of financial services. This ought tomeanthat further whole layers ofregulation need not be added. 
Thiscouldwell prove fortunate, because addingtoomany layers ofregulation would have anti-competitive effects, further increasing the barriers toentry for start upsand resulting in unfair advantages accruing to big banks and tech giants who are now enteringthis space. Like many innovative areas in technology, AI will need to be fair, safe anduser-friendly if it is to gain massadoption. Part of this step will involve harmonisedstandards that,ataminimum,offer guidanceandconsumerprotection and, ideally, provide aregulatory framework. When considering the structure of this framework, regulators may want to consider leveraging and adapting existing regulatory solutions,tohelp these newtechnologiestobe used fairly and safely. However, new categories of risk may also emerge, in which casetargetedregulatory remedies should beavailable in order to protect consumers, encourage healthy competition, and ensure market stability. Regulators oftenfavour aprincipled, outcome-basedapproach for regulating fast paced and quickly innovating areas -with this, developed frameworks are adaptable and canevolve over time. Alternatively, given the huge range of variables andusecasesfor AI, it may equally be sensible toadaptarisk-based approach and consider the issues that arise on a case by case basis. 
For thisreason, in this document we consider theimpact of AI technologies on the B&F sector in light of the seven key requirements for Trustworthy AI laid down in the Ethics Guidelines (High-Level Expert Group onArtificial Intelligence, 2019), the six key features provided by the European Commission White Paper onAI (European Commission, 2020), aswell asthe European Parliament Framework of ethical aspects of artificial intelligence, robotics and related technologies (European Parliament, 2020). Given existing regulation (for example, in Europe, the Markets in Financial Instruments Directive andRegulation, theGeneralDataProtection Regulation, ortheCapital Requirements Directive) we believe thatthefollowing five requirements shouldbe addressed as priorities within the B&F sector: 
1) Fairness and non-discrimination 
2) Technical robustness and safety 
3) Transparency and explainability 
4) Accountability 
5) Human oversight 
These five principles are ourmain points of focus. Theremaining principles may be important for other industries, orfor AI in general, but we concentrateonthe five above, as we believe they are the most applicable to our setting. 
We begin by providing ageneraloverview of AI and its role in the B&F sector. Thenfor eachofthefive principles we provide usecases,review theresearch and 
literature produced by academia,banksandregulators, before supplying our recommendations. Finally, we summarisetherecommendations bothfor firmsand regulators. 
A key issue throughout this document is that principles such asfairness, accountability etcdonothave auniversally agreed interpretation inthecontextofB&F. Inother words, they are contestable. This document highlights some ideas and suggestions as to how to address this problem. A very useful comparison canbe drawn between existing industrial ethics frameworks, and new AI ethical frameworks for that same industry. In particular, inthe contextof the ethics of AI in medicine, (Mittelstadt, 2019) argues thatwhilst emergingframeworks are seemingly alignedwithtraditionalprinciples, language is being used that  hide[s] deep political and normative disagreement . 
2.Overviewofaianditsroleinbankingandfinance 
Artificial Intelligence isabroad termwhich captures arange of technologies. As such, it is somewhat difficult to pin anexact definitiononit. In (Hofstadter, 1979), the authorquipsthat AI iswhatever hasn't beendone yet. For ourpurposes,we can understand AI tomeanthose technologies which emulate those humancapabilities we value, but which we traditionally understand to be beyond the reach of computing devices. Thisincludesactivitiessuchasunderstanding the meaning  ofpictures, videosand audio, rather thanjust treating them asdigital signals, aswell asproblem solving and reasoning about unseensituations. Often, we like to think of AI as the study of intelligent agents   software we candelegate tasks to in order to achieve our goals. 
AI has co-evolved with the inceptionof computing inthe 1940s, and has beenan active area ofinterest throughout the20thand 21stcenturies. Since 2011,however, there has been anexplosionof both practical results, aswell aspopular interest, inthe subject. This is largely due to one strand of AI becoming increasingly prevalent  Machine Learning (ML). Whilst the techniques of ML have existed since the 1980s, they only really cameinto their ownwhenresearchers showed how to efficiently implement them onGraphics Processing Units  relatively inexpensive, commodity hardware. Since then, AI has touched almost every industry and now affects us allonaday-to-day basis  B&F is no exception. B&F is a vast, complicated industry. Inthe UK, for example, the financial services industry is responsible for almost 7% of economic output (Rhodes, 2019). 
Banking and finance differ from other industries covered by the other AI4People panels, inthat the different parts of the sector, for example trading, are themselves trillion-dollar industries (Pound, 2019). The sectoris already highly regulated, especially after the 2007-2008 financial crisis. This increased regulationcarries significant costs for the industry, with B&F firms spending considerable amount of their resources oncompliance. 
AI is already being used inmany areas across B&F, including algorithmic trading, robo-advisors, fraud detectionand automated loan decisions, and has the potential to effect furthertransformation inbanking andfinance. TheB&Fsectorhasunique characteristicswhich may make theuseofAIdifficulttoregulate: inparticular, the sector s need to focus on risk management should always be held in mind whenreviewing new technologies. Furthermore, the industry is often held tohigher societal standards thanotherindustries, especially since the2007-2008financial crisis.It isimportant regulators get the balance right betweenregulationand innovation, whilst reassuring the public that the institutions taking care of their money are safe. 
It is illustrative to study the growth of FinTech, having disrupted traditional financial services, and largely assisted by AI technology. Figure 1 below (Vulkan, 2019a) represents thefunctional framework for understanding finance and illustratestherelationship betweenthe services provided by financial institutions and the systems andstructures that form the foundation for these services. Inthe UK asurvey by the Bank of England and FCA onmachine learning in financial 

Figure 1: A functional view of the B&F sector 
services found that two thirds of respondents already used machine learning insome form (Bank of England, and Financial Conduct Authority, 2019). Officials onWall Street plan touseartificialintelligence systems and machinelearning tomonitor thestock markets and predict patterns of fraud (Reuters, 2016). The potential of AI should not be underestimated  according tosomeresearch the useof AI across theeconomy could boost the UK s labour productivity by 25% by 2035 and add  650bn to UK gross value added (GVA) (International Regulatory Strategy Group, and Accenture, 2019). 
Figure 2below (Pinsent Masons, and Innovate Finance, 2019)illustratessome areas where AI is already being used inthe sector. Taking fraud detectionasanexample, it is clear that AI canbe much more efficient than humans. ML technologies canbe used toanalyse largevolumes of data and detect irregular patterns much faster thanaperson could. As such, AI makes it substantially easier for officials to examine largeamounts of potentially suspicious data and patterns (OECD, 2017). 

Figure2:CommonusesofAIinthefinancialservicessector 
In(International Regulatory Strategy Group, and Accenture, 2019), the authors list the key benefits of AI for industry participants and consumers as: higher revenue growth, increased cost savings, improved customer experience and better risk mitigation. However, AI is not without its risks. Inthe next section, we will explore the potential dangers of AI, discuss their mitigations and provide recommendations for both industry and policymakers on how AI can be Trustworthy within the B&F industry. 
Before proceeding, we note that the B&F sector is not isolated from other sectors of industry  there are deepconnections betweenB&Fand the remaining six sectors outlined by AI4People. Thus, whentalking about risks, mitigations and recommendations for B&F, it is important to be cognisant of how these elements link with corresponding ideasinothersectors.Inparticular, giventheporous barrierswithinthecollective ecosystem of sectors, when we make recommendations, we should consider their impact not just on B&F, but on industry as a whole. 

3.Analysis and Recommendations 
3.1. Fairness and non-discrimination 
Fairness withinartificial intelligence isanactive area ofresearch, and there are many proposals both for detecting/defining fairness (Sahil et al., 2018) and for mitigating discrimination bias (Mehrabi et al., 2019) in machine learning models. 
Defining  fairness  initself is difficult, since there isnot aunique intuitive notion ofwhat fairness indecision making should be. Inthe context of B&F, what is fair is still very much uncertain. Proposals have been put forward to measure  average  discrimination betweengroups of people (e.g. the different percentage of loans granted to womenand tomen), and alsotoprovide individualnotions offairness (two similarindividuals should be giventhesamedecision). Both of the above notions of fairness (i.e. group fairness and individual fairness) canbe further broken down into aplethora of different concepts, which are, ingeneral, not mutually compatible (Kleinberg, 2016). Additionally, different mitigation techniques have beendeveloped inorder to build models that meet specific fairness requirements. 
Thus,alotofwork hasalready beendone inthelastfew years tounderstand discriminationinmachine learning, measure it and mitigate its effects. However, caution must be takeninorder to understand what notionof fairness is most suitable for the use caseinquestion. This problem goes beyond the technical implementationof tools and procedures, and assuch, calls for jointcontributions from AI specialists, legal experts, firms, governments and even philosophers. 
Fairness and non-discriminationare universally considered to be key requirements for AI systems. The aim of the non-discriminationprinciple is to allow all individuals an equaland fair prospect to accessopportunities available in asociety. Individuals who are insimilarsituations shouldreceive similartreatment andshouldnot betreated less favourably due to their protected characteristics. Indirect discriminationis present when certaincharacteristic orfactoroccursmore frequently inthe populationgroups against whom it is unlawful to discriminate. Since algorithmic decision-making systems may be basedoncorrelations, there isarisk to perpetuate orexacerbate indirect discrimination through stereotyping, whendifferential treatment cannot be justified (Council of Europe, Committee of experts onInternet MSI-NET, 2017). Financial data is prone to bias and imbalance (Zhang and Longsheng, 2019) and putting fairness principles into practice in industrial processes isanopenissue. In(Saxena, 2019), the authors conducted online experiments to examine people's perceptions around fairness definitions and found that one specificdefinition, calibratedfairness,tended tobepreferred over two other possibilities.However, theseexperimentswere conductedsolely around one problem (loandecisions) and cannot betakenasthefinal word onthematter. Additionally, (Greenand Chen, 2019) explored the algorithm-in-the-loop paradigm, and demonstrated inanexperiment how humanpredictions, aided by a risk assessment algorithm, exhibited a great deal of bias. 
Bias 
Shortly after Apple released its own credit card in 2019, a number of people reported that women were receiving lower credit limits than men 
withsimilarfinancialbackgrounds(Knight,2019).GoldmanSachs,the 
issuing bank, claimed that a third party had audited the credit algorithm for bias and stated that it did not depend on protected attributes such as 
race,ageorsex,butonlyontheir creditworthiness .However,itisclear 
that a person s sex can be inferred from their past transactions, and as such, their algorithm may have indeed (accidentally) learned to discriminate against women. 
Even if a team of developers have the best intentions, bias can easily slip into a model. This can happen in a multitude of ways - from how the data was collected and aggregated, to an oversight of a developer, to the algorithm used in the training of the model itself. 
How can we eliminate such bias from our models, when we may not even 
beawarethatweareintroducingitinthefirstplace?Oneappealing 
approach is that of Counterfactual Fairness (Kusner et al., 2017). Suppose your algorithm has made a prediction, and you want to determine if that 
predictionis fair withregardtosomeprotectedattribute.Counterfactual 
Fairness asks you to imagine a parallel world, where that attribute has 
beenchangedandtoseewhetheryouralgorithmmakesadifferent 
prediction. Far from being an abstract thought experiment, (Kusner et al., 2017) actually introduced a debiasing algorithm to formally implement 
thisnotion,anditofferspromisingresultsintermsoflargelyretaining 
accuracy of the original algorithm, whilst minimising unfairness. 
However, Counterfactual Fairness is only but one method for removing bias from the decision making process - for a survey on bias and techniques for its removal, (Mehrabi et al., 2019) is a good reference. 
Despiteitsappeal,CounterfactualFairnesscomesnotwithoutflaws. 
Among others, the fact that it requires very strong assumptions about the causal relationships among variables at play, some of which are not even 

falsifiable.Forthisreason,theideaitselfof counterfactuals isstillhighly debatedbythescientificcommunity. 
Trying toeliminate bias inalgorithms is currently anart, rather thanascience. To this end, there are a number of routes one cantake. Whilst fairness is not a universally agreed upon concept, there are a number of metrics which have beendeveloped in order to judge whether analgorithmisacting fairly  ornot. Whendatascientists are developing algorithmsand training models, they shouldcalculatethesemetrics,toprovide some insight as to whether they are being unfairly discriminatory in any way. 
Recommendations: 
  
Instead of trying to entirely eliminate bias, one should learn to manage it instead. This includes being able to identify the potential for bias in models and datasets, as well as understanding the types of algorithms that can mitigate its effect. 

  
Documentandpublishanethicalcodeofconductpolicytopromote non discrimination principles (CSSF, 2018); 

  
Include bias assessment and mitigation into the AI project pipeline. This canbe done, e.g., by meansof additional steps such asin (Castelnovo etal., 2020): a careful exploration and understanding of the problem at hand and of the available data inorder to identify the possible unfair impact on the user; monitor different metrics of fairness along the train, validation and test phases; implement, when necessary, available strategies in order to mitigate the bias; evaluate results along multiple dimensions and compare the implemented strategies; re-think someof the steps or iterate the process, when necessary; 

  
Constantly monitor the performance of the model notonly overall but also at the level of potentially discriminated groups; 

  
Since trade-offs between fairness and fidelity (or accuracy) still persist, which ML model should be used for the problem in question significantly depends on the context and its business domain. 


3.2. Technical Robustness and Safety 
As alluded to inthe previous section, minimising loss and maximising accuracy is not thebe-alland end-allofML.There isaclassic(probably apocryphal) example (Branwen, 2011) that illustrates this point nicely  allegedly anAmerican government agency wanted to train a neural network to differentiate friendly tanks from enemy tanks. They followed the basic recipe for machine learning, taking care toseparate their data into trainand test sets, before running the training algorithm onthe dataset. The model was a huge success  it could successfully differentiate between friendly and enemy tanks with close to 100% accuracy. However, when they decided to further test the model with new photos, it was useless  seemingly performing no better thanrandom chance. It then emerged that the pictures of friendly tanks had been takenonasunny day, whilst the pictures ofenemy tanks had beentakenonacloudy day. The model had simply learned how to tell how bright a photo was. 
Whilst the above storymay only contain agrainof truth, it does encapsulate some important points. Primarily, ML isn t just anoptimisation game  it is a game of balance. There isavast difference betweenamodelwhich performs well indevelopment, and a modelwhich performs well inproduction. There are two key ways inwhich we cantry and ensure that an ML deployment is a successful one. First, a model should be statistically sound.Thatmeans evaluating itwithrespect toarange ofmetrics,ratherthanjust picking and choosing the ones that make it look like it is performing well. Inparticular, thenumberoftrue/false positives/negatives ofamodelshouldbeclosely analysed. Additionally, before eventraining amodel, the underlying data mustbe hand-analysed and understood, toensure theengineers appreciate any  quirks  inthe data. Note that this may require the use of domain-specific experts. Second, there are many regularisation techniques which canbeusedtoimprove modelrobustness. There are fartoomany individual techniques to list here, but resources such as(Goodfellow et al., 2016) offer a good coverage of the most important techniques. 
Building on this point, as AI is software, we stress that the techniques and frameworks from traditional software testing should firmly remain one of the core steps inany AI deployment. This includes all the usual exercises of unit testing, integrationtesting and acceptance testing  these activities provide assurance that the system not only works, but functions asacohesive whole. However, inthe contextof ML, we canprovide further assurance through the subfield of neural network verification. Whilst this field is relatively new, anumber ofdifferent approaches have beensuggested,many ofwhich seem promising. Again, we believe it is up to regulators to track the progress within this field and determine whether any of these methods are appropriate and/or necessary for the subsector in question. 
Finally, itisworth touching ontwo more topics-modeldata leakage and adversarial attack vectors. 
It has been shown (Fredrikson et al., 2015; Song et al., 2017; Carlini et al., 2019) that ML models are capable of  leaking  their training data. That is, givenapretrained model, they show how examples of the training data canbe reverse-engineered from it in certaincircumstances. This has the damaging potential toleak private andconfidential client information. As such, engineers should be aware of how to minimise this possibility. One emerging, promising field is that of differential privacy (Ji et al., 2014). This tries toensure that the informationfrom the model inquestiondoes not change significantly ifonetraining example is added orremoved. This has the knock-on effect of it being more difficulttoextractthe data of individuals from the model. As ML becomes more prevalent and firms contract third-parties totrainmodels for them, ideas like this will become increasingly important. As such, it is crucial that both engineers and regulators track the development of ideas inthis subfield closely, inanattempttomitigate data leakage. 
Adversarial data (Szegedy et al., 2014;Goodfellow et al., 2015) consists of examples which look normal tohumans, butwhich neural networks consistently classify incorrectly. For instance, (Goodfellow et al., 2015) create apicture ofapanda, which is consistently classified asagibbon. Raising the stakes, (Sitawarinet al., 2018) show how road signs canbe subtly modified to trick autonomous vehicles into following the wrong traffic rules, whilst (Sharif etal., 2016) show how tomodifycommoneye-glasses to impersonateanotherindividual from theviewpointofafacialrecognitionalgorithm. Perhaps evenmore surprisingly, (Hendrycks etal., 2019) provide natural (i.e.non-engineered) images which are repeatedly classified incorrectly. Thankfully, work has beendone which aims to mitigate the effect of such attacks (Madry et al., 2017) and we believe engineers should be aware of and know how to implement such measures. 
Recommendations: 
Engineers should be encouraged to take ontraining instatistical analysis of models and the datasets   in particular statistical verification and the risks of overfitting; 
  
Traditional software testing should remain acritical part of any AI deployment andnewideas from neuralnetwork verification should also be integrated into the process; 

  
Engineers should be aware of model data  leakage , aswell asadversarial attack vectors andreceive training inthelatestnews inthisarea andlearnhow to mitigate such risks (as much as possible). 




3.3. Transparency and explainability 
Another crucial line of thought from (High-Level Expert Group onArtificial Intelligence, 2019) is the following - Users should be able to make informed autonomous decisions regarding AI systems. They should be given the knowledge and tools to comprehend and interact with AI systems to a satisfactory degree and, where possible, be enabled to reasonably self-assess or challenge the system . Herein liestwo important points lettingusersunderstand the extent of their interactionwith AI systems and providing engineers and regulators with the training to build, regulate, and understand these systems inan appropriate way. 
We believe it is important for humans toknow whether theyare interacting with an algorithmoranother person. If a client or customer is talking to achatbot,say, then said chatbot should make clear at the start of the interactionthat it is not human. Equally, if adecisionhas beentaken with the help of anAI algorithm, this informationshould be disclosed during the process. This provides two core benefits  it preserves and reinforces therole of human autonomy in ourinteractionwith AI systems, and also gives humans the capacity to challenge decisions made by AI. This helps emphasise that AI is simply a tool inany business process, rather thananunchecked, autonomous agent. We will touch on another side of this same coin, explainability, when we come to talk about fairness. 
Explainability 
Historic approaches to AI (pre-1990) largely relied on symbolic, logic 
basedtechniques.Akeyexampleofthisisthatof expertsystems -these encapsulateddomain-specificknowledgeanduseddeductionrulestoform 
conclusions. For example, if you have the rule  If a patient has a high 
temperatureandacough,thenthereisan80%chancetheyhavetheflu , 
along with the information that patient A has a high temperature, along with a cough, then we can conclude that patient A has an 80% chance of 
havingtheflu.Expertsystemswouldhavethousandsoftheseruleswhich 
could be used in succession to reach advanced conclusions. Such systems 
offeredimmediateexplainability ifadecisionorpredictionhadbeen 
made and you wanted to understand where it came from, you can simply trace back the rules used to make such a decision/prediction until you have reached the desired level of granularity of understanding. Whilst expert systems are not used widely nowadays (they are labour intensive to create, 
requiretheinputfromdomain-specificexperts,andwereneverhugely successful),theirinfluencecanbeseeninmodernknowledge-based 
systems, such as Google s Knowledge Graph, IBM s Watson, as well as in personal AI assistants such as Siri, Cortana and Alexa. 
Conversely,modernArtificialIntelligencelargelyusesmethodsrootedin 
Machine Learning and statistics, rather than logic. In particular, one of the major successes of AI in the 21st century has been the explosion of deep 
learningmethods.Thesetechniquesarealarminglyeffectiveatcapturing 
domain knowledge, by learning patterns in training data. However, this 
 knowledge isencapsulatedinmany layers of neurons ,andtheir individualbehaviourisdefinedbyanarrayofnumberscalledthe  parameters oftheneuron.Stateoftheartmodelsoftenhavemillionsand 
billions of parameters in total. As such, it is almost impossible to ask questions like  what does the number -0.29 mean in neuron 47 in layer 6 
ofthemodel? -itisdifficulttounderstandthecontextofindividual parameters.Putdifferently,whilstmachinelearningdoeseffectively 
capture intuition and knowledge within a domain, it is hard to extract this knowledge in a human-understandable format   they are black-box models. 
Two recently proposed but already popular model-agnostic explainability tools are LIME (Local Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations). While their internal mechanisms 
aredifferent,thegeneralideaisthesame:buildingsurrogatemodels 
capturing the changes in the prediction given the changes in the input. They perform a sensitivity analysis by tweaking the input slightly and then testing the resulting changes in prediction. For example, if the model prediction does not change much by tweaking the value of a variable, that variable for that particular data point is not likely to be an important predictor. Both these tools provide local explanations only, namely they 
areabletoprovideinsightfortheoutcomerelativetospecificobservations, 
rather than giving a global rationale of the model functioning. However, LIME and SHAP are not the end of the story - there are a number of other 
methodsthatcanbeused,andthefieldofexplainableAI(XAI)israpidly 
developing. For more information, an excellent reference for current trends in Explainable AI is (Arrieta et al., 2020). 
Explainability in AI will become increasingly more important as it enters 
more and more facets of our daily lives. In particular, explainability provides transparency, promotes fairness and aids auditability. Without explainability methods, these three goals will be far harder to achieve. 
Whilst naturally engineers and regulators will comeinto their roles with acertain setof skills, we argue that each group needs this settobe augmented toreason about ethicalAIeffectively. Engineers may understand how toefficiently trainamodeland increase its accuracy; however, this is only oneside of the story -accuracycanbeapoor metric to measure performance with1,and the model may be actively discriminatory, for instance. It is crucial that engineers understand the statistical implications ofwhat they are doing. Moreover, due to the issues of bias raised earlier inthe document, we believe engineers shouldreceive specific training inethical AI and how toimplement it.The exacttraining they should receive is up for debate, but should align with the material covered in(High-Level Expert Group onArtificial Intelligence, 2019) and should cover considerations suchasfairness, accountability and transparency (Lepri, 2018). Second, we believe that given the complicated, fast-moving nature of AI as a technology, regulators shouldreceive somesort of basic technical training, sotheycanappropriately judge and evaluate the systems theyneed toregulate. Whilst this training does not need tobeas detailed as anengineer s training, it should be comprehensive enough that they are aware of the ethical issues that may arise during an engineer s implementation of a system. 
Finally, we take inspirationfrom existing regulatory methods.For instance, in Norway, the data protection authority, Datatilsynet, has established aregulatory sandbox for developing AI solutions (IAPP, 2020), where industry works closely with regulators and policymakers toshape policy, while retaining the capacity torapidly innovate. We believe this helps strike a good balance betweeninnovation and regulation, allowing firms to move fast, whilst still remaining safe. For new applications and use cases, AI sandboxes offer anatural route for exploring theseideas,whilst stilloffering thepotential for aggressive expansion. Another idea is to use escrow arrangements: while there is a growing practice within academia that the code for machine learning models is released along withthescientific paper, inacommercial environment, thismay not beinafirm s interest. Thus, to provide assurances thatanalgorithm has not been tampered with, an escrow service could be used to hold acopy of the original dataset and algorithm and be verified by athird partyservice.Ifthere were any doubtsastowhether theoriginal algorithmhadbeentampered with,theescrow servicecouldfeed aninput into their version of the algorithm to show it produces the sameresults. This helps build trust in commercial ML systems, whilst still protecting corporate intellectual property. 
1 Suppose we want to determine if a user has a fraudulent transaction on their account. Most transactions on a given person s accountare legitimate,soifregardless of the data, we classifyevery transaction asnon-fraudulent, then this model will have a high accuracy, but is effectively useless for the purpose of identifying fraud.  
Recommendations 
  
Arecommendation could be toembed certain capabilities orfeatures in the AI system to allow it to be self-explanatory. In this way, the system would be able totell the regulator, through specific mechanisms, why acertain decision has occurred in order to trace such decision; 

  
Regulators and engineersshould be trained in explainability, and should ensure that appropriate methods are applied before deploying a system into production. Such methods are newand still in development but look very promising. We recommend that regulators in particular should be continuously informed by the latest developments in this field (for example, by regularly attending relevant conferences); 

  
Thebestway toensure AIfirmscomeupwithmodelsthatare compliant is through them working with regulators asearly aspossible. The sandbox scheme which began in the UK with Fintech and is now being extended in some countries tospecifically focus onAI (e.g. Norwegian AI Sandbox that began few months ago is agood example) is particularly efficient in achieving this goal. We highly recommend expansion of this scheme; 

  
Institutions should implement measures toensure explainability of their AI/ML systems from the design phase. Even when full transparency cannot be achieved due to the intrinsic nature of the algorithm employed (e.g. deep neural networks), stepscanbe taken to identify and isolate in ahuman understandable format the main factors contributing to the final decision; 


  Thoroughly document the development process leadingtoconstruction of AI/ ML model since the design phase with assumptions and choices made ateach step, for example:   Documenttheblueprintofthedatapreparation flow usedtobuildthe inputfeatures, includingtransformations applied ontheraw data(e.g. normalization, dimensionality reduction, exclusion of correlated features, aggregation, etc.), andtheanalysis performed tocheckthefeatures  importance. Indeed, theinformation aboutwhich are themaininput features influencing the model is already providing a first basic explanation of the model behaviour;   Document the algorithms assessed and the comparative results justifying the selectionmade, including the analysis done onthe model to ensure it isfit,thevalidation methodologyemployed andtheresults obtained. Indeed, different algorithms have different degrees of inherent complexity and the choice of the algorithm directly influences the model explainability; 
  Document the metrics used to monitor model performance and promptly identify deviations. 
  
Keep themodelassimple aspossible:very oftenasimple modelwithan appropriate data pre-processing is as efficient and accurate as a very complicated one. This has the two-fold advantage of having a more transparent model and of fostering a deeper understanding of the dataset by the developers; 

  
When appropriate, embed interpreters into the model design to ensure traceability of the main internal steps leading to the final prediction: 


  According tothe criticality of the underlying usecaseand the need for 
transparency, evaluate thepossibilitytoimplement explainability 
techniques; 
  Finally, ensure that the solution implemented in production is auditable, sinceauditlogscanhelptounderstandhow dataisprocessed (CSSF, 2018). 

3.4. Accountability 
Regulationis fundamentally a humanactivity. Regulators are humans -appointed to posts inorganisations that operate onthe basis of independent, non-political decision-taking and that are answerable to the executive orlegislative armsof government asdictated by their governing law. Infulfilling their responsibilities, regulators interact with individuals inbanking and finance institutions onabasis of trust, fortified by confidence that those individuals are  fit and proper  directors and controllers of their institutions, meeting tests of probity, competence, and financial standing. However, many B&F may rely onAI to aid decisions and operate their firms, the rationale for those decisions and operations mustultimately beexplicable inassurances ofregulatory compliance, i.e.assurances, expressed innon-AI language, that humanbeings (directors and controllers of firms) can provide to other human beings (prudential regulators). Thus, we believe it is important that there is a clear chainof responsibility and accountability whenever AI is used within aninstitution if an algorithm is making decisions onbehalf of a firm, thenthere should besomeone who is accountable for its behaviour. This aids auditability and provides a first point of contact should any queries regarding the system arise. Another optionfor helping ensure accountability isby introducing afirm-specific framework for how AI should be used internally. Some firms have experimented with this idea one example is that of the Fair Banking Framework (Castelnovo et al., 2020)   but just like sets of principles for ethical AI, there isno universally adopted standard yet. 
Recommendations 
  
A clear chain of accountability should be established wherever AI is deployed, assigningapersonresponsible toeach instance of analgorithm used within a firm.Allexecutives overseeing thedepartmentswhere thesemodelsare used should receive training so they understand the implications of their deployment; 

  
Institutions should assume clear responsibility and accountability for the actions 

and decisions taken by automated AI systems and processes. Ultimate responsibility should lie with the senior management of the institution which integrates the AI logic into its business processes. Whenever off-the shelf packages are acquired, clearliabilityprovisions shouldbedefinedatcontractuallevel. Furthermore, clearroles andresponsibilities shouldbedefinedalongalltheAIlifecycle, includingthedevelopment andoperationsactivities,toensure continuous engagement and accountability (CSSF, 2018); 

  
Introduce aninternal code-of-conduct or framework for implementing AI within the firm 



3.5 Human oversight 
Aswe stressed before, it is crucial we view AI technologies asaset of tools to be used by humans, rather thanassolutionstoreplace them. As such, the humanelement should be regarded asacrucial part of any AI productiondeployment, rather than asan afterthought. As elucidated in(High-Level Expert Group on Artificial Intelligence, 2019), which emphasises that AI systems shouldsupporthumanautonomy anddecision-making, as prescribed by the principle of respect for humanautonomy. This requires that AI systems should both act asenablers to ademocratic, flourishing and equitable society by supporting theuser s agencyand foster fundamental rights,and allow for human oversight , itisimportantthatAIsystems donot rununchecked orunsupervised. Acknowledging human roles inAI entails amuch bigger role for individuals thanthat of justalearner orsupervisor. Individualsare now decisionmakers, who caninteract, design, interrogate, and delegate work to the AI system, just like managers inan institution. Individuals should be able to define expectations, decide roles and decision rules. 
A key example to explore is that of the 2010 Flash Crash. On6 May 2010, starting ataround 14:32EDT, theDow lost9%ofitsvalue. Roughly forty minuteslater, it regained 6 percentage points. Initially, thecauseof this crash was debated anearly suspected culprit was a so-called  fat-finger trade , and there was evenconcernthat some sortof cyberattack may have occurred. However, it later emerged that high-frequency, automated trading had caused alarge bulk of the massselling that occurred. Debatably, if humans had beenmore  in-the-loop  onlargeorders, such acrashwould never have occurred. To avoid similar mistakes whenusing AI inB&F, it is crucial that we donot entirely defer to algorithms whenmaking important decisions and ensure that appropriate checkpoints and human intervention/control or supervision mechanisms are in place. 
Recommendations 
  
Human oversight helps ensure thatanAIsystem doesnotunderminehuman autonomy orcauseother adverse effects. The objective oftrustworthy, ethical 

andhuman-centricAIcanonly beachieved by ensuringanappropriate involvement by human beings in relation to high-risk AI applications. 

  
Even thoughtheAIapplications considered inthisreport for aspecific legal regime are all considered high-risk, the appropriate type and degree of human oversight may vary from one case to another. It shall depend in particular on the intendeduseof the systems and the effects that the usecould have for affected citizens and legal entities.For instance, human oversight could have the following, non-exhaustive, manifestations: 


  the output of the AI system doesnotbecome effective unless it has been previously reviewed andvalidated by ahuman(e.g. therejection ofan application for social security benefits may be taken by a human only); 
  theoutputoftheAIsystem becomesimmediately effective, buthuman intervention is ensured afterwards (e.g. the rejection of anapplication for acredit card may be processed by anAI system, but human review must be possible afterwards); 
  monitoring of the AI system while in operation and the ability to intervene inreal time and deactivate (e.g. a stop button or procedure isavailable in adriverless carwhen ahuman determines that caroperation is not safe) (European Commission. 2020); 
  in the designphase, by imposing operational constraints onthe AI system (e.g. adriverless carshall stop operating in certain conditions of low visibilitywhen sensors may become less reliable or shall maintain a certain distance in any given condition from the preceding vehicle). 
4. Final recommendations 
Before summarising andcollating ourrecommendations, itmay seemthere is nothing but innate risk inthe use of AI. However, AI solutions canactually reduce a firm s exposure to risk. As established inthe previous section, when AI technologies are used to automatebusiness processes, theirusageneeds beregulated  assuch,oneideaisto actually useAIintheseregulatory procedures themselves. Thisconcept isknownas Regulatory Technology, orRegTech. Whilst RegTech doesn t necessarily need to useAI, its adoption inrecent years has certainly beenpowered by it. This direction offers a great deal of promise in relieving someof the burdentraditionally associated with regulatory compliance. 
RegTech 
RegulatoryTechnology,orRegTech,isaburgeoningfieldwhichuses technologytohelpsatisfytheregulatoryrequirementsafirmmightface.As 
Finance is already one of the most highly regulated industries, any tools which either partially or fully automate the work associated with regulatory 
compliancehavethepotentialtoprovideahugebenefitforafirm. 
Deloitte (Hug  et al., 2020) characterise the RegTech industry by 
dividingitintofivebroadcategories: 
1) Regulatory Reporting   automatically collecting, collating and 
publishingthedatarequiredforfirm sregulatoryreportingrequirements; 
2) Risk Management   monitoring and mitigating risk exposure, both in the sense of commercial risk and regulatory risk; 
3) Identity Management & Control   automating KYC and AML processes, as well as customer due diligence and anti-fraud procedures; 
4) Compliance   tracking adherence to current, as well as future, regulatory requirements; 
5) Transaction Monitoring   providing real time monitoring of transactions, often through the use of Blockchain technologies. 
Given the increasing amount of regulation in recent years (for instance, 
themeasuresintroducedafterthe2008financialcrisis,andGDPRcoming 
into force in 2018), the costs associated with compliance have increased. Moreover, regulatory adherence has historically been a time-intensive activity, reducing the capacity of the workforce. However, a key theme 
withintheabovefivecategoriesisthatofautomatingexistingprocesses  assuch,RegTechoffersanewapproachtorelievingthefinancialand temporalburdensofregulatoryrequirements,allowingfirmstobemore 
agile, whilst still adhering to the standards required of them. 
a) Recommendations for Regulators 
The best way to ensure AI firms come up with models that are compliant is through working with regulators as early aspossible. Schemes such as thesandbox schemewhich beganin the UK with FinTech and is now being extended insome countries to specifically focus onAI (e.g. the NorwegianAI Sandbox) is particularly efficient in achieving this goal. We highly recommend expansion of such schemes. We understand the EU is already considering anEU-wide sandbox and we believe thatfor many AIapplications such approach would be very helpful. 
Incaseswhere gaps still remainbetweenthe tech and the regulators  understanding or trust in its various risk management arrangement, we recommend copies of the code are kept by the regulators inescrow sothat it canberunondata independently to check if agreed risk parameters have been interfered with. This is common practice inalgorithmic trading in hedge fund management and is known to be effective. 
We recommend that regulators hire people with technical background and provide regular training toallpractitioners involved withregulating banking and finance AI application. In this training, we recommend the inclusionof the following topics in the contextofB&F: statisticalverification(e.g. underfitting and overfitting and therisk associated with explaining the past rather thanpredicting the future), bias and fairness training, and explainability. There already exist someprogrammes such asthe Oxford Algorithmic Trading programme where such topics are explained clearly to non academic audiences.We alsorecommend thatespecially withnew, emerging topics (such as explainability, for instance), regulators should receive regular updates onthe latest AI tech advancements and what is emerging in research and academia. 
b) Recommendations for Industry 
Wherever AIisdeployed, aclearchainofaccountability shouldbeestablished, assigning apersonresponsible toeach instance ofanalgorithm used withinafirm. All executives overseeing thedepartmentswhere thesemodelsare usedshouldreceive training so they understand the implications of their deployment. Additionally, engineers and managers should receive training pertaining to ethical AI and what it means for their systems. Awareness of the relevant regulationis important at every stage of development of AI systems and canprevent problems later on. Engineers should be encouraged to take ontraining in statistical analysis of the models and the datasets  inparticular statistical verification and fitting (and the risks of overfitting). 
Furthermore, traditional software testing should remainacritical part of any AI deployment, and new ideas from neural network verification should also be integrated into the process. Engineers should be also aware of adversarial attack vectors, and receive training inthe latest news inthis area and learnhow to mitigate such risks (as muchas possible). 
Finally, engineers should become aware of risks such as how models  leak  data and receive training tobetterunderstand these risks and how tomitigate them. Instead of trying to entirely eliminate bias, we should learnto manage it instead. This includes being able to identify potential for bias inthe models and datasets and understanding the types of algorithms that can mitigate its effect. 
References 
Amazon Web Services. 2020.  AWS AI | Coinbase.  Amazon Web Services, Inc. Accessed November 16, 2020. https://aws.amazon.com/machine-learning/customers/innovators/coinbase/. 
Arrieta, Alejandro Barredo, Natalia Di az Rodri guez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garci a, et al. 2020.  Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges Toward Responsible AI.  Inf. Fusion 58: 82 115. https://doi. org/10.1016/j.inffus.2019.12.012. 
Ben Green and Yiling Chen. 2019. The Principles and Limits of Algorithm-in-the-Loop Decision Making. Proc. ACM Hum.-Comput. Interact. 3, CSCW, Article 50 (November 2019), 24 pages. https:// 
doi.org/10.1145/3359152 

Branwen, Gwern. 2011.  The Neural Net Tank Urban Legend.  www.gwern.net. https://www.gwern. net/Tanks. 
Carlini, Nicholas, Chang Liu,  lfar Erlingsson, Jernej Kos, and Dawn Song. 2019.  The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks.  In 28th USENIX Security Symposium, USENIX Security 2019, Santa Clara, CA, USA, August 14-16, 2019, edited by Nadia Heninger and Patrick Traynor, 267 84. USENIX Association. https://www.usenix.org/conference/ usenixsecurity19/presentation/carlini. 
Chatfield, Daniel. 2017.  Fighting Fraud with Machine Learning.  Monzo. https://monzo.com/ blog/2017/02/03/fighting-fraud-with-machine-learning. 
Castelnovo, Alessandro, Crupi, Riccardo, Greco, Greta, Del Gamba, Giulia, Naseer, Aisha, Regoli, Daniele, and San Miguel Gonzalez, Beatriz. 2020.  BeFair: Addressing Fairness in the Banking sector.  Proceedings of the IEEE International Workshop on Fair and Interpretable Learning Algorithms (to appear). 
Commission de Surveillance du Secteur Financier (CSSF). 2018.  Artificial Intelligence: opportunities, risks and recommendations for the financial sector . https://www.cssf.lu/wp-content/uploads/files/ 
Publications/Rapports_ponctuels/CSSF_White_Paper_Artificial_Intelligence_201218.pdf 

Council of Europe, Committee of experts on Internet MSI-NET. 2017.  Study on the human rights dimensions of automated data processing techniques and possible regulatory implications . 
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.  BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.  CoRR abs/1810.04805. http://arxiv.org/ abs/1810.04805. 
The Economist. 2018.  AI, Radiology and the Future of Work.  The Economist. https://www.economist. com/leaders/2018/06/07/ai-radiology-and-the-future-of-work. 
Bank of England, and Financial Conduct Authority. 2019.  Machine Learning in UK Financial Services.  https://www.fca.org.uk/publication/research/research-note-on-machine-learning-in-uk-financial-services. pdf. 
Equifax. 2017.  Cybersecurity Incident & Important Consumer Information | Equifax.  2017 Cybersecurity Incident & Important Consumer Information. https://www.equifaxsecurity2017.com/. 
European Commission. 2020.  White Paper on On Artificial Intelligence - A European approach to excellence and trust . https://ec.europa.eu/info/sites/info/files/commission-white-paper-artificial intelligence-feb2020_en.pdf. 
European Parliament. 2020. Framework of ethical aspects of artificial intelligence, robotics and related technologies. https://www.europarl.europa.eu/doceo/document/TA-9-2020-0275_EN.pdf. 
Fredrikson, Matt, Somesh Jha, and Thomas Ristenpart. 2015.  Model Inversion Attacks That Exploit Confidence Information and Basic Countermeasures.  In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security, Denver, CO, USA, October 12-16, 2015, edited by Indrajit Ray, Ninghui Li, and Christopher Kruegel, 1322 33. ACM. https://doi. 
org/10.1145/2810103.2813677. 
Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. 2015.  Explaining and Harnessing Adversarial Examples.  In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, edited by Yoshua Bengio and Yann LeCun. http://arxiv.org/abs/1412.6572. 
Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press. 
Anhong Guo, Ece Kamar, Jennifer Wortman Vaughan, Hanna Wallach, and Meredith Ringel Morris. 2020. Toward fairness in AI for people with disabilities SBG@a research roadmap. SIGACCESS Access. Comput., 125, Article 2, 1 pages. https://doi.org/10.1145/3386296.3386298 
Hendrycks, Dan, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. 2019.  Natural Adversarial Examples.  CoRR abs/1907.07174. http://arxiv.org/abs/1907.07174. 
High-Level Expert Group on Artificial Intelligence. 2019.  Ethics Guidelines for Trustworthy AI.  European Commission. https://ec.europa.eu/futurium/en/ai-alliance-consultation. 
Hofstadter, Douglas R. 1979. G del, Escher, Bach : An Eternal Golden Braid. Harvester Studies in Cognitive Science ; 12. Hassocks: Harvester. 
Hug , F.-K., Laurent, P., Ramos, S. and Berliner, L. (2020). RegTech Universe. [online] Deloitte Luxembourg. Available at: https://www2.deloitte.com/lu/en/pages/technology/articles/regtech 
companies-compliance.html. Accessed 28 Nov. 2020. 
IAPP. 2020.  Norwegian DPA Creating Regulatory Sandbox for AI.  https://iapp.org/news/a/norwegian dpa-creating-regulatory-sandbox-for-ai/. 
International Regulatory Strategy Group, and Accenture. 2019.  Towards an AI-Powered UK: UK-Based Financial and Related Professional Services.  https://www.irsg.co.uk/publications/irsg-report-towards an-ai-powered-uk-uk-based-financial-and-related-professional-services/. 
Ji, Zhanglong, Zachary Chase Lipton, and Charles Elkan. 2014.  Differential Privacy and Machine Learning: A Survey and Review.  CoRR abs/1412.7584. http://arxiv.org/abs/1412.7584. 
Khandani, Amir E., Adlar J. Kim, and Andrew Lo. 2010.  Consumer Credit-Risk Models via Machine-Learning Algorithms.  Journal of Banking & Finance 34 (11): 2767 87. https://EconPapers.repec.org/ 
RePEc:eee:jbfina:v:34:y:2010:i:11:p:2767-2787. 
Kleinberg, Jon, Sendhil Mullainathan, Manish Raghavan.  2016.  Inherent Trade-Offs in the Fair Determination of Risk Scores.  Proceedings of Innovations in Theoretical Computer Science (ITCS). https://arxiv.org/abs/1609.05807 
Knight, Will. 2019.  The Apple Card Didn t  see  Gender and That s the Problem.  Wired, November. https://www.wired.com/story/the-apple-card-didnt-see-genderand-thats-the-problem/. 
Krebs, Brian. 2019a.  First American Financial Corp. Leaked Hundreds of Millions of Title Insurance Records   Krebs on Security.  Krebsonsecurity.com. https://krebsonsecurity.com/2019/05/first american-financial-corp-leaked-hundreds-of-millions-of-title-insurance-records/. 
   . 2019b.  What We Can Learn from the Capital One Hack   Krebs on Security.  Krebsonsecurity.com. https://krebsonsecurity.com/2019/08/what-we-can-learn-from-the-capital-one 
hack/. 
Kusner, Matt J., Joshua R. Loftus, Chris Russell, Ricardo Silva. 2017. Counterfactual Fairness. Advances in Neural Information Processing Systems 30 (NIPS 2017). https://arxiv.org/abs/1703.06856. 
Lanier, Jaron. 2019. Ten Arguments for Deleting Your Social Media Accounts Right Now. London. 
Lepri, Bruno, Nuria Oliver, Emmanuel Letouz , Alex Pentland, and Patrick Vinck. 2018.  Fair, Transparent, and Accountable Algorithmic Decision-Making Processes.  Philosophy & Technology 31 (4): 611 27. 
Madry, Aleksander, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. 2017.  Towards Deep Learning Models Resistant to Adversarial Attacks.  CoRR abs/1706.06083. http://arxiv. org/abs/1706.06083. 
Mehrabi, Ninareh, et al. 2019. "A survey on bias and fairness in machine learning." arXiv preprint arXiv:1908.09635. 
Mittelstadt, Brent. 2019. Principles alone cannot guarantee ethical AI. Nat Mach Intell 1, 501 507. https://doi.org/10.1038/s42256-019-0114-4 
Moskowitz, Tobias J. 2016.  Momentum Crashes.  www.aqr.com. https://www.aqr.com/Insights/ Research/Journal-Article/Momentum-Crashes. 
Naughton, John. 2019.  Can the Planet Really Afford the Exorbitant Power Demands of Machine Learning? | John Naughton.  The Guardian; The Guardian. https://www.theguardian.com/ commentisfree/2019/nov/16/can-planet-afford-exorbitant-power-demands-of-machine-learning. 
Noya, Eloi. 2019.  The Fintech Revolution: Who Are the New Competitors in Banking?  Forbes, July. https://www.forbes.com/sites/esade/2019/07/30/the-fintech-revolution-who-are-the-new-competitors in-banking/. 
OECD. 2017.  Algorithms and Collusion: Competition Policy in the Digital Age.  http://www.oecd.org/ competition/algorithms-collusion-competition-policy-in-the-digital-age.htm. 
Pinsent Masons, and Innovate Finance. 2019.  AI in Financial Services Impact on the Customer.  
Pound, Jesse. 2019. CNBC. https://www.cnbc.com/2019/12/24/global-stock-markets-gained-17 
trillion-in-value-in-2019.html. 
Pedreschi, Dino, Ruggieri, Salvatore, and Turini, Franco. 2008. Discrimination-aware data mining. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD '08). Association for Computing Machinery, New York, NY, USA, 560 568. DOI:https:// doi.org/10.1145/1401890.1401959 
Reuters. 2016.  Wall Street Watchdogs Turn to Artificial Intelligence.  Fortune. https://fortune. com/2016/10/25/how-artificial-intelligence-could-catch-stock-market-cheaters/. 
Rhodes, Chris. 2019.  Financial Services: Contribution to the UK Economy.  House of Commons Library. https://commonslibrary.parliament.uk/research-briefings/sn06193/. 
Robert, L. P., Gaurav, B., and L tge, C. 2020. ICIS 2019 SIGHCI Workshop Panel Report: Human  Computer Interaction Challenges and Opportunities for Fair, Trustworthy and Ethical Artificial Intelligence. AIS Transactions on Human-Computer Interaction, 12(2), pp. 96-108. 
Saxena, Nripsuta Ani, Huang, Karen, DeFilippis, Evan, Radanovic, Goran, Parkes, David C., and Liu, Yang. 2019. How Do Fairness Definitions Fare? Examining Public Attitudes Towards Algorithmic Definitions of Fairness. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society (Honolulu, HI, USA) (AIES  19). Association for Computing Machinery, New York, NY, USA, 99  106.https://doi.org/10.1145/3306618.3314248 

Sharif, Mahmood, Sruti Bhagavatula, Lujo Bauer, and Michael K. Reiter. 2016.  Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition.  In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, Vienna, Austria, October 24-28, 2016, edited by Edgar R. Weippl, Stefan Katzenbeisser, Christopher Kruegel, Andrew C. Myers, and 
Shai Halevi, 1528 40. ACM. https://doi.org/10.1145/2976749.2978392. 
Sitawarin, Chawin, Arjun Nitin Bhagoji, Arsalan Mosenia, Mung Chiang, and Prateek Mittal. 2018.  DARTS: Deceiving Autonomous Cars with Toxic Signs.  CoRR abs/1802.06430. http://arxiv.org/ abs/1802.06430. 
Song, Congzheng, Thomas Ristenpart, and Vitaly Shmatikov. 2017.  Machine Learning Models That Remember Too Much.  CoRR abs/1709.07886. http://arxiv.org/abs/1709.07886. 
Stripe. 2020.  Stripe Radar: Fraud Prevention for Credit Cards & Payments.  stripe.com. Accessed November 16, 2020. https://stripe.com/gb/radar. 
Szegedy, Christian, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. 2014.  Intriguing Properties of Neural Networks.  In 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, edited by Yoshua Bengio and Yann LeCun. http://arxiv.org/abs/1312.6199. 
Thanendran, Abhi. 2018.  How We Use Machine Learning to Protect You from Fraud | Revolut.  Revolut Blog. https://blog.revolut.com/how-we-use-machine-learning-to-protect-you-from-fraud/. 
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.  Attention Is All You Need.  In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, edited by Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, 5998 6008. http://papers. nips.cc/paper/7181-attention-is-all-you-need. 
Verma, Sahil and Rubin Julia. 2018. "Fairness definitions explained." IEEE/ACM 
International Workshop on Software Fairness (FairWare). 
Vulkan, Nir. 2019a. Oxford Programme on FinTech 
Vulkan, Nir. 2019b. Oxford FinTech MBA elective, Said Business School, Oxford. 
Yang, Zhilin, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019.  XLNet: Generalized Autoregressive Pretraining for Language Understanding.  In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, edited by Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d Alch -Buc, Emily B. Fox, and Roman Garnett, 5754 64. http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language understanding. 
Zhang, Yukun, and Longsheng Zhou. 2019. Fairness Assessment for Artificial Intelligence in Financial Industry. arXiv preprint arXiv:1912.07211. 
3 
ENERGY 
Chapter 1. Introduction 
1. Background and Overview 
The European Commission defines AI as systemsthat display intelligent behavior by analyzing their environment and taking actions   with some degree of autonomy   to achieve specific goals. AI-based systems canbepurely software-based, acting inthe virtual world (e.g., voice assistants, image analysis software, search engines, speech and facerecognition systems) orAIcanbe embedded in hardware devices (e.g., advanced robots, autonomous cars, drones or Internet of Things applications)  [1]. Figure 1shows themainAIstrategiesandsomenationalplans between 2017and 2019 [2]. 

Fig. 1. AI strategies by countries. 
1.2. Aim and Objectives 
The aim of this report is to provide comprehensive guidelines for the development of AI for the energy industry sector with practical recommendations based on fundamental rights and the ethical principles of the seven key requirements. 
Theseare applicable todifferent stakeholders (developers, deployers, endusers,and society) engaged in the life cycle of AI systems, having equal importance but different roles to play in ensuring that the requirements are met. 
Trustworthy AI canrepresent agreat opportunitytosupport the mitigation of pressing challenges facing society such as climate change. While tackling climate change should be atop priority for policymakers across theworld, the digital transformation and trustworthy AI have a great potential to reduce human impact on the environment andenable theefficientandeffective useofenergy andnaturalresources [3].For instance, digital transformation andtrustworthy AIcanbecoupled withbigdata analysis inorder todetectenergyneedsmore accurately, which willprovide more efficient energy infrastructure and consumption. 
The concrete objectives for the energy industry are to create an ecosystem of trust and excellence along the entire value chain, to create the right incentives for accelerating the adoption of solutions-based AI, including by SMEs, and to consider the key elements of a future regulatory framework for AI in Europe to ensure compliance with EU rules [4]. 
Also, combining itstechnologicalandindustrialstrengths withahigh-quality digital infrastructure andaregulatory framework based onits fundamental values, the EU can become a global leader in innovation of the data economy and its application in the energysector. This brings the benefits of AI technology toEU citizens, business development, andservices of public interest. 
1.3. Report Outlines 
Chapter 1 gives ashort introduction to this report highlighting its aim and objectives. Chapter2provides comprehensive guidelinesfor theapplication oftheseven key requirements and their impact onvarious energy industry applications while Chapter 3 suggests concrete and practical steps that the energy sector must take to be compliant with the seven key requirements. In Chapter 4, someconclusionsare drawn aswell as practical recommendations for AI adoption in the energy industry. Thecorrespondence andconnectionbetween chaptersandtopicsinthisreport is highlighted in Fig. 2. 

Fig. 2. A block diagram with report outline. 
Chapter 2.How the Seven Key Requirements Impact the Energy Sector 
2.1. Concrete Objectives and Key Factors Considering the Impact 
Inorder tobe prepared for Industry 4.0, we have toenvisage how AI, big data, and machinelearningcantransform theway we work andwhich toolsare available or suitable to work alongside robots and machines in the energy sector. We also need to optimize thework that is necessary so that the interaction between humans and robots in shaping the future workplace is tailored to the needs of society as a whole. The aim of the framework, in partnership between the private and public sectors, is to: 
--Create an ecosystem of excellence along the entire value chain 
--Conceive the right incentives for accelerating the adoption of solutions-based AI,including by SMEs (digital innovation hubs should provide support to SMEs to understand and adopt AI) 
--Specify the key elements ofafuture regulatory framework for AI in Europe that will create a unique ecosystem of trust to ensure compliance with EU rules. 
--Combine technological andindustrialstrengths withahigh-quality digital platform/infrastructure 
2.2. The Seven Key Requirements (K1-K7) 
Basedonthefundamental rights and ethical principles, the seven key requirements that AI systems should meet in order to be trustworthy are listed below: 
K1. Human agency and oversight   including fundamental rights, human agency, and human oversight 
The most important fundamental rights that companies within the energy sector should consider are: -- Dignity and non-discrimination; -- Personal data and privacy protection; These are considered in detail later in this document. 
Human agency: Uses of AI in the energy industry must take account of the fact that individuals and groups will have divergent but still legitimate goals, and allow the possibility of exercising choice. AI must enable better, more informed human decision-making, not take away control. Inparticular, thedevelopment oflarge-scaleand long-term infrastructure, whether physical orvirtual, must consider the need to preserve flexibility for the future, so that as political preferences, scientific understanding, and social conditions change, new approaches canbeimplemented withaslittledifficultyand expenses as possible. There must always be scope for alternative conceptions and configurations of the marketplace, suchasdecentralized, community-owned, ornot-for-profit approaches. Preserving the possibility for innovationis crucial. Inaddition, individuals and businesses must be able to make free and informed choices about how to manage their energy use and expenditure. Transparency inthe ways inwhich AI, big data, and machine learning are being applied tomanagedemand, determine tariffs,and smoothchargesare essential foundations for this, particularly for those suffering from energy or financial poverty. 
Human oversight: Although AI provides amechanism for making decisions atanotherwise impossible paceandscale,humanoversight mustbemaintained. Renewable energy systems (RES)mustbedesignedsothatunusualeventsorfluctuationsarebroughtto the attention of operators as quickly as possible, and intervention is always possible, so that runaway algorithms or cascading failures are avoided. Human oversight is also an important mechanism for identifying security breaches and risks, and, therefore, systems should be both auditable and regularly audited. This is discussed further in the next section on technical robustness and safety. Machine learning models mustalso be regularly checked and verified toensure that they have not developed rules that are socially harmful, biased, or otherwise undesirable. 
K2. Technical robustness and safety   including resilience to attack and security, fall-back plan and general safety, accuracy, reliability and reproducibility 
Having access to stable and reliable energy supply is among the key rights of citizens. Thus, technical safety and security of energy systems, including resilience toattacks, are key elements for Europe. 
Moreover, the energy supply and the entire energy system (i.e., energy production, trading, distribution, storage, and utilization) are part of critical European infrastructure affecting European industry, logistics, and mobility (of both people and goods). To summarize, therefore, AI technology should contribute to the improvement of safetyandtechnicalrobustness,aswellastheidentificationandmitigationof attacks. To this end somesecurity and safety aspects are discussed below and some key technologies, enabled by AI, are highlighted. 
Technical robustness and safety In terms of technical robustness and safety, all parts of the energy system (production, trading, distribution, storage, and utilization) should be supported by predictive health maintenance technologies where AIcangive its key contribution. By analyzing both physical dataor results from model estimations, an AI based prediction health manager canreduce the risks of faults and ageing degradation and hence canreduce the risk of accidents or denial of service. 
Thankstopredictive management, all maintenance operations canbe scheduled duringordinary maintenance steps, thereby reducing severity and the probability of faults. 
Predictions based onAI techniques canbe used not only for energy production, storage and distribution, but also for energy trading, e.g., to find the right compromise between energy production-selling price-user consumption in afree but transparent energy market. 
With the increasing decentralization and digitalization of the power grid/ 
smartgrid,itisbecomingmoredifficulttomanagethelargenumberofgrid 
participants and to keep the grid in balance. This requires evaluating and analyzing a large amount of data. AI helps to process, evaluate, analyze, and 
controlthesedataasquicklyandefficientlyaspossible. 
Resilience to attack and security Differing from faults oraging degradation, failures due to cyberattacks may be difficult todetectby basicanalysis ofphysical acquired data.To overcome thisobstacle, the energy system shouldalsobeequippedwithanomaly detectiontools.Exploiting AI techniques, fingerprinting and anomaly detection methods aim to detect anomalies in the behavior of a network or of a software (SW)-defined control system. Fingerprinting andanomaly-detection methods also aim to classify if there isafailure due to aging or random fault or a failure due to malicious software. 
The security problem is exacerbated by the increased connectivity of the energy system (production plants, energy storage stations, distributiongridandrecharging stations) since due to thesmart grid paradigm andthe V2G (vehicle to grid) approach the range of possible attacks is increasing. Moreover, the capability of remote update of the SW of anenergy control unit has the drawback of increasing the possibility of malicious software s taking control of an energy subsystem. 
Fingerprintingandanomalydetectionisthefirststeptoclassifyfaultsand cyber-attacks correctly thereby providing a fall-back plan to ensure safety of the energy system. 
AI-basedfingerprintingandanomalydetectioncanbeusednotonlyfor energy production, storage, and distribution plants, but also to detect anomalies during energy trading sessions for a transparent energy market. 
Fall-back plan and general safety As discussed above, AI can enable predictive healthmaintenance and anomaly detection and fingerprinting techniques to be adopted in energy systems. However, to ensure that energy systems are resilient and safe, these prediction and detection capabilities should becomplemented by thepresence offall-back energy plans incasesomething goes wrong. Here also, AI canbe useful to define the optimal level of redundancy finding a trade-offamong complexity, cost,andrecovery performance oftheenergyback-up solution. 
Accuracy Theproposed solutionsfor AI-based predictive healthmaintenanceandanomaly detection above should be tested and validated toensure accuracy, minimizing false alarms or missed detections. Indeed, in industrialized countries, such as the EU Member States, the power supply is already quite reliable, and hence any change to anewone (perhaps improved in terms of affordability or green sustainability) using AI-prediction techniquesmustbeaccurate.Unintended consequences should be avoided, aswell as both false alarms or missed detections. Similar accuracy of AI-based predictions are key for their success in the energy market (e.g., electricity trading). 
Reliability and reproducibility Reliable energy solutions must be also reproducible to allow scaling to different contexts (industrial, domestic, mobility, and transport system) and rules (e.g., different national regulatory frameworks). Moreover, reproducibility is akey element toallow system validation and testing and, through large-scale production, to allow for affordability. 
K3. Privacy and data governance   including respect for privacy, quality, and integrity of data, and access to data 
Thevolume andtheway inwhich dataare stored andprocessed willchange dramatically in the next five years. As discussed inthe previous section, akey issue for energy system sustainability and affordability is the capability to profile and predict the energy needs of the energy users(citizens, but also industries, logistics, and mobility systems). One of the aims of using AI technologies is tooptimize the way energy is used,shared, produced, andtraded.Alloftheseaspectsrequire thedevelopment of distributed intelligence needing data analytics and exchange of alargevolume of data. However, thelevel of profiling possible canenable the identification of very private details of anindividual s lifestyle, routines, and habits, which could be extrapolated to make inferences about their health. 
Hence, thanks alsotoAI, privacy of sensitive data should be ensured and data governance should be put in place. 
To give the energy industry and, in particular, end-use consumers more confidenceinAI(e.g.,inrelationtosmart-hometechnologies,smartmeters,or optimizationofcharginginfrastructureinsmartgridapplicationsforelectrified mobility), it must be clearly communicated how the data are used and by whom, while data security must be guaranteed. Therefore, the European Commission has developed four ethicalprinciples for AI:AIshouldrespect humanautonomy, avoid social harm, be fair, and be explainable [4]. 
In addition to following ethical guidelines, it is essential that AI in the energy sector comply fully with European law, particularly the General Data Protection Regulation (GDPR). It should also apply the Data Protection Impact Assessment Template prepared by the Commission s. Smart Grid Task Force [5], andfollows theCommission s recommendation (2012/148/EU)ontheroll-out of smart metering systems, which underlines the importance of taking all reasonable steps toensure that data cannot be de-anonymized, incorporating data protection by design anddataprotection by defaultintothemethodologiesusedfor thedevelopment of smart grids, and designing in data security from an early stage. 
Datagovernance willbeimportantinorder toensure compliance withdata protection law and to maximize the potential use and re-use of energy data for modeling, machinelearning,andotherapplications inthefuture. Energysystems musthave mechanisms in place to track the nature of collected data, to determine the lawful basis for their processing, and to manage sharing, eitherwithin an organization or with third parties. Care must be taken toensure that future processing is in line with the original purposes for which data were collected. Furthermore, if the lawful basis was consent, than it should be ensured that future processing is within the scope of that consent and that there is some mechanism for those who have withdrawn consent to have their data removed from all copies of adataset, particularly if they have been externally shared. Individuals must be able to vindicate their rights of access, erasure, and data portability, inlinewiththerecommendations oftheEuropean Smart GridsTask Force Expert Group 1 report on My Energy Data  [6]. This may prevent the development of AI applications that are technically possible but cannotcomply with the law. The early application of data protection by default and by design methodologies to new ideas will prevent wasting resources on projects that cannot be legally completed. 
The energy supply and the entire energy system are part of critical infrastructure. This is why cybersecurity is becoming increasingly important in order to protect the highly networked power grid/smart grid from attacks and data theft from the outside. Consequently, there should be clear security requirements for participants in the electricity market to ensure data confidentiality,integrity,andauthenticationalongthewholechainofdata acquisition,processing,andstorage.Thiscanbeachievedbydefiningbest practicestobefollowedforsecurityofdatatakingnoteofthedifferentroles: users, production industries, companies working as service providers, and institutions (e.g., standardization bodies, regulatory authorities, and governments). 
Data protection and data security (especially mitigation of cyberattacks) are very sensitive points for theuseof AI in the energy industry. AIcanmake animportant contribution in the fight against cyberattacks by quickly checking large amount of data and thus detecting deviations. The Smart Grid Task Force Expert Group 2 report on cybersecurity [7], which proposes a Network Code on Cybersecurity for energy system operators, should be taken into consideration. Toward thisend,theAI4People initiative canfind synergieswiththeIEEEGlobal Initiative onEthically Aligned Design for aSustainable Planet [8]/). More detailsare available in Section 2.2/K7. 
K4. Transparency   including traceability, explainability, and communication 
As with other sectoral applications of AI, its acceptance will depend on its transparency. Inthefinancial andlegalsectors,AIisincreasingly deployed withwidespread ramifications for society, e.g., in terms of credit scoring and parole decisions. Just like otherforms ofautomation,AIcancreate value by relieving humanintervention in routine tasks that are based on processing increasingly large amounts of data. However, AI s mechanisms canappearopaque,thereby undermining itscredibility. Thus,AI applications with societalconsequencesmay require anauditing procedure thatcan trace the logic of its decision-making nexus. This is not dissimilar to financial auditing, which was necessitatedby theadvent ofpublicly heldcompaniesthatissuedstock. Such aprocess provided creditors andinvestors withasnapshotofthecompanies  financial architecture. Likewise, regulatory standards for AI audits could be established. 
In the energysector, thesewould involve testing AI onprototype usecases, e.g., energytradingoreconomicdispatch,thatcouldbecompared withoutputfrom conventional decision-making approaches. Inasense,regulators intheelectricity industryalready facethischallengeduetoderegulation aspower companiesuse sophisticated offering strategies in order tomaximize profit, which could be atodds withthemaximizationofsocialwelfare. Thetaskofmonitoringmarkets couldbe daunting, but it is aided somewhat in the electricity industry due to physical network constraintsandtheneedfor energybalanceinreal time.Hence, marshaling this existing expertise together with algorithmic advances in inverse optimization [9]9 could be used by auditors to unveil how AI applications function in the energy sector and to ensure that their use is in line with social objectives. 
K5. Diversity, non-discrimination, and fairness   including the avoidance of unfair bias, accessibility and universal design, and stakeholder participation 
The diversity problem is about gender, race, and most fundamentally, about power and energy. It affects how AI companies work, which products get built, who they are designed to serve, and who benefits from their development. 
Matching characteristics of the energy market tomodels of discrimination, we needtoidentify the necessary conditions for the license condition tohave apositive effect for consumers, and to explore whether the policy has helped potentially vulnerable consumers.Non-discrimination law anddataprotection law are themainlegal instrumentsthatcouldprotect therighttonon-discriminationinthecontextof algorithmic decision-making. 
Algorithmic decision-making and other types of AI canthreaten human rights, suchastherighttonon-discrimination. Intheprivate sector, algorithmicdecision-makingcanhave discriminatory effects. For instance, AI canbe used by firms to select employees, while targeted online advertising is largely driven by algorithmic decision-making.Such advertising isaprofitable sectorfor somecompanies.For example, Facebook and Google make mostof their money from online advertising. However, online advertising canhave discriminatory effects. While decisions by algorithms can have discriminatoryeffects, algorithmsare notinherently badordiscriminatory. Algorithms might still perform better than human decision-makers. 
Non-discrimination law anddataprotection law are themostrelevant legal instruments to fight illegal discrimination by algorithmic systems. If effectively enforced, then both legal instruments can help to protect people. 
Biased training data, however, could lead to discriminatory decisions. The training data can be biased because they represent discriminatory human decisions. In order to ensure non-discriminatory programming and functioning, the systems need to be trained and tested for unfair bias. Therefore, companies should test their algorithms for bias and discrimination and demonstrate that certain fairness standards are met [10]. The IEEE P7003 standard for algorithmic bias considerations lays outrelevant instructions for eliminating issues of negative biaswhen developing algorithms. 
TheEuropean EconomicandSocialCommittee(EESC) suggests thattheEU should develop acertification for trustworthy AI applications, tobe delivered by an independent body after testing the products for key requirements suchasresilience, safety, and absence from prejudice, discrimination or bias [11]. 
K6. Societal and environmental wellbeing   including sustainability and environmental friendliness, social impact, society, and democracy 
Given the focus of Europe onclimate-change, all technologies like ICT and AI applied toenergysystems shouldimprove environmental sustainabilityandthe affordability of green energy solutions for citizens. 
Key initiatives to this endare the development andadoption of renewable energy sources (RES) (even atthesmallscaleofrooftop photovoltaic orhybrid plants at single building level) and the possibility tocreate peer-to-peer community of energy prosumers (producers/consumers), to profile/predict their energy needs and to optimize theway energy is used, shared, produced, and traded. All of these aspects require the development of adistributed intelligence needingdata analytics and alargeexchange of data. 
AI in power trading helps to improve forecasts (based on weather data, 
historicaldata,consumptionprofilesandmobilitypatterns)aswellastofacilitate 
and speed-up the integration of renewables (green energy). Better forecasts also increase grid stability and, thus, security of supply. For example, wind power futures for Germany have existed since 2017 for hedging their risks [12]. 
AIsystems couldmitigateclimatechangeandenvironmental degradationby contributing to positive solutions for criticalresource usage and energy consumption. Consumers, intelligently connected in the electricity system, cancontribute toastable and green electricity grid. Smart home solutions and smartmetersalready exist but theyare not yet widely used. Part of the obstacle is the lack of a business model, which leads to market failure. In essence, individual consumersontheirown do not have the economicincentive todeploy energy-efficiency technologies.However, given the mechanism to pool consumption profiles and preferences, an aggregator using AI could profit from utilization of such demand-reduction and load-shifting potential at ascale thatcouldfacilitatetheintegrationofbothdemand-sidemanagementandRES technologies. 
Related to K4 above, deployment of AI by profit-maximizing entities may not be fully aligned with societal objectives such as welfare maximization, equity, and emission reduction. Besides the auditing procedure alludedtoin K4 above, empowerment of citizen groups to harness AI could provide aformidable counterparty onthe demand side. Thus, by enabling both sides of the energy market to function, it may be possible for AI to keep itself in check from putting suppliers  interests above those of consumers. Regulatory authorities  use of AI would add another layer of intelligence that could anticipate the actions of market participants and tweak market designs to discourage deviations from societal objectives. Underpinning legislation already exists in Article 16 (local energy communities) from the EU s clean energy package 
[13].By requiring Member States to ensure non-discriminatory accessto all markets through establishedorautonomously managednetworks, itcouldprovide anAI dimension to energy communities to flourish. 
For consumersonlow incomes and suffering from energy poverty, smart meters withdynamic pricing managed by AIsystems may provide anopportunitytosave money, but the differencethat this makes is likely to be marginal, while much of their consumption cannot be moved to lower-price time periods. They are also less likely to be able to benefit from feed-in tariffs. The cost of prepayment should, therefore, be kept aslow aspossible,sothat all have anopportunitytoparticipate fully in newenergy markets. 
K7. Accountability   including auditability, minimization and reporting of 
negativeimpact,trade-offs,andredress 
The EC defines accountability as Mechanisms should be put in place toensure responsibility and accountability for AI systems and their outcomes.] Auditability,which enables the assessment of algorithms, data, and design processes, plays akey role inthis, especially incriticalapplications. Moreover, adequateand accessibleredress shouldbeensured  [3].TheEChasannouncedthefollowing requirements related to the accountability in AI together with the dataset: 
  
Theuseof AI-enabled products and services mustbe safe. TheymustmeetEU safety rules (existing as well as possible complementary ones) and the standards set.Thesestandards andEUsafety rulesare now changingworldwide, (see below), withnewproposals tobecompleted andapproved. TheStandards Development Organizations (SDOs) develop IT standards using different models to address varying standardization needs. 

  
The datasets would be available upon request for any competent authorities, e.g., inspections. 

  
The process should protect confidential information (e.g., trade secrets). 

  
The datasets employed to train the AI systems need to consider all the scenarios required and to be sufficiently broad. 

  
Thedatasets, documentation, andrecords, would needtoberetained over a sufficient time period (reasonable and limited) toguarantee the application of the aforementioned standards and EU safety rules. 

  
Competentauthoritiesandaffected partiesshouldbeinformed aboutthe limitationsandcapabilitiesoftheAIsystems, mainly aboutitspurpose, functionality conditions. 


Since people are at the center of the AI strategy set by the EC [2], in the caseof any AIsystem which works without human interaction, the people affected by the AI system shouldbeinformed, witheasily understandable, concise,andobjective information. The appropriate form will depend onthe particular context [4]. The EU data protection legislationconsiders this [14], but the EC is working on new rules and standards to guarantee it.By contrast, in cases where it is sufficiently clear that citizens are interacting with AI systems, this information does not need to be provided. 
The competitiveness of European business and a high standard of safety would be reached by asuitable legal environment. The damage caused by AI systems generates anunclear liability scenario under the Product Liability Directive, and it is notclear also how to apply this Directive to certain defects due to AI systems [14]. The General Product Safety Directive [15] is the EU legal framework for product safety to provide ahigh level of safety andhealth. This Directive must be considered together with the different systems of civil liability for damages caused by products or services. 
The U.S. President issued anExecutive Order in 2019 to  Ensure that technical standards reflect Federal priorities for innovation, public trust, and public confidence in systems that use AI technologies and develop international standards to promote and protect thosepriorities [16]. It directs theSecretary ofCommerce, through the National Institute of Standards and Technology (NIST), toissue a plan for Federal engagement in the development of technical standards and related tools in support of reliable, robust, andtrustworthy systems thatuseAItechnologies  [17]. TheAI standards will consider the following aspects: 
  Conceptsand terminology; Data and knowledge; Human interactions; Metrics; Networking; Performance testingandreporting methodology;Safety; Risk management;Trustworthiness (guidanceandrequirements for accuracy, explainability, resiliency, safety, reliability, objectivity, and security). 
According to the plan, thetools/applications include, but are not limited to the following [17]: 
  Datasets in standardizedformats, including metadata for training, validation and testingofAIsystems; Tools for capturingandrepresenting knowledge, and reasoning in AI systems; Fully documented use cases that provide a range of data and information about specific applications of AI technologies and any standards orbestpractice guides usedinmakingdecisionsaboutdeployment ofthese applications; Testing methodologies tovalidate andevaluate AItechnologies  performance; Metrics to quantifiably measure and characterize AI technologies; Benchmarks,evaluations, andchallengeproblems todrive innovation; AI testbeds; Tools for accountability and auditing. 
Standards under the direct responsibility of ISO/IEC secretariat related to AI are described in Appendix I. 
China has published the  New generation artificialintelligence development plan  [18]. According to point V. Guaranteemeasures, the strategy fixed the following issue: 
  
EstablishanAI technology standards and intellectual property system: It will be implemented in two consecutive steps: (1) Work oninteroperability security, traceability, andavailability; (2) Setandimprove AIrelated tothenetwork security, interoperability, privacy protection, industry applications, andother technical standards. The directive aims for industry to participate ondeveloping international standards, with the objective of fostering overseas applications. 

  
Establish an AI security supervision and evaluation system: AI security supervision should be conducted not only to assure national security and secrecy, but also to secure protection of people, technology, material, and management support. AI security monitoring will construct anearly-warning mechanism, which together withanAI technology prediction will grasp the technology and industry trends. Restraint guidanceandprospective prevention shouldbestrengthened by a tailored risk assessment study, together with its prevention and control. China, like the EU, considers theimplementation of accountability. It will be done by a transparent and open AI supervision system. Atwo-tiered regulatory structure will be applied to the supervision of the whole process. China claims to promote self-discipline. Evaluationmechanisms will be developed onAI, together with indicatorssystem andsystematic testingmethods.Finally, theAIsecurity certification will be done onacross-domain AI test platform with systems key performance and assessment of AI products. 


A decree of the president of the Russian Federation onthe development of AI affirmsthat: Creation ofanIntegratedSystem for Regulating theSocialRelations Arising in Line with the Development and Use of Artificial Intelligence Technologies  [19]. It will be based onmainly onthe followingpoints applied tothe technological solutions developed on the basis of artificial intelligence: ensuring conditions for access to data; ensuring favorable legal conditions for accessto data; formulating ethical rules for humaninteraction with artificial intelligence;eliminating administrative barriers during the exportation of civilian products; creating legal conditions and establishing procedures for thesimplified testingandintroduction oftechnological solutions, as well asdelegating the possibility of individual decision-making to information systems (with theexception ofdecisionsthatmight infringe upon therightsandlegitimate interests of individuals); creating unified systems for the standardization and assessment. 
Thelegalconditionsare planned tobeimplemented by 2024, andby 2030a flexible legal regulatory system must be functioning. 
Japanhas created the  Strategic Council for AI Technology  [20]. lFive National Research and Development Agencies will be managed by the Council, under the jurisdiction of the Ministry of Internal Affairs and Communications, Ministry of Education, Culture, Sports, Science and Technology, and Ministry of Economy, Trade and Industry. Three research centers willdepend onthefollowing agencies: Center for Informationand Neural Networks (CiNet) and Universal CommunicationResearch Institute (UCRI) of the National Institute of Informationand Communications Technology (NICT); RIKEN Center for Advanced Intelligence Project (AIP) of the Institute of Physical and Chemical Research (RIKEN);Artificial Intelligence Research Center (AIRC) oftheNational Institute of Advanced Industrial Science and Technology (AIST). 
There are two institutions toimplement theprojects, theJapan Scienceand Technology Agency (JST) and New Energy and Industrial Technology Development Organization (NEDO). The strategy is based onnine principles while the strategy of the EU is based onseven [17]. According tothe accountability,  Developers should make efforts to fulfill their accountability to stakeholders including AI systems  users.  
Inotherwords, thedevelopers willprovide explanations andinformation to users/providers abouttheAIsystem s characteristicswithactive involvement of stakeholders. There isapolicy proposal about harmonization of AI and regulations, strategically takingtheinitiative ininternational standards andholdingintellectual property. 
The Institute of Electrical and Electronics Engineers (IEEE) Global Initiative on Ethics ofAutonomous andIntelligentSystems canserve asabaselinetoaddress requirements for accountability of Energy Industry. The IEEE P7000 series is focused onethicalconcernsintechnologicalissues[8].IEEEcenterstheaccountabilityon: autonomous and intelligent technical systems; government and industry stakeholders; themanifestations generated by autonomousand intelligenttechnicalsystems. The accountabilityshouldbeEthically Aligned Designed (EAD) withuniversal human values, politicalself-determination dataagency, andtechnicaldependability. The chaptersofEADapply thegeneralprinciples tothepracticeby aseriesof recommendations: General Principles; Affective Computing; Methods autonomous and intelligent systems (A/IS) design; A/IS for Sustainable Development; Embedding Values intoA/IS;Policy; Law. According to[20]:  A/IS shallbecreated andoperatedto provide an unambiguous rationale for decisions made. Appendix II shows the standard projects that are under development. 
2.3. Responsible Applications of AI in the Energy Industry 
Any AI application in the energy sector should involve responsibilities. The responsibilities should be clearly defined by laws, standards, rules, etc., and from design to the useof theAIproduct orservices, including alsodevelopment, procurement, deployment, operation,and, finally, validation ofeffectiveness [21].Theresponsibilities should consider the following issues (K7): 
  
The code/hardware trackrecord employed by the AI product. It will also support the transparency record, including the records from inside and outside of the developers, e.g., some codes can be open source or created by third parties. 

  
All of the roles and responsibilities of every agent involved in the value chain. This is required in order to assign liabilities and the proportion of them according to the faults. 

  
Following onfrom this, all agents will need to have the information and training necessary to understand their responsibilities and the legal consequences of their work. 

  
All of the rules and standards must be created andapplied according to the laws andregulations system where theAIproduct couldhave any consequence or effect. 


It is also desirable that anybody involved in the value process of the AI product preserve documentationrelative toprocedures, certifications, decisions, etc., for the period that should be defined in any casesby the responsible agency, in casethat it can be required for any authority. 
Section 2.2. shows that several countries have begun to work on this issue, creating agencies in order to ensure compliance. Accountability shouldbeclearly linked totheothersixkey requirements, mainly effectiveness, competence, and transparency. Theycannotbe considered individually; instead, they must be treated as a whole to ensure ethical and legal compliance, and the technical safety and reliability of the AI products. 
The recommendations given by [14] are also based ontheseven key requirements, such as: 1.Creators of AI products should define clearly the outcomes of the product, e.g., accuracies, validation, etc., and also their responsibilities (K2). 
2.All of the agents involved in the operation of the AI product should understand the responsibilities and the potential legal liability. As mentioned above, the legal environment it is not yet sufficiently clear for developers to guard against liability and governments are working on this to improve it (K3). 
3.Responsibilities should be clearly defined in the contracts (K7). 
4.Operators and creators should have the support of professional and independent mechanismstoguarantee therequirements oftheAIproducts, e.g., agencies, audits, etc. (K1, K2) 
5.The government, agencies,etc.,responsible for legalconditions,standards, normalization, etc., should also create individually and collectively avariety of incentives toensure thatthe outcomesof the AIproduct are in line with the ethical and accountability issues (K7). 
6.Inquiries todetermineresponsibilities shouldconsidertheabove points, consideringalloftheagentsinvolved intheAIproduct andalltheactivities performed in the value chain of the product (K1, K7). 
Ascanbeseenin Fig. 3, the most important applications of AI in the Energy sector are energyefficiencyinsmart buildings,energyutilization/power consumption,energy storage, and smart grids. 
AI becomes increasingly more important in the energy industry with agreat potential for the future design of energy systems. AIcanhelp the energy industry tobecome more efficient and secure, for instance by analyzing and evaluating the large volume of data. 
AI in the power grid and smart grids (K1, K3):AI is present in the field of intelligent networking of electricity generation and consumption. With the increasing decentralization and digitalization of the power grid it is more difficult to manage the grid participants keepingthe grid in balance in the sametime. AI canbe used here for evaluating and analyzing the big amount of data asquickly and efficiently aspossible. Insmartgrids,where power plant generation has dramatically increased, AI canhelp toevaluate, analyze, andcontrol thedataofdifferent actors,suchasconsumers, producers, and energy storage systems. A particular casefor smart grid applications is integrating EVs. AIcanhelptomonitor and coordinate the charging of EVs, thereby offering the possibility of storing electricity and stabilizing the grid. 
AI applications for variable RES integration (K2, K3, K6): The potential of AIisbeingunlocked by thegenerationofbigdataandincreased processing power enabling fast and intelligent decision making. This leads to increased grid stability and reliability as well as flexibility by integration of variable RES, like wind and solar, using for instance optimized energy storage solutions. Digitalization and digital technologies can better support RES integration in smart grids. 
AI in electricity trading (K2): AI in power trading can help to improve forecasts by systematically evaluating large amounts of data, such asweather data and historical data. Better forecasts can also increase grid stability and, thus, security of supply. AI can also help to automatically monitor and analyze trading on the electricity market thereby enabling the detection of faults preventing deviations from standard values. 
AIusedforimprovingtheenergyefficiencyinsmartbuildings(K3): AI can monitor and control power consumption using smart solutions based on smart sensors andsmart meterscontributingtoamore stable and green electricity grid. Analyzing data-baseduserpreferences leadingtoinformed responses intheelectricitymarket could save electricity, thus reducing the cost. 
AI applications for energy accessibility (K4): Smart home applications-based AI, such asVerv and PowerScout [8], canassistuserswith energy management. These kinds of applications enable userstomonitorrecords onhow each appliance in their homesusesenergy and to regulate their energy expenses. These applications also have safety features and provide tips for reducing carbon emissions, thereby helping clients in making the right decisions when deploying RES for their homes. 

Fig. 3. The most important applications of AI in energy sector based on SCOPUS publications. 
2.4. Contribution to Energy Sector Transformation and Challenges 
Theenergysectorisundergoingamajortransformation withtheincrease ofRES technology (solar and wind) that provides variable energy, distributed energy resources (DERs), bidirectional power flow of electricity, large flows of data collected by IoT and otherdevices, increased useofenergy storage and theevolving role ofutilitiesand consumers. 
According to the EC s white paper on AI [4], the competent authorities should be in a position not only to investigate individual cases, but also to assess the impact on society. The trustworthiness and security of AI, based on European rulesandvalues,mustbeeffectivelyenforcedbyaffectedpartiesandtheEuropean and competent national authorities. Procedures for certification, inspection, or testing, including prior conformity assessment, must be carried out because of the high risk of some AI applications. For example, checkingthe datasets used in the development phase and the algorithms, in the conformity assessment mechanisms that already exist. 
Incaseswhere these mechanisms do not exist, similar mechanisms may need to beestablished,consideringtheinputsoftheEuropean standards organizations, stakeholders andthebestpractices.TheAImustbenon-discriminatoryand proportionate, employing objective andtransparent criteriainaccordance with international obligations.It would be mandatory for all economic operators addressed by the requirements the conformity assessments. 
The EC establishes some support structure in order to limit the burden on SMEs, through the Digital Innovation Hubs, dedicated online tools, and standards. According tothewhite paper  Any prior conformity assessmentshould be without prejudice to monitoring compliance and expost enforcement by competent national authorities.  Competent authorities and third parties cantestAI applications by ex-post controls. Therefore, theyshouldbeenabledby correct documentation.Acontinuousmarket surveillance scheme will be required for thecompliance monitoring. Finally, effective judicial redress should be ensured for parties who are negatively affected by AI systems. 
2.5. Barriers and Risks to AI Adoption 
There are not yet clear rules, laws, or standards, etc. that the agents involved can apply. Therefore, it is difficult or not possible to assign any responsibility regarding AI products orservices. According to[15], if aproduct is notassignedany responsibility, then it cannot be trusted. 
AI is aproduct that canbe offered asa black box,  i.e., opaque, generating ahigh risk to the agents involved. For example, it canhave complex algorithms with alarge number of codes, and the algorithms canchangeover the time toadapttothe data inputs or theoutcomes. The data can have a large number of variables, volume, variety, complexity, etc. The process is also complex, with many agents involved in the process, e.g., programmers, engineers, data analysts, owners, operators, etc. involved in the value chain, from the design to the end of the life cycle of the AI product. Thus, it generates a great challenge in order to set responsibilities and accountability. 
It has been shown in Section 2.1 that countries worldwide are working onthis issue, to create agencies to control AI, develop standards, etc. to guarantee accountability, but it is not done yet. It must set clear responsibilities to verify agents along the value chain of the AI product. 
Data protection and data security are some of the most prominent weak 
pointsoftheuseofAI.In2018,theGermanFederalOfficeforSecurity(BSI) 
observed that the number of cyberattacks on critical infrastructure tripled in comparison with the previous year. This is why cybersecurity is becoming increasingly important in order to protect the power grid [7]. 
Many end users are critical to AI, especially in relation to smart home technologies. Thebiggestobstacletotheacceptanceofsmartmetersisfear ofrevealing private information withoutknowing exactly how itisused.There isstillnoregulation in many countries on how to handle these sensitive data. 
Chapter 3. What the Energy Sector Must Do to be Compliant with the Seven Key Requirements 
According to [21], the cost reduction in the renewable energy industry, mainly based on wind turbines and solar photovoltaics, is given by the Paris Agreement [23] and the energy scenarios [22]. The European Union has setanobjective toreduce the total domestic greenhouse gas (GHG) emissions by 80% in 2050 compared to 1990 levels. 
3.1. Concrete and Practical Steps that the Energy Sector Must Take to be Compliant 
AI systems and their outcomes should be defined and detailed in the contracts, i.e., an agreement between private parties creating mutual obligations enforceable by law. Law includes also  private law,  i.e., the terms of the agreement between the parties who are exchanging promises. Therefore, it should, meet the EU s safety rules (existing aswell aspossiblecomplementary ones). There are nostandards yet, buttheyare under development by, for example, ISO and IEEE (see Appendices). The contract/agreement should consider almost all of the following issues that the law will enforce: 
  
Bodies: almost all of the property and contractor 

  
Insurance: the contractormustcontractinsurance for safety, and insurance(s) must include to the property. 

  
Responsibilities: responsibilities are all from thecontractorand the contractor shows to the property the organizational chart and the responsibilities. 

  
Standards and laws 

  
Guaranties and penalties 

  
Must be fixed in the appendix of the contract. They are applied to the contractor 

  
The contractor mustreport enough detailed information to the property weekly and monthly by detailed report about the AI activities, dataset and any incidents 

  
Any new AI activity by the contractormustbereported tothe property and to have its approbation. 

  
Any loss, theft, accident etc.regarding toAI hardware/software, i.e., AI system, is the responsibility of the contractor. 

  
It shouldbedesirabletohave thesupportofprofessional andindependent mechanismstoguarantee therequirements oftheAIproducts, e.g., agencies, auditors, etc. 


According to the EU [13], the contract should also include (see also Section 2.2): 
--The use of the products or services where the AI system is applied must be safe. --The datasetsshouldbeavailable uponrequest for any competent authorities, e.g., inspections. --The process should protect confidential information (e.g., trade secrets). --The datasets employed totrain the AI systems need toconsider all of the scenarios required and to be sufficiently broad. According to the IEEE, the following should also be considered [24]: 
  
The code/hardware track record employed in the AI product. 

  
Definitions of the roles and responsibilities of every agent involved in the value chain. 

  
According to the previous point, all agents will need to have the information and formation needed to understand their responsibilities and the legal consequences of their work. 

  
All of the rules and standards must be created andapplied according to the laws andregulations system where theAIproduct couldhave any consequence or effect. 


It isalsodesirablethateverybody involved inthevalue process oftheAIproduct preserve documentationrelated to procedures, certifications, decisions, etc., almost for the period that should be defined in any caseby the responsible agency, in casethat it can be required for any authority. 
Concrete and practical steps for AI Technology in improving RES based on the seven key requirements: 
--Smart-grid-based energy storage optimization solutions (K1);Energy storage systems, in termsof large-scale, aggregated small home battery orplugged-in electricvehicles are key solutionsfor renewable integration.AIcansupportthese solutions more efficiently, thereby maximizing RES integration, including the reduction of forecast errors, minimizing prices for electricityconsumers, and maximizing returns for theowners of the storage systems. The speed of complexity of managing energy storagesystems inadynamicenvironment requires advanced AItechniquesand algorithms, which are able to extend the battery life. 
--Improving the integration of (hybrid) microgrids (K2):AIcanhelp with the integration of microgrids and managing distributed generation. The AI-powered control system canplay avitalrole in solving the quality and congestion issues, balancing the energy flow within the hybrid microgrids (AC and DC). 
--Better algorithms for energy forecasting (K1, K2) for PV systems and wind turbines integrated into power systems. Big data, machine learning (ML), and AI can produce accuratepower generationforecasts thatwillmake itfeasible tointegrate muchmore renewable energy into the grid. For system operators, accurate forecasting canimprove unitcommitment,thereby increasing dispatch efficiency andreducing reliability issues. 
--Digitalization to support the energy sector (basedonthe European Green Deal): AI and ML become increasingly more important in the energy sectormaking the industry more efficient and secure by analyzing and evaluating the large amount of datafasterandmore accurately. Typical areas ofapplication are electricity trading, smartgrids, heating, and transport, etc.Digital technologies cansupport the energy sector in several ways, including better monitoring, operation, and maintenance of RES assets,more system operation and real-time control strategies, and implementation of new market design etc. 
--Integration of electromobility- K2, K4 AIand big data techniques canease the full integration of theelectrified mobility with the smart grid and RES by enabling anopportunistic charging strategy where state-of-charge of on-board battery energy storage (BES) is monitored and charging/discharging phases are coordinated. This way thesmartgrid does notcollapse (avoiding excessive simultaneousrequests of vehicle recharging), vehicle recharging time can be minimized, and there is the possibility (e.g., in parking areas) of bidirectional energy transfer, i.e., from one vehicle BES to another, or from vehicle BES to the grid. 
--Coordination of maintenance work and determination of optimal times for maintenanceofnetwork (K2).Thishelpsinminimizingcostsandlosses.For example, by detecting anomalies in generation, consumption, ortransmission, AI can stabilize the power grid by developing suitable solutions. 
--Software platforms and tools (K1) thatleverages AItooptimizeenergy consumption:OPTIMAX from ABB [25]isascalableandhighly flexibleenergy managementindustrialplatform, which caneasily beintegratedintoexistingand complex infrastructures toimprove energyefficiencyinsmartbuildings,smart transportation, and smart homes. 
--The introduction of advanced and intelligent technologies into the power sector needs to be weighed up against cybersecurity. Policymakers need to strike a balance between supporting the development of AI technologies and managing any risks from malicious actors (K7). AI algorithms and tools canimprove their knowledge soasto understand threats and cyber risks, minimizing and reporting the negative impacts. 
3.2. Case Studies in Energy Sector Considering the Ethical Principles and Guidelines 
Buildinganin-house infrastructure from the ground up is costly, heavy onresources, and largely dependent onspecific skills many industrial companies lack. But there isa solution: industrial IoT platforms that give asolid foundation and necessary tools to unroll predictive maintenance activities. Varying inscope and the set of features offered, theyusually provide companiesthefollowingcapabilitiesandservices:device management to connect hundreds of thousands of sensors and meters on one platform; support for industrial messaging protocols; software development environment, tools, and APIs to integrate with existing enterprise solutions; scalable data storage and abig dataprocessing engine; analytics engines andmachinelearningasaservice; Digital Twin technology  visualizations of the equipment s condition in real-time; and ready to-use asset management software and analytical engines tailored for tasks. 
Case studies: 
--Smart-grid-based energy storage optimization solutions (K1).For example, in Australia Tesla s battery in its first year of operation generated an estimated USD 24 million in revenue, whilealso providing areduction of between USD 40-50 million in frequency control ancillary service costs [26]. 
--Better algorithms for energy forecasting (K1, K2).IBM was abletoshow animprovement of 30% in solar forecasting while working with the U.S. Department of Energy s Sun Shot initiative in 2015. A successful example for accurate variable RES forecasts is that of the EWeLiNE research project using ML-based software in Germany in 2017 [27]. 
--Predictive maintenance (K2) basedonAIandMLtechnologiesfor: early fault detection by using real-time platforms (for using wind farm and solar plant data), equipment failure, condition monitoring systems and production reliability. The goal of predictive maintenance is to optimize the balance between corrective and preventative maintenance,by enabling just intimereplacement ofcomponents.Thisapproach replaces only those components when they are close to failure. By extending component lifespans (compared to preventive maintenance) and reducing unscheduled maintenance and labor costs(over corrective maintenance), businesses cangaincostsavings and competitive advantages. 
An example of predictive maintenance applied in the energy industry: predictive analytics can take sensor data from a wind turbine or PV system to monitor the components and to predict with high accuracy when the system need maintenance. GE in Japan with the help of AI succeeded in enhancing wind turbineefficiency,reducingmaintenancecostsby20%andincreasingpower output by 5% [4]. 
--Energy consumption profiling: The issue of big energy companies using AI and big data to track consumers  behavior is critical: ononeside there is the opportunity for an accurate (in space and time) estimation of energy demand to optimize the trade-offamong energy comfort for theusers,energyefficiency for thegrid,andenergy saving for the planet. On the other side, there is the risk of unbalanced power between the two parties, big energy company vs. EU citizen asanenergyuser, with threats of privacy andconfidentialityviolationfor thelatter (K1, K2, K6). Hence, regulation related to big data and AI usefor energy consumption profiling should ensure  equal power  to data access and management. 
--Local energy communities have the right to access and manage the energy they created (K1, K4, K6). Concernedby the eventsat Chernobyl in 1986, the citizens of Sch nau ([28]),avillage of approximately 4,400 inhabitants in the German stateof Baden-W rttemberg, proposed more reliance onRES instead of nuclear power plants. When their idea was dismissed by the local energy provider, the citizens sought to take back the franchisefor power supply [29]. They had to do so over the objections of their own localgovernment, which hadobtainedafranchisefee from theprovider. Aftertwo referendums, thecitizensfounded acooperative calledElektrizit tswerke Sch nau (EWS) in 1996 that was granted the concession. Instead of relying on coal and nuclear power asthe incumbent utility had done, EWS s members learned how tousesolar PV andgeothermalpower tosupply Sch nau.Facilitated by theliberalizationofthe electricity industry around this time, EWS also began to win concessions in other parts of the country and currently operates nine electricity and gas networks in Germany. In addition to its own generation, EWS also purchasesrenewable energy from independent producers inAustria, Germany, and Scandinavia tosupply its end-use consumersall while maintaining acooperative governance structure. Thisisrelated toK4from Chapter2regarding transparency: EWS notonly provides itscustomerswitha certification of how their energy is sourced but also involves its members in its decision-making processes. While the desire for amore environmentally friendly energy system instigated the formation of EWS, its successultimately hinged onregulation that was crafted to break the power of incumbent monopolies and provided a level playing field. Nevertheless, EWS may never have been launchedif its founders had been unable to buy out the concession from the incumbent at the appraised value of DM 8.7 million even though another appraiser had it valued atDM 4 million. This is related toK6 from Chapter2aboutreducing barrierstoentry for environmental solutions:asa pioneer, EWS had to litigate its way into existenceand was fortunate that its takeover of the Sch nau franchise coincided with electricity industry liberalization in 1998. 
Thus, it was able to become financially viable by expanding to the extent that it now supplies some170,000 consumers[28]. Future environmental solutions should not face such barriers, and regulation for them could be based onthe existing Article 16aboutenergy communitiesintheEUDirective 2019/944 [13] ontheinternal market for electricity. In particular, it prevents Member States from barring local energy communities from access to energy markets and ensures that their financial settlements take place inatransparent manner. Moreover, the implementation of atrustworthy AI paradigm canbe an opportunity to facilitate the creation of a community of prosumers, i.e., local energy communities where citizens act as both energy producers and consumers and have the right to access and manage the energy they created. 
--A network platform   K1, K6 (IoT middleware platform-based cloud computing)orinfrastructure tobe able toexchange the energy produced. Verdigris Technologies offers a software platform that leverages AI to optimize energy consumption incommercial buildings.SomehotelsinSanFrancisco, USA, thatusedtheappto identifyenergy insufficiency in their commercial kitchens confirmed that within three months of using the application inefficiencies were identified that were costing them over USD 13.000 in preventable annual losses. 
--Data protection by default and data protection by design (K3).Some social issues, such asdiversity, nondiscrimination, and fairness, need to be taken into account inthetechnical design process andimplementation ofAI.Article25oftheGDPR requires organizationstoimplement bothdataprotection by design andby default. Thismeansthat legal andcompliance issues need to be considered from the beginning of a project, and that privacy must be a deciding factor in implementation choices. 
However, detailedinvestigation ofsmartgridprojects reveals thatpractice sometimesfallsshortofthis.For example, Brown [30] examinedtheBritish smart meter program and found that little attention was paidto privacy in the early phases of its development. It becameamore significant feature of the design after apublic consultation process. Similarly, Murphy [31] found that the importance of privacy by design was not fully appreciated in the early stages of the Irish smart metering program, and it was neitherakey design principle noranevaluation criterion. Privacy was seen assomething to be balanced against other competing interests, in azero-sum fashion, which is acontradiction of the privacy by design concept. However, Cavoukian [32] highlights how someproviders are engaging fully with privacy, suchasthe German supplier Vattenfall, which included adata privacy representative throughout all stages of the design of their smart meters, and has highly restricted the useof data from the system; and the grid operator Alliander, which invested heavily to obtain a Data Privacy and Security certification as part of a consumer reassurance campaign. 
--IBM, Microsoft, and the AEE Institute in the last two years tried to respond toemergingthreatsdevelopingdifferenthardwareandsoftwaresolutionsagainst cyberattacks,which canbe used toautomateand secure demand-side management basedonIoT, thereby minimizingthenegative impacts (K7). Protections under development aimtomake thedistributionelectricity system more resilient against cyberattacks. This meets the EU s safety rules (existing as well as possible complementary ones) and the standards set.These solutions aim toguarantee the AI with regard to network security, interoperability, privacy protection, industry applications, and other technical standards. In the caseof China, it is in accordance with its plan to promote the self-discipline in the industry. These initiatives aim to reduce the damaged caused by AI systems. Some other macro-initiatives are being carried out in the EU [10], e.g., France has established agroup of experts toverify algorithms and databases and to improve understanding incivil society, thereby creating a consultative ethics committee for digital technologies and AI, which would organize public debate in this field. In a similarvein, the U.K. has created aCentre for Data Ethics and Innovation that will be tasked with ensuring safe, ethical, and ground-breaking innovation in AI and data-driven technologies, etc. 
Chapter 4.Conclusion, Practical Recommendation and Obligations 
Practicalrecommendation and obligations emerge from the analysis onhow to utilize the laws, standards, and regulation to accommodate the newcapacities, practices, and behaviors. Many oftheserecommendations and obligationsare underpinned by the GDPR, which requires respect for fundamentalrights (recital 2), adequate security (Article 5 (1)(f)), good data governance (Article 26), transparency (Articles 12 to 14), fairness (Article 5(1)(a)), and accountability (Article 5(2)). 
Practical recommendation for Energy Industry based on the seven key requirements (K1-K7): 
K1. Recommendation for Human agency and oversight   including fundamental rights, human agency, and human oversight: 
Uses of AI in the energy industry must take account of the fact that individuals and groups willhave divergentbutstilllegitimate goals,andallow thepossibilityof exercising choice.AImustenablebetter, more informed humandecision-making, not take away control. RESmustbedesignedsothatunusualevents orfluctuationsare brought tothe attention of operators asquickly aspossible, and intervention is always possible,so thatrunaway algorithms orcascading failures are avoided. Human oversight is also animportant mechanism for identifying security breaches and risks, and therefore systems should be both auditable and regularly audited. 
K2. Recommendation for Technical robustness and safety   including resilience to attack and security, fall-back plan and general safety, accuracy, reliability, and reproducibility: 
Predictions basedonAItechniquescanbeusednotonly for energy production, storage and distribution, but also for energy trading, e.g., to find the right compromise between energy production-selling price-user consumption in afree but transparent energymarket. AIshouldcontributetotheimprovement ofsafety andtechnical robustness, as well as the identification and mitigation of attacks. 
Fingerprinting and anomaly detection is the first step toclassify faults and cyber attackscorrectly thereby providing afall-back plan toensure safety of the energy system. AI-based fingerprintingandanomaly detectioncanbeusedfor notonly energy production, storage, anddistributionplants, butalsotodetectanomaliesduring energy trading sessions for a transparent energy market. 
K3.Recommendation for Privacy and data governance   including respect for privacy, quality, and integrity of data, and access to data: 
To give the energy industry and in particular end-use consumers more confidence in AI(e.g., inrelation tosmart hometechnologies-smartmetersoroptimizationof charging infrastructure in smart recharging grid for electrified mobility), it must be clearly communicated how the data are used and by whom, while data security must be guaranteed. In addition to compliance with ethical guidelines, it is essential that AI in the energy sector complies fully with Standards (IEEE P7002) and European law, particularly the General Data Protection Regulation (GDPR), and applies the Data Protection Impact Assessment Template prepared by the Commission s Smart Grid Task Force. There should be clear security requirements for participants in the electricity market toensure dataconfidentiality, integrity, and authentication along the whole chain of dataacquisition,processing, andstorage.Thiscanbeachieved by definingbest practices to be followed for security of data taking note of the different roles: users, production industries, companies working asserviceproviders andinstitutions (standardization bodies, regulatory authorities, and governments). 
K4.Recommendation for Transparency   including traceability, explainability, and communication: 
Marshaling thisexisting expertise together withalgorithmicadvances ininverse optimization could be used by auditors to unveil how AI applications function in the energy sector and to ensure that their use is in line with social objectives. 
K5. Recommendation for Diversity, non-discrimination and fairness   including the avoidance of unfair bias, accessibility and universal design, and stakeholder participation: 
Inorder to ensure non-discriminatory programming and functioning, systems need tobetrainedandtestedfor unfairbias.Therefore, companiesshouldtesttheir algorithms for bias and discrimination and demonstrate that certain fairness standards are met. The IEEE P7003 standard canserve asabaseline to address and eliminate issues of harmful bias in development of AI algorithms. 
K6.Recommendation for Societal and environmental wellbeing   including sustainability and environmental friendliness, social impact, society, and democracy: 
AI in power trading helps toimprove forecasts (basedonweather data, historical data, consumption profiles and mobility patterns) aswell asfacilitate and speed up the integration of RES (green energy). Better forecasts also increase grid stability and thus security of supply. Regulatory authorities  useof AI would add another layer of intelligence that could anticipate the actions of market participants and tweak market designs to discourage deviations from societal objectives. 
K7. Recommendation for Accountability   including auditability, minimization, 
andreportingofnegativeimpact,trade-offs,andredress: 
Accountability shouldbeclearly linked totheothersixkey requirements, mainly effectiveness, competence,andtransparency. Theycannotbeconsidered only individually; instead, they mustbetreated asawhole toensuring ethical and legal compliance, and the technical safety and reliability of the AI products. TheGlobalInitiative onEthics ofAutonomous andIntelligentSystems from the Institute of Electrical and Electronics Engineers (IEEE) canserve asabaselineto address requirements for accountabilityoftheenergy industry. For instance,the IEEE-P7001 standard can serve as a baseline to addressrequirements for transparency and accountability of the energy sector. Energysectorcompaniesshouldtransparently communicate andreport negative impacts of the AI products. 
References 
[1] European Commission,  Building trust in human-centric artificial intelligence  (Communication from the Commission to the European Parliament, the Council, the European economic and social committee and the committee of the regions) COM (2019) 168.es 
[2] European Commission,  Artificial intelligence for Europe  (Communication from the Commission to the European Parliament, the Council, the European economic and social committee and the committee of the regions) COM (2018)_237e.n 
[3] High-Level Expert Group on Artificial Intelligence,  Ethics Guidelines for Trustworthy Artificial Intelligence  2019. [Online]. Available: https://ec.europa.eu/futurium/en/ai-alliance-consultation 
[4] European Commission,  On Artificial Intelligence   A European approach to excellence and trust  2020 t. [Online]. Available: https://ec.europa.eu/info/sites/info/files/commission-white-paper artificial-intelligence-feb2020_en.pdf 
[5] Smart Grid Task Force Expert Group 2,  Data Protection Impact Assessment Template for Smart Grid and Smart Metering systems  (2018). [Online]. Available: https://ec.europa.eu/energy/sites/ ener/files/documents/dpia_for_publication_2018.pdf 
[6] Smart Grids Task Force Expert Group 1,  My Energy Data  (2016). [Online]. Available: https:// ec.europa.eu/energy/sites/ener/files/documents/report_final_eg1_my_energy_data_15_ november_2016.pdf 
[7] Smart Grid Task Force Expert Group 2,  Recommendations to the European Commission for the Implementation of Sector-Specific Rules for Cybersecurity Aspects of Cross-Border Electricity Flows, on Common Minimum Requirements, Planning, Monitoring, Reporting and Crisis Management  (2019). [Online]. Available: https://ec.europa.eu/energy/sites/ener/files/sgtf_eg2_report_final_ report_2019.pdf 
[8] Institute of Electrical and Electronics Engineers,  Ethics in Action in Autonomous and Intelligent Systems  https://ethicsinaction.ieee.org (accessed Sep. 17 2020). 
[9] R. Fern ndez-Blanco, J. M. Morales, S. Pineda,  . Porras,  Kernel-Based Inverse Optimization: Application to the Power Forecasting and Bidding of a Fleet of Electric Vehicles . arxiv.org. https:// arxiv.org/abs/1908.00399 (accessed Sep. 17 2020). 
[10] S. Samuel,  A new study finds a potential risk with self-driving cars: failure to detect dark-skinned pedestrians . Vox.com. https://www.vox.com/future-perfect/2019/3/5/18251924/self driving-car-racial-bias-study-autonomous-vehicle-dark-skin (accessed Sep. 17 2020). 
[ 11] M.A. Van Sluisveld, A.F. Hof, S. Carrara, F.W. Geels, M. Nilsson, K. Rogge, B. Turnheim, and 
D.P. van Vuuren,  Aligning integrated assessment modelling with socio-technical transition insights: An application to low-carbon energy scenario analysis in Europe,  Technological Forecasting and Social Change vol. 151, pp. 119177, 2020. 
[12] Nasdaq,  Nasdaq Renewable Index Wind Germany . nasdaqomx.com. http://www.nasdaqomx. com/transactions/markets/commodities/renewables (accessed Sep. 17 2020). 
[13] Directive (EU) 2019/944 of the European Parliament and of the Council of 5 June 2019 on common rules for the internal market for electricity and amending Directive 2012/27/EU. OJ L 158, pp. 125 199, 14 Jun. 2019. 
[14]
 Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing directive 95/46/EC. OJ L 199, pp. 1 88, 4 May 2016. 

[14]
 L. Delponte and G. Tamburrini,  European artificial intelligence (AI) leadership, the path for an integrated vision,  European Parliament, 2018. 


[15] Directive (EC) 2001/95 of the European Parliament and of the Council of 3 December 2001 on general product safety. OJ L 11, pp. 4 17, 15 January 2002. 
[16] Executive Order 13859 on maintaining American leadership in artificial intelligence. 2019. 
[17] NIST.  U.S. Leadership in AI: A plan for federal engagement in developing technical standards and related tools , National Institute of Standards and Technology. U.S. Department of Commerce, 2019. 
[18] G. Webster, R. Creemers,; P. Triolo, and E. Kania,  Full translation: China s  new generation artificial intelligence development plan  (2017) . Digi China, August 2017, 1. 
[19] Russian Federation, Decree of the President of the Russian federation on the development of artificial intelligence in the Russian Federation. Office of the President of the Russian Federation 2019. 
[20] The Conference toward AI Network Society. Draft AI R&D guidelines for international discussions. The Conference toward AI Network Society 2017. 
[21] B. Steffen, M. Beuse, P. Tautorat, and T.S.Schmidt,  Experience curves for operations and maintenance costs of renewable energy technologies,  Joule, vol. 4, pp. 359 375, 2020. 
0ys[22] IRENA 2019-International Renewable Energy Agency, Innovation landscape brief: Artificial Intelligence and big data, Abu Dhabi, Report available online: www.irena.org/publications 
 s.g[23] J. Rogelj, M. Den Elzen, N. H hne, T.Fransen, H. Fekete, H. Winkler, R. Schaeffer, F. Sha, K. Riahi, and M. Meinshausen,  Paris agreement climate proposals need a boost to keep warming well below 28C,  Nature vol. 534, pp. 631-639, Jun. 20169. 
[24] IEEE Standards Association,  The IEEE global initiative on ethics of autonomous and intelligent systems.  IEEE.org. https://standards.IEEE.Org (accessed Sep. 23, 2020). 
[25] Optimax - ABB Mission to Zero, www.abb.com/mission-to-zero-optimax 
[26] M. Mezengarb (2019),  Tesla big battery paves way for artificial intelligence to dominate energy trades.  Renew Economy, http://reneweconomy.com.au/tesla-big-battery-paves-way-for artificial-intelligence-to-dominate-energy-trades-31949 (accessed Sep. 23, 2020). 
[27] EWeLiNE research project,  Renewable energy generation forecasting , Germany. 
[28] https://www.ews-schoenau.de/oekostrom/ 
[29]https://www.next-kraftwerke.com/knowledge/artificial-intelligence. What is Artificial Intelligence in the Energy Industry? 
[30] I. Brown,  Britain's smart meter programme: A case study in privacy by design,  International Review of Law, Computers & Technology vol. 28, pp. 172 184, 2014. 
[31] M. H. Murphy,  The Introduction of Smart Meters in Ireland: Privacy Implications and the Role 
of Privacy by Design,  Dublin University Law Journal vol. 38, pp. 191 207, 2015. s[32] A. Cavoukian,  Smart meters in Europe: Privacy by Design at its best,  Information and Privacy Commissioner of Ontario, 2012.
 . . ..n. .. 50H. e 1. s ) xI n44 6,l.6A t ann5l g 2 Appendix I. Standard and/or project under the direct responsibility of ISO/IEC secretariat related to 
AI 
 ISO/IEC
 CD TR 20547-1: Information technology   Big data reference architecture   Part 1: Framework and application process 

 ISO/IEC
CD 22989: Artificial intelligence   Concepts and terminology 

 ISO/IEC
 CD 23053: Framework for AI Systems Using Machine Learning (ML) 

 ISO/IEC
 AWI 23894: Information Technology   Artificial Intelligence   Risk Management 

 ISO/IEC
 AWI TR 24027: Information technology   Artificial Intelligence   Bias in AI systems and AI aided decision making 

 ISO/IEC
 PRF TR 24028: Information technology   Artificial intelligence   Overview of trustworthiness in artificial intelligence 

 ISO/IEC
 CD TR 24029-1: Artificial Intelligence (AI)   Assessment of the robustness of neural networks   Part 1: Overview 

 ISO/IEC
 CD TR 24030: Information technology   Artificial Intelligence (AI)   Use cases 

 ISO/IEC
 AWI TR 24368: Information technology   Artificial intelligence   Overview of ethical and societal concerns 

 ISO/IEC
 AWI TR 24372: Information technology   Artificial intelligence (AI)   Overview of 

computational approaches for AI systems 

 ISO/IEC
 AWI 24668: Information technology   Artificial intelligence  Process management framework for Big data analytics 

 ISO/IEC
 AWI 38507: Information technology   Governance of IT   Governance implications of the use of artificial intelligence by organizations 

Appendix II. IEEE P7000 standards projects The following standard projects are being done: 

 
 IEEE P7000  - IEEE Standards Project Model Process for Addressing Ethical Concerns During System Design. 

 
 IEEE P7001  - IEEE Standards Project for Transparency of Autonomous Systems. 

 
 IEEE P7002  - IEEE Standards Project for Data Privacy Process. 

 
 IEEE P7003  - IEEE Standards Project for Algorithmic Bias Considerations. 

 
 IEEE P7004  - IEEE Standards Project for Child and Student Data Governance. 

 
 IEEE P7005  - IEEE Standards Project for Employer Data Governance. 

 
 IEEE P7006  - IEEE Standards Project for Personal Data AI Agent Working Group. 

 
 IEEE P7007  - IEEE Standards Project for Ontological Standard for Ethically Driven Robotics and Automation Systems. 

 
 IEEE P7008  - IEEE Standards Project for Ethically Driven Nudging for Robotic, Intelligent and Autonomous Systems. 

 
 IEEE P7009  - IEEE Standards Project for Fail-Safe Design of Autonomous and Semi-Autonomous Systems. 

 
 IEEE P7010  - IEEE Standards Project for Well-being Metric for Autonomous and Intelligent Systems 

 
 IEEE P7011  - IEEE Standards Project for the Process of Identifying and Rating the Trustworthiness of News Sources 

 
 IEEE P7012  - IEEE Standards Project for Machine Readable Personal Privacy Terms 

 
 IEEE P7013  - IEEE Standards Project for Inclusion and Application Standards for Automated Facial Analysis Technology 


4 
HEALTHCARE 
Applying a Human-Centered Approach to Assess Risks of Using AI Systems in Healthcare 
Authors 
Raja Chatila 
Chairman Healthcare Committee, AI4People; Professor and Director of the Institute of Intelligent Systems and Robotics (ISIR) at Pierre and Marie Curie University in Paris, France 
Stephen Cory Robinson 
Senior Lecturer/Assistant Professor in Communication Design at Linking University, Norrkoping, Sweden 
Donald Combs 
Vice President & Dean of the School of Health Professions, Eastern Virginia Medical School, USA 
Paula Boddington 
Senior Research Fellow, New College of the Humanities London, UK 
Herv  Chneiweiss 
Directeur de Recherche au CNRS, Paris, France 
Eugenio Guglielmelli 
Senior Advisor on Publications for IEEE RAS Professor of Bioengineering Prorector for Research Founder, Research Unit of Biomedical Robotics and Biomicrosystems Universit  Campus Bio-Medico di Roma 
Danny van Roijen 
Digital Health Director at COCIR 
Jos Dumortier 
Honorary Professor of ICT Law at the University of Leuven, Belgium 
Leonardo Calini 
Policy Manager, European Government Affairs at Microsoft 
1. Introduction 
On April 8, 2019, the High-Level Expert Group onAI (HLEG-AI) appointed by the European Commission issued the  Ethics Guidelines for Trustworthy AI  (European Commission, 2019a). On June 26, 2019 the group issued the  Policy and Investment Recommendations for Trustworthy AI  (European Commission, 2019b). 
In its Ethics Guidelines, the HLEG-AI has identified seven requirements considered key for the design, development, deployment and useof AI systems. AI-based systems complying with these requirements would be considered to be trustworthy and aligned with a human-centered approach. 
The HLEG advocated that these requirements become anecessary condition for the adoption of AI systems in Europe. 
InitsWhitepaper entitled OnArtificialIntelligence -AEuropean approach to excellence and trust released onFebruary 19,2020,theEuropean Commission summarized its planned AI policy asincluding Policy options for a future EU regulatory framework that would determine the types of legal requirements that would apply to relevant actors, with a particular focus on high-risk applications.  (European Commission, 2020a) 
The risk-based approach defined in the Whitepaper is actually based on a two-tier definition of risk. 
To be considered  high-risk , the AI system must be deployed in asector known to be high-risk, e.g., the healthcare sector. Second the AI system must be used within this sector in an application, which is itself considered high-risk. 
A subsequent report by the HLEG-AI,  The Assessment List for Trustworthy Artificial Intelligence (ALTAI), was publishedonJuly 17, 2020 in aneffort to provide aninitial, more concrete, approach toevaluating compliance withtheEthics Guidelinesfor Trustworthy AI systems (European Commission, 2020b). The HLEG-AI stated that: 
 TheAssessment Listfor Trustworthy AI(ALTAI) isintendedfor flexibleuse: 
organisations can draw on elements relevant to the particular AI system from this 
Assessment List for Trustworthy AI (ALTAI) oradd elements toitastheyseefit, 
taking into consideration the sector they operate in. It helps organisations understand 
what Trustworthy AI is, in particular what risks an AI system might generate, and 
how to minimise those risks while maximising the benefit of AI. It is intended to help 
organisations identify how proposed AI systems might generate risks, and to identify whether and what kind of active measures may need to be taken to avoid and minimise those risks. Organisations will derive the most value from this Assessment List (ALTAI) by active engagement with the questions it raises, which are aimed at 
encouraging thoughtful reflection toprovoke appropriate actionandnurture an 
organisational culture committed to developing and maintaining Trustworthy AI systems. It raises awareness of the potential impact of AI on society, the environment, consumers, workers and citizens (in particular children and people belonging to marginalised groups). It encourages the involvement of all relevant stakeholders. It helps to gain insight on whether meaningful and appropriate solutions or processes to accomplish adherence to the seven requirements are already in place or need to be put in place.  
ThecombinationoftheGuidelinesandtheAssessment Listprovide asolid foundation for theassessment of high-risk AI systems operating in high-risk sectors such as healthcare. 
Following its past work on ethics and governance for AI (Floridi, 2018; 
Pagallo,2019),AI4Peoplehasidentifiedhealthcareasoneofthestrategicsectors 
for the deployment of AI, and has appointed a working group to analyze how trustworthy AI can be implemented in this sector, which is considered high-risk. This paper examines how the seven requirements are relevant and can be used, along with other tools such as ALTAI, to assess risk in the deployment of AI systems in healthcare. 
Theaimistoillustrateapractical approach toassessingriskandtoprovide recommendations to stakeholders in this sector. It is important tonotehere that risk inhealthcare, andthusinusingAIinhealthcare, ismulti-dimensional. Thismulti dimensionality will be explored through two examples of usecasesthat illustrate how risk can be assessed. 
2. AI and healthcare 
Healthcare is complicated. It involves assessing the current and trending state of health amongpatientswithdiffering geneticmakeups, personalhistory, environmental exposure, behavioral patterns, social contexts, cultures, economic status, self-awareness and patterns of healthcare usage. 
Healthcare practitionersusevarious kinds of data for decision-making, including diagnostic laboratory and radiologic tests, written notes and electronic health records recounting patient interviews and anamnesis, data about the history of family health conditions, epidemiologic modelsofinfectious diseases,andknowledge ofavailable resources. They often usethese data in situations of high urgency. The amount of data available from all these sources overwhelms the processing capacity of practitioners. It istherefore nosurprisethatAIsystems findagreat numberofapplications inthis domain,where they promise faster and more comprehensive decision-making support than practitioners canmusterontheirown. One substantial clinical application of AI has been in the imaging professions (Ting, 2018) -radiology and sonography -where, thanks to data availability and to progress in the development of effective algorithms, AI systems have shown ahigh level of accuracy, helpingtoidentifytumorsin breast cancer, retinal diseaseandrecently even fastdiagnosisofCOVID-19 pulmonary diseases (McCall, 2020). 
Recognizing the progressthat has been made does not mean, however, that the AI systems shouldbealready considered fully trustworthy. Generally speaking,many algorithms based on deep learning techniques are considered blalck boxes (Castelvecchi, 2016;Barredo Arrieta,2020). Regarding interactionsofpractitionersandpatients with AI systems, there is little understanding of the degree to which practitioners defer to the algorithms. And, finally, the big question of who is ultimately responsible for the diagnosis, treatment and outcomes of healthcare has not been answered yet. 
3. Risk, danger and hazard 
The notion of risk used in the EU Whitepaper (European Commission, 2020a) and in the ALTAI (European Commission, 2020b) needs tobe clarified in order toanalyze how to appropriately assess AI-based systems in healthcare. 
A risk isapossibleharm, more orless foreseeable, measurable by aprobability of seeing adanger materialize, while the hazard is anunpredictable and unexpected event, even if it could be probabilistically modelled. 
A danger isthepresence ofafactorthatcompromises theintegrity, security, wellbeing, orexistenceofaperson,anentityoranobject.Adangermay remain without risk, if oneknows how toavoid it completely, while arisk always hasatits source adanger, which must be identified. For example, in healthcare, adanger might beaninfectious pathogenandtheriskisthefrequency withwhich anindividual develops the corresponding disease. 
A putative risk,notgrounded in scientific orempirical evidence, mustalso be distinguished from aproven risk. Indeed, aproven riskisnever zero. For example, although air travel is the safest form of transportation, the risk of aplane crash is not zero (the probability is estimated to be about 10-7 for current airliners). On the other hand,aputative riskcanbecomezero. For example, in 1836 Fran ois Arago, famous scientistandmathematicianasked authoritiestoprohibit people from ridingtrains because he foresaw a major danger for health beyond the speed of 27 km/h and while traversing tunnels (Arago, 1836). He believed, incorrectly, that the human body would not resist the pressures produced. 
The introduction of new technologies in our society can generate not only benefits, butalsodangers andriskswhich mustbeproperly assessedandmanaged.Thisis frequently thecase,even for very relevant andpopular technologies. For instance, automotive technologiesare widely diffusedandappreciated, but, according tothe World HealthOrganization (WHO, 2020), they are the first cause of death for citizens aged 5-29 years and they have aneconomic impact, mainly in termsofcostfor the healthcare systems, of 3% of the gross domestic product worldwide. Ensuring asafer system approach for allroad usersis one of the main goals of the UN 2030 agenda for Sustainable Development, which requires importantinnovation onautomotive technologies, including AI-based solutions forgiving human errors. Generally speaking, governing the introduction of new technologies inoursociety clearly requires also toelaborateandpromote guidelines, policies and regulatory issuestoprevent and mitigate potential risks. These examples call for aprecautionary approach toevaluate thereality ofdangers,andanevaluation oftheresulting risksothatunnecessary measures are not taken for nonexistent or very low risks, and appropriate measures are taken to mitigate proven risks. 
When we consider AI-based systems in healthcare, the stakes are high because we are dealing with human life. There are potential dangers, for example, an interpretation mistake inmedicalimagerycouldleadtoacancerous tumorgoing unnoticed; a mishandlingor lack of security measures for health data could lead to the disclosure of patient personal data. It is therefore important to correctly qualify the actual dangers of specific technical solutions and toaccurately evaluate the related risks sothat AI systems are deployed for the benefit of patients and society. 
The question is: how to define risk indicators that make it possible to identify if there isarisk at all. For example we could ask the  worst casescenario  questions: Are there catastrophic consequences of a system failure? And To what extent could the system be considered dependable, i.e. capable of mitigating associated risks for users ? By performing suchanalyses, theoccurrence probabilities ofthoseevents leadingtosystem failure have to be considered, and appropriate measures taken to reduce them to safely manage failure implications andprevent failure repetition. Thisprecautionary process can imply e.g., system redesign, different use protocols, clearer interfaces, user training, and assessment of user capabilities, These measures are classical in critical applications. 
The notion of  high  vs.  low  risk underlies akind of threshold, under which risk could become acceptable. However, using such abinary scale (high/low) might be toolimitedtoexpress risk impact diversity. A risk scale expressing damage intensity should be multidimensional,accounting for different values that could be atrisk. For example, dataprivacy, physical integrity, physical wellbeing, moralimpact.Ineach dimension,riskcouldbeevaluated takingintoaccountseveral parameterssuchas 
patient context, e.g., age, lifetime expectancy; medical history; healthcare general context, e.g.,availability ofmeansorequipment,orofalternative treatments; impact onthe healthcare system itself. Risk should also be considered over time, to assessshort term and longer term impacts. Finally, the scale should be a continuum to avoid arbitrary and a priori thresholds. This implies adegree of complexity that would be difficult to capture withabinary  high/ low  scale. 

Figure 1. Example of dimensions of risk (or atrisk) and factors that influence them. The time dimension is also tobe considered due to cumulative effects 
Thedanger/risk analysis assessmentandriskmitigationshouldbeperformed from theonsetof system specification, and continually, after deployment throughout the system s life cycle, taking into account standards and certification processes. 
A sound methodology should be developed tocorrectly make theseevaluations andtomitigaterisk.Inthedomainofsoftware engineering,solidconceptsand methodologieshave beenproposed todealwiththedependabilityorresilience of software systems. Dependability (Avizienis, 2004), defined as Delivery of service that canjustifiablybe trusted hasseveral attributes, including system availability (readiness for correct service), reliability (continuity of correct service) and safety (absence of catastrophic consequencesontheuser(s) and the environment).  Justifiably  means thatthere isagroundedandproven assessmentoftheseproperties. Thenotionof dangerunderliescatastrophic failure consequences.Limitingconsequencesoftask failure includesverification andvalidation techniques,suchaserror detectionand recovery mechanisms, modelchecking,detectionofincorrect orincomplete system knowledge, andresilience tounexpected changes duetoenvironment orsystem dynamics. There are meanstoreach these objectives, such assoftware system design diversity, redundancy, as well as software architectures enabling system state assessment for decision-making in order to produce error-free results. 
This last step may however be performed by ahuman specialist for example, and requires aspecific protocol. For example, the ALTAI (European Commission, 2020b) could be aguide here. This raises issues related to the organization and governance of the healthcare system, and not merely of apiece of software providing agiven service. Thecombinationofriskassessmentanddecision-making isactually asource of complexity, because there is a cost in making the systems fail-safe. Eliminating dangers, i.e.,reducingrisks while keeping benefits might indeed incur important investments in time and finances aswe canlearn from theaviation industry for example -hence the 10-7 probability of anairliner crash. But we canalsoseethis approach in healthcare, especially inthepharmaceutical industryandfor thedesignofmedicaldevices.AI systems addalevel of difficulty when they are basedonlearning methods, which are opaque and assuch challenge classical verification and validation techniques. A whole field of research currently addresses transparency and explainability issues of AI systems (Barredo Arrieta,2020). Also, healthauthoritieshave already issuedguidelinesfor assessingmedicaldevices which includeAI-based systems, e.g.,inFrance (Higher Health Authority, 2020). 
4. Case studies 
In order to discuss a risk-based approach, we analyze next two case studies that illustrate the use of AIsystems in healthcare in order to identify where the implementation of AI systems might bring potential benefits(improvements totreatment outcomesand diagnostic accuracy, healthcare system efficiency, etc.) and tohighlight ethical issues, specifically the seven key requirements for trustworthy AI identified by the HLEG-AI (which are thebasisfor theALTAI), when implementing AItechnologiesinthe healthcare sector considering a risk evaluation approach as defined in the EU Whitepaper. 
1.AI systems for patient triage and prioritization, also dealing with crisis situations such as the COVID pandemic; 2.AI systems for diagnosis. 
Case 1. Patient triage and prioritization 
When the waiting list of patients is quite long, and the diversity and urgency of healthcare that is sought is multifaceted, anAI system canhelp to compensate for the lack of adequate personnel todeal with the flow of patients. Recommendations from anAI healthcare system canhelp to thoroughly analyze patients  healthcare records in combination with their presenting symptoms. 
Basedonthesefactors,thehealthcare staffwillbeabletoprioritiseandtreat those with the mosturgent needs. Note also that emotion detection could be used in such systems, which would raise additional ethical issues which are beyond the scope of this paper (see (Grandjean, 2008; Greene, 2019)). 
Patient triage and prioritization canbe done through anAI system interacting directly withthe patients, with the healthcare personnel,orwith both. Question and answering systems, orchatbots,which are likely tobe the interface tothe AI system, willsortthepatientstotheappropriate level ofurgencythrough adialogue.The challenge is whether the AI embedded within, or connected to, the chatbot will reliably triage patients totheappropriate level ofcare. Triage involves acombinationof complicating factors--the communication skills of both the AI system and the patient, andtheassumptionofafactualdescriptionofthecurrent symptomsandrelevant physical and mental history to mention only two such factors. 
A chatbot  orArtificial Conversational Agent (ACA) is asoftware system that hasnaturallanguage processing (NLP) capacities enablingittoenterinadialogue witha user through akeyboard oravoice recognition and synthesis systems and could alsouseavisualavatar. One of the first such systems was ELIZA developed by Joseph Weizenbaum at MIT in the mid-1960s, which was based on using keywords and scripts. Interestingly, ELIZA s scriptswere basedonreformulating userinputsasquestions to her/him in a way resembling the communication strategies of Rogerian psychotherapists (Weizenbaum, 1966). 
Someofthemostknown andpopularchatbotstoday are commercial systems suchasAmazon s Alexa, Google Home, orApple s SIRI, connected to the Internet and thus able toaccessconsiderable data toanswer questionsortoconducte-commerce. Chatbots are also integrated in several specific systems, such as GPS car route planning or queries for travel companies on their websites. Some systems, such as those mentioned above, include alearning capability, see(Kim, 2018), enabling them to improve their response according tonewdata,previous choicesmadeby theuser, orexploiting inputs from other users. 
General ethical issues with Chatbots 
There are several ethical issues related to developing and using chatbots, and afew of themcanbe exacerbated when healthcare becomes the application domain. To list but a few: 
  
The users might not be aware that they are actually interacting with a computer program and not a human being. 

  
The chatbot s voice and/or appearance might have aspecifictoneoraspect that might influence user behavior. 

  
The chatbot s behavior will be based on algorithms, which might include AI and learning capabilities, and on a variety of data. Similarly to all such systems, the data may be biased and the chatbot language or behavior as well. 

  
Like all algorithms, including AI systems, a chatbot lacks semantics and does not actually understand what it is doing or what consequences its outputs might have on humans. 


Risk assessment for chatbots in healthcare 
Chatbotsare also used inhealthcare, e.g., in psychiatry (Philip, 2020). In ahealthcare context,onemustdistinguishthe operator  i.e.,themedicalprofessional (or organization), which deploys the chatbot, from the  user , the person who is going to actually interact with it through Q&A, from the device manufacturer. The operator is generally notaware about the internal workings of the system, but knows how touse itandwhat toexpectfrom it.Theuserisoftentotally ignorant oftheunderlying algorithms of the chatbot and its capacities. There are different issues to consider from these two perspectives. 
The consequences of chatbot advice ordecisionsmight be severe for theuser. One main issue is relatedtothe fact that the chatbot ignores the general contextin which it is used,andcanonly usespecificinformation aboutpatient condition and possibly their medical data. This does notmeanthe decisions canbewrong. On the contrary, sometimes, and often, the decisions are correct and suggest that the system hasbeenwell designedandtrained.However, theriskrelated towrong decisions remains high because of the consequences for the health of the patients. This has to be acknowledged asafactor for deploying chatbots and evaluating their conclusions by the operator. 
Furthermore, correct decisionswilltendtoincrease operators confidenceand trust in the system, perhaps leading them to not question the triage decisions over time. 
The chatbot might influence the user through the form and content of its questions andanswers,thereby inducing abias in the user s behavior, that may, in turn, produce a bias in the chatbot decisions. In the instance of anincorrect triage decision, that could prove catastrophic. 
Thechatbotmightnotbeabletosay  Idon t know  unlessit s explicitly programmed to do so, and might persist in forcing the dialogue to acquire additional data, orienting users  answers. This might produce inappropriate concluding decisions. 
Trustworthy AI elements 
Almost all of the seven requirements for  Trustworthy AI  need tobe considered in evaluating the chatbot AI system, given the ethical questions raised above. 
1.Agency: Patients (users) should be informed that they are dealing with a machine (see transparency) and should have the possibility toopt-out andtoaccessa human. Operators should be able to assess and validate the results of the chatbot decisions through metrics (e.g., confidence, performance, explainability). 
2.Technical robustness and safety: Chatbots should be verified and validated by certification bodies or trusted third parties. 
3.Privacy anddatagovernance: Datacollectedby thechatbotsandunderlying platform shouldrespect and comply with general regulations aswell ashealth data sensitivity (anonymity, proportionality, purpose of use, storage and access) as recognized by the GDPR (EU 2016/679). 
4.Transparency: The patients should be clearly informed that they are conversing with a chatbot and not with a human through an interface. The purpose of using achatbot should be stated. The professional operator should be informed about the system s decision process 
5.Diversity: The chatbot interface (voice, visual appearance, attitude) should be as neutralaspossible and its makers should not try to give the image of ahuman initsvisualappearancetoavoid confusion.Issues ofdiversity are relevant, specifically where facialrecognition andemotiondetectionare utilizedfor patient sorting. As documented widely, certain ethnic groups can suffer erroneous facial recognition detection, such as populations of color (Grother, 2019). These false-positives mightincludeinadditionincorrect emotiondetection,which jeopardizes the entire concept of fair patient sorting based on real-time behavioral responses including emotion, etc. 
6.Accountability: Accountability andliabilitymustremain withhumanbeings (designers, operators, users,etc.)andnotonthemachineitself.Indeed,AI systems should not have a legal personality. 
It is possible to examine the risk assessment process more thoroughly by applying the ALTAI toone of the trustworthy requirements. The fourth guideline for trustworthy AI addresses transparency, which encompasses three elements: 1) traceability, 2) explainability and 3) opencommunication aboutthelimitationsoftheAIsystem. Indeveloping an approach to assessing risk, the HLEG posed someillustrative questions that operators and users of AI inhealthcare might employ to identify and mitigate risk (European Commission, 2020b). Their discussion is worth excerpting in the next few paragraphs. 
Traceability 
This subsection helps to self-assess whether the processes of the development of the AI system, i.e.thedataandprocesses thatyieldtheAIsystem s decisions,isproperly documented to allow for traceability, increase transparency and, ultimately, build trust in AI in society. 
  Didyou putinplace measures thataddress thetraceabilityoftheAIsystem during its entire lifecycle? 
. 
Did you put in place measures tocontinuously assessthequality of the input data to the AI system? 

. 
Can you trace back whichdata was used by the AI system to make certain decision(s) or recommendation(s)? 

. 
Canyou tracebackwhich AImodelorrulesledtothedecision(s) or recommendation(s) of the AI system? 

. 
Did you put in place measures tocontinuously assessthequality of the output(s) of the AI system? 

. 
Did you put adequate logging practices in place to record the decision(s) or recommendation(s) of the AI system? 


Thiscouldtake theform ofastandard automatedqualityassessmentofdata input: quantifying missing values and gaps in thedata; exploring breaks in the data supply; detecting when data is insufficient for atask; detecting when the input data is erroneous, incorrect, inaccurate or mismatched in format  e.g., a sensor is not working properly orhealthrecords are notrecorded properly. Aconcrete example issensor calibration: the process which aims to check and ultimately improve sensor performance by removing missing or otherwise inaccurate values (called structural errors) in sensor outputs. This could take the form ofastandard automated quality assessmentof AI output:e.g.,predicted scores are withinexpected ranges;anomaliesinoutputare detected and input data leading to the anomaly detected and corrected. 
Explainability 
Assessing theexplainability of the AI system isasecond element of trustworthiness. Thiselementrefers totheabilitytoexplain boththetechnicalprocesses oftheAI system and the reasoningbehind the decisions or predictions that the AI system makes. Explainability iscrucialfor buildingandmaintainingusers trustinAIsystems. AI driven decisions totheextentpossible mustbe explained toand understood by those directly and indirectly affected, in order to allow for contesting of such decisions. An explanation astowhy amodel has generated aparticular output ordecision (and what combination of input factors contribute that) is not always possible. These cases are referred toas black boxes  (Castelvecchi, 2016) and require special attention. In thosecircumstances, otherexplainability measures (e.g. traceability, auditabilityand transparent communication onthe AI system s capabilities) may be required, provided thattheAIsystem asawhole respects fundamentalrights.Thedegree towhich explainability is needed -which depends onwhom it is intended to (Barredo Arrieta, 2020) -dependsonthe context and the severity of the consequences of erroneous or otherwise inaccurate output to human life (European Commission, 2020a). 
  
Did you explain the decision(s) of the AI system to the users? 

  
Do you continuously survey the usersif they understand the decision(s) of the AI system? 


Communication 
This subsection helps to self-assess whether the AI system s capabilities and limitations have been communicated to the usersinamannerappropriate to the usecaseat hand. This could encompass communication of the AI system'slevel of accuracy aswell asits limitations. 
  
Incasesofinteractive AIsystems (e.g., chatbots,robo-lawyers), doyou communicatetousersthat they are interacting with anAI system instead of a human? 

  
Did you establish mechanisms toinform usersabout the purpose, criteria and 


limitations of the decision(s) generated by the AI system? . o Did you communicate the benefits of the AI system to users? . oDid you communicate the technical limitations and potential risks of the 
AI system to users, such as its level of accuracy and/ or error rates? . oDid you provide appropriate training material and disclaimers tousers on how to 
. adequately use the AI system? 
Case 2. AI prediction ( and outcome/diagnosis reassessment) 
AI has the possibility for greatly changing healthcare  from cost savings through more efficient healthcare, preventing physician burnout by lessening administrative tasks and increasing direct patientcare -theuseofartificialintelligenceinthehealthcare environment willbevast andthepotential improvements immense.Specifically, the ability for AIto1) predict onsetof health conditions leading toproactive healthcare interventions, or2) through reaffirming orrejecting physician diagnoses, orfinally 3) assisting in patient healthcare record management, canultimately result in saving lives or increased patient quality of life. 
One scenario where AIcanimprove healthcare outcomesis by predicting onset of health conditions/diagnoses, leading to capabilities to proactively manage healthcare. For example, ifapatient isgenetically predisposed tocancer, AIcanbeutilizedin personalized medicine toflag the patient s risk of cancer, andrecommend healthcare interventions for the patient. These scenarios are nothypothetical, but could become reality in Denmark and Estonia. In Estonia, for example, there already exists precision prevention for breast cancerorcardiovascular diseases, this is enabled by the nation s biobank (Milani et al, 2015), which currently houses genetic information for 5% of the population.TheuseofAIcouldincrease thespeedofidentifying citizens/patients facing these potential diagnoses. Use of AI in routine patient diagnosis could also assist in basic research into disease including classification, prognosis, and treatment. 
A second scenario where AIcanbenefit both healthcare providers and patients is the ability to reconfirm or correct  healthcare findings (i.e., tests, diagnoses) -similar tothe partnership between the UK s NHS and Google s Deepmind AI (King, 2019), healthcare institutions mightutilizeAItoverifyphysician findings.Inscenariosof telemedicine,suchasvideo meetings,theAImightverify asmallsample ofdigital healthcare meetings to reaffirm that telemedicine is achieving similar, or on-par, results found in face-to-face healthcare. 
Athird scenarioentailsAIassistinginpatienthealthrecord management.For example, after apatient visits their physician, the resulting notesof the visit canbe automatically transcribed and added tothe patient s healthrecords. The AI canalso determine the nextappropriate steps for the patient, aswell asschedule appropriate follow-up visitcommunications. TheAIsystem provides adviceaboutthepatient condition and further actions to be taken by the physician. In essence, the AI assists the physician by managing the healthcare record. 
Given all these benefits from the adoption and implementation of AI in healthcare, there also come corresponding risks and consequences arising from AI in healthcare. 
Risks analysis for AI predictions and outcome assessment in healthcare 
Various types of risk exist. 
Some risks pertain to the quality of the data.AI could potentially both predict and reaffirm healthdiagnoses.However, someusesofAItoreach medicaldiagnosesand recommendations may beflawed; there willbeaneedfor ongoingresearch and transparency (Antun, 2020). 
Issues related to poor, erroneous, orincomplete patient data are significantwhen AI is utilizedfor maintaining patient records. Training data may be poor, for example, failingtoadequately reflect patient groups, may notadequately reflect variations in record keeping (Panch et al., 2019). 
Healthcare meetingsare complex human interactions andanAI system is likely tomissmany elements of human importance. Subtle biases may be incorporatedin data recorded in health records which the AI may a) fail to notice and hence reproduce orb) be able to detect and address (Char et al., 2018). Patients  medical data might be located in different electronic health record systems (EHRs enable patient data tobe digitally accessed in onecentral file, allowing stakeholders toseamlessly share, access andexchange apatient s health information (Shortliffe, 1999)), orinclude data from wearables and othersensors-AImight pose asolutionfor interpreting datafrom different sources and different classifications of data, however AI-based solutions are early and mainly used in medical research (Lotman & Viigimaa, 2020). 
Some risks may arise from the nature of the data required.Verification of results could be based onavariety of indicators including medical outcomesbut also patient and physician satisfaction. 
Assessingsuccessofsuchhealthcare meetingsthusmay necessitatefacial recognition andemotiondetectionAI.Thiscouldhave risksofdiscriminationand labellingofcertainpatients.Situations posingcertainrisksmay arisefrom the combination of human and AI expertise. 
Suppose aphysician disagrees with all orpart of the AI s recommendations -a system may not allow this; conversely a well-functioning system may be overridden. An institution should develop protocols to deal with such situations. 
Mitigation of many risks includes attention to issues beyond AI itself, both within and beyond the medical setting.Within a medical setting, some of these concern pre-conditions for the successful useof AI, someconcernpossible longer-term impacts of its use.For the AI to function effectively in managing patient health records and advising follow-up communication, prior work is needed integratingcomputing systems across different sectors. Without this, gains may be fragmentary and illusory (Panch et al., 2019). 
There isapossible risk of impact ondevelopingphysician s skills and learning from clinicalexperience, which would need to be monitored and addressed. This is also necessary for good communication from physician to patient regarding their condition and recommendations. 
There isariskoffocus oncertaintechnologysuchasAIattheexpenseof necessary work on other technologies and the importance of the clinics (symptoms and real-life experiences of the patient). 
Faster and easier detection ofvery early disease stages and focus onrisk carries benefits but its routine and long-term usealso complex questions pertaining to issues suchasriskperception andmedicalizationwhich may require relevant expertiseto address (Featherstone et al., 2020). 
Wider economic and legal issues may arise.Possible flagging/screening by insurance companiesofanindividual s terminalillnessbefore onset,may leadtoexclusion of thesepatientsfrom thehealthcare insurancemarket. Here, thepredictive screening capabilities of AI will notresult in saving lives orbetter healthcare outcomes, but in creating patient discriminationandpossiblesurveillance ofthosewithsensitive or undesirable healthcare conditions/disease/diagnoses (HIV, covid19, etc). 
A general unknown riskconcernsthe future of litigation and caselaw in medical practicefrom  badcases . What willhappentothecurrent relationship between a patient and individual physicians with the use of AI systems in recommending treatment? (Char et al., 2018). 
Trustworthy AI elements 
1. 
Human agency: such useof AI should notunnecessarily override individual medical judgement and autonomy. Protocols for dealing with mismatch between the judgements of physician and of AI systems raise the risk that this may not always function in the best interests of each individual patient, may for instance be guided by fear of litigation, and/or by focus oncertain audited risks rather thanonother less tangible riskswhich are not audited. Will there beroom for genuine difference of medical opinion (Char et al., 2018)? Further, patient and physician AIeducationwillbeneededfor humanstofully comprehend the impact of AIin healthcare  without AI literacy  humans are not able to fully embrace and protect their own agency. 

2. 
Technical robustness and safety: asoutlined in the last section, reliance onAI must not be premature. The AI systems deployed in medical care must be both technically robust, and their technical safety ensured through repeated auditing of such systems (Raji et al., 2020). Further, oneof the biggest problems facing citizensisthelackofinformation aboutthetypesofdataanalyzed inAI systems (Vinuesa etal.,2020). Bothissuescanbepartially rectified by mirroring the use public registers of algorithms used in Helsinki and Amsterdam allowing auditing ofsuchAIsystems (Johnson, 2020), allowing citizensto identifythedatabasesthattrainedthemodel,how individualsutilizethe prediction, description of how each algorithm is used, and how biasorrisks were assessed in the algorithms. 

3. 
Privacy and data governance: The protection of sensitive health data utilized in AI-assisted healthcare isnotonly powerful initsabilitytodeliver targeted, personalizedhealthcare, butalsohassignificantissuesfor potential discriminationorsurveillance ofpatients.Datashouldremain subjectedto GDPR rules which should be strictly applied. Healthcare institutionsare based 


4. 
5. 
6. 
7. 
on trust  trust in the physician, trust in the healthcare institution, and trust in the sanctity of patient data. When trust is broken (not an if , buta when ), it is key to identify whom s data was breached, which specific data (i.e. diagnoses or prescriptions), and subsequential potential for fraud or discrimination must be minimized with haste. Critically, individuals should be required to provide clear, meaningful consent for useof their data in healthcare making decisions powered by AI systems, which would enable a better data traceability. 
Transparency: Transparency and explanation to the physician will be needed at ahigh level. The importance of checking may be highest for patient groups withreduced capacity to understand the involvement of anAI system, such as those with cognitive impairment. 
Diversity: imposing uniformity onhealthcare records may becounterto nuancesneeded toaccommodate different groups. Conversely greater easeof personalized medicine and diagnosis may assist in fine-tuning diagnosis and treatment for groups whose diseasepresentationandtreatment may differ from theaverage ofthepopulation.Additionally, itbenefitsallhealthcare institutionstoensure that diverse trainingsetsare utilized in order tomake public healthdecisions. In facial recognition systems, we have seenalack of diversity in training setswhere the algorithms resulted in poor identification ofindividuals ofcolor(Maurer, 2017,Merler etal., 2017) andtherefore databases used for training must hold diversity (in the data) as sacrosanct. 
Societal and environmental wellbeing: increased diagnosis and medicalization canhave downsides aswell asbenefits, includingincreasing healthcare costs, weighed against increases inpreventative healthandpersonalizedmedicine which may save costsbothmonetaryandpersonalcoststothepatientof unnecessary or delayed treatment. Unknown risks relate to the possible impact onlitigation with complex questions for medical professionals, patients, and societyasawhole, including risksofincreasing litigiousness inmedicine. Societalwellbeing canbe jeopardized (including public trust) if debacles such asthe NHS   care.data  scandal are notlearned from (Vezyridis & Timmons, 2017). Individuals notabletopractice informed consentmustbe protected and prioritized, as well, as AI brings with it many issues of comprehension and public understanding. 
Accountability: There will be acertain amount that is unknown about how the law might develop in this area sohospital managers and those in charge will have aresponsibility tomonitor such situations carefully. Individual medical practitionersandpatientsalsoneedprotectionandcautionasthefull implications become apparent. 
5. General discussion and conclusions 
Theamountofinformation -from databasesofincreasing diversity, from the proliferation of sensors, and from smartphone-based apps linked toelectronic health records, just tomentionafew drivers of change -is beyond the capacity of human intellectual processing. For thatreason alone, there will be asteadily increasing useof AI applications by medical and health professionals and the organizations in which they work. These applications will improve healthcare, but they also have the potential to introduce new risksfrom AIfor bothpatientsandprofessionals. However, trustin purposeandinoperationisthefoundation for thedevelopment andadoptionof technologies, and AI is no exception. 
Given that healthcare isahigh risk sector, these additional sources of risk are beginningtobe addressed through the development of requirements for trustworthy AI andtools for assessing risk such asthe ALTAI. This report has examined the issue of risk and approaches to assessing and managing risk in healthcare. We have illustrated anapproach orusecaseastohow theseven elementsoftrustworthy AImightbe mergedwiththeALTAI listsdeveloped by theHLEGtoassesssomeoftherisks associated with the useof chatbots in ahealthcare setting. The primary argument is thataskingfocused questions aboutAIinaspecific application orsettingfrom the perspectives ofoperatorsanduserscanhelptodeterminerisk and trustworthiness. Ourpurposeislesstoprovide specific guidance for assessingtrustworthiness ofAI applications and more to suggest that developers and usersneed to take responsibility for developing an appropriate assessment process in their particular setting. 
In summary, the following findings have been ascertained: 
  
Complying withahuman-centered AIapproach canbeassessedthrough the compliance with the 7 key requirements. 

  
Risk is notbinary. There isamultidimensionality in its nature, acontinuum in its intensity as well as a time factor. Assessing risk requires identifying the values that are impacted and the degree to which they are. 

  
Healthcare is by nature adomain of high stakes. It is also adomain in which several factors are interrelated. A hospital procurement policymay impact its ability to cope with emergency situations. Its management of appointments may impact the availability of beds or operation rooms. It is difficult to assign a priori a risk level to such or such application. 


Another important issue is the potential relevance of the correct development of trustworthy AI tools toreduce burn-out of healthcare professionals (correlating with adverse events) and medical malpractice (typically correlating with defensive medicine). 
AIcouldreally becomeanoperationaltooltomanagethesecriticalsituations,by optimizingtherole ofhumanagentsandtheirliability, thankstodecision-support systems andrigorous, standardised process datatracking.Thisissuecouldbevery relevant in the short term for the healthcare domain, much more thanmore radically innovative solutions for diagnoses and therapies. 
Additionally, education and training programs for health professionals must, in their various curricula, include a substantive discussion of AI, its promise and potential perils, and management of its risks. 
Education for healthcare personnel,whether administration orphysician/nurses, isaclear priority, too. Because the public struggles to understand the basics of how AI systems operate (Coeckelbergh, 2019), it should be assumed the samefor healthcare personnel. In order for healthcare personneltounderstand the risks inherent inuse and implementation of AI systems in healthcare, theymustbe knowledgeable about these systems work -how algorithms arrive at specific decisions, how machine learning and big datacanmake predictive healthcare diagnoses. Educational literacy about AI for healthcare personnel could follow existing gamification models of digital educational training (Mesko et al., 2015). 
Making sure that it is patients who benefit the most from the surge of AI health technology remains a key challenge. This will need new approaches in medical education to improve digital literacy, understanding of mathematical modelling, basis of decision theory, and continuous learning about AI of physicians. This should include awareness of biases in data, and how these undermine any claims about how AI models are able to produce objective, neutral results. 
Accountability for AI systems in healthcare is also of great concern. The ability to audit healthcare systems is necessary, and could be built on the aforementioned models utilized in Helsinki and Amsterdam (Johnson, 2020), envisioning physician and patient ability to peer into the  black box  for auditabilitypurposes. Further underscoring the need for education, audibility of AI systems is only possible when stakeholders involved inauditingthesevery systems comprehend theunderlying technologies -where physicians do not fully comprehend all processes involved in AI (Diprose et al., 2020). Ethical-by-design healthcare AI needs to better integrate patients  views andvalues to understand better different realities and kinds of knowledge, including the subjective aspectofillness.Patients  wishesare acrucialmeasure for anticipatinghow AI technologies contribute to their health and wellbeing. Engineers and physicians need to work with patients to establish whether the use of AI is an empowered choice. This will needresearch programs tounderstand the patient s own relationship with AI. A first stepwillbeeducationandallow abetterpatient s literacy. Asecondstepwillbe patient s engagement by feeding the dialogue with AI designers. The final step should be patient s empowerment to gain a better health through self-customized AI use. 
Diversity for AIsystemsinhealthcare mustbealsofocused by industryand stakeholders from different perspectives, mainly such as: 
1)Diversity of the team of the designers and developers of the AI-based solutions. Asaminimumrequirement the team should be balanced in gender, soto elucidate the wide spectrum of the needs, behavioural, communication and emotional styles which canbevery different for male and female healthcare professionals, patients, relatives and all other human agents involved in the application scenarios 
2)AI for diversity, i.e. the implementation of AI-based solutions which should cover theabove general specifications by fully exploiting AIalsofor simulatingthe widevariety of diversity-open application scenarios, e.g. different facial morphologies andcolors,different voice languages,expressiveness andaccents,different motor behaviours, different culturalandsocialcontexts.Using AI-based simulationscan simplify, accelerate and better calibrate the development process sotodeliver highly inclusive solutions. 
3)Diversity of the sample population in data. The validation of the proposed AI-based solutions must be carried out by recruitingadiverse setof individuals soasto rigorously assess the actual performance when interacting with different\diverse human agents. 
Thecurrent pandemic has increased examination of issues related to accessibility of healthcare. The implementation of AI systems in healthcare also brings forth issues ofaccess,where we are now faced with scenarios of affordability. For example, if a private company markets analgorithm for detecting early onset of stroke, how canwe ensure allgovernments have equalaccessandthetechnologyisnotoutofreach economically? Rapid developments in AI will indeed increase issues of affordability and access.AIcanbecomeakey driver for thedevelopment ofaffordable healthcare solutions, optimizing cost-effectiveness, quality and dependability of novel solutions. 
Privacy andsecurityofdataare important,aswell. Machine learningrequires massive amounts of data (Hedlund et al., 2020), and healthcare data possess ahigher level of sensitivity and risk versus non-healthcare data. The security of these data and protection of patient privacy is imperative -however, we should not assume that GDPR is flexible enough to keep pace with seemingly quick developments in AI. 
Perhaps ourmostimportantrecommendation isthathealthcare organizations need to design anexplicit process for assessing AIrisk and for mitigating that risk for each application of AI they are consideringorusing. That process mustinclude the professionals, the organizational leadership, the patients and the public. 
References 
Anderson T. The theory and practice of online learning. Edmonton, Canada: AU Press, 2008. 
Antun V, Renna F, Poon C, et al. On instabilities of deep learning in image reconstruction and the potential costs of AI. Proceedings of the National Academy of Sciences 2020: 201907377. DOI: 10.1073/pnas.1907377117. 
Arago, Fran ois. Speech before the French Parliament (Chambre des D put s), 14 June 1836, cited in Le Monde, 18 June 1954. 
Avizienis Algirdas , Laprie Jean-Claude, Randell Brian , and Carl Landwehr. Basic Concepts and Taxonomy of Dependable and Secure Computing. IEEE Transactions On Dependable And Secure Computing, Vol. 1, No. 1, January-march 2004. 
Barredo Arrieta Alejandro, Natalia D az-Rodr guez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, Francisco Herrera. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI, Information Fusion, Volume 58, 2020, Pages 82-115, ISSN 1566-2535, https://doi.org/10.1016/j.inffus.2019.12.012. 
Bulla 2020. Bulla, Chetan & Parushetti, Chinmay & Teli, Akshata & Aski, Samiksha & Koppad, Sachin. (2020). A Review of AI Based Medical Assistant Chatbot. 2. 1-14. 10.5281/zenodo.3902215 
Char DS, Shah NH and Magnus D. Implementing Machine Learning in Health Care   Addressing Ethical Challenges. New England Journal of Medicine 2018; 378: 981-983. DOI: 10.1056/ NEJMp1714229. 
Castelvecchi, Davide. Can we open the black box of AI? Nature 538, 20 23 (06 October 2016) doi:10.1038/538020a 
Coeckelbergh M. Artificial Intelligence, Responsibility Attribution, and a Relational Justification of Explainability. Science and Engineering Ethics 2019. DOI: 10.1007/s11948-019-00146-8. 
Diprose William K. , Nicholas Buist, Ning Hua, Quentin Thurier, George Shand, Reece Robinson, Physician understanding, explainability, and trust in a hypothetical machine learning risk calculator, Journal of the American Medical Informatics Association, Volume 27, Issue 4, April 2020, Pages 592 600, https://doi.org/10.1093/jamia/ocz229 
Eaneff, S, Obermeyer, Z and Butte, AJ The case for algorithmic stewardship for artificial intelligence and machine learning technologies. JAMA. Published online September 14, 2020. DOI:10.1001/ jama.2020.9371. 
European Coordination Committee of the Radiological, Electromedical and Healthcare IT Industry (COCIR). Artificial Intelligence in EU Medical Device Legislation. September 2020. https://www.cocir.org/fileadmin/Position_Papers_2020/COCIR_Analysis_on_AI_in_medical_Device_ Legislation_-_Sept._2020_-_Final_2.pdf 
European Commission 2019a. Independent High-Level Expert Group on Artificial Intelligence. Ethics Guidelines for Trustworthy AI. Brussels, 8.04.2019. https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai 
European Commission 2019b. Independent High-Level Expert Group on Artificial Intelligence. Policy and Investments Recommendations. Brussels. 26.06.2019. https://ec.europa.eu/digital-single-market/en/news/policy-and-investment recommendationstrustworthy- artificial-intelligence 
European Commission 2020a. White Paper On Artificial Intelligence - A European approach to excellence and trust. Brussels, 19.2.2020. COM(2020) 65 final. 
https://ec.europa.eu/info/sites/info/files/commission-white-paper-artificial-intelligencefeb2020_en.pdf 
European Commission. 2020b. Independent High-Level Expert Group on Artificial Intelligence. The Assessment List for Trustworthy Artificial Intelligence (ALTAI) for self-assessment. Brussels, 17.9.2020. https://ec.europa.eu/digital-single-market/en/news/assessment-list-trustworthy artificialintelligence-altai-self-assessment 
Featherstone K, Atkinson P, Bharadwaj A, et al. Risky Relations: Family, Kinship and the New Genetics. New York, NY: Taylor & Francis, 2020. 
Floridi Luciano, Cowls Josh, Beltrametti Monica, Chatila Raja, Chazerand Patrice, Dignum Virginia, Luetge Christoph, Madelin Robert, Pagallo Ugo, Rossi Francesca, Schafer Burkhard Valcke Peggy and Vayena Effy. AI4People - An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations. Minds and Machines, December 2018. Available at SSRN: https://ssrn.com/abstract=3284141 
Grother Patrick, Mei Ngan, Kayee Hanaoka. Face Recognition Vendor Test (FRVT). Part 3: Demographic Effects. NISTIR 8280. Information Access Division Information Technology Laboratory, National Institute of Standards and Technology (NIST), 2019. Available at https://doi.org/10.6028/NIST.IR.8280 
Grandjean Nathalie, Matthieu Corn lis and Claire Lobet-Maris. Sociological and Ethical Issues in Facial Recognition Systems: Exploring the Possibilities for Improved Critical Assessments of Technologies? 10th IEEE International Symposium on Multimedia, 2008. 
Greene Gretchen. The Ethics of AI and Emotional Intelligence. Partnership on AI, 2019. https://www.partnershiponai.org/the-ethics-of-ai-and-emotional-intelligence/ 
Hedlund, J., Eklund, A. & Lundstr m, C. Key insights in the AIDA community policy on sharing of clinical imaging data for research in Sweden. Sci Data 7, 331 (2020). https://doi.org/10.1038/s41597-020-00674-0 
Higher Health Authority, Paris, France, 2020 (in French). https://www.hassante. fr/upload/docs/application/pdf/2016-01/guide_fabricant_2016_01_11_cnedimts_vd.pdf. 
Johnson K. Amsterdam and Helsinki launch algorithm registries to bring transparency to public deployments of AI. Venture Beat, 2020. 
Young-Bum Kim, Dongchan Kim, Anjishnu Kumar, Ruhi Sarikaya. Efficient Large-Scale Domain Classification with Personalized Attention. 56th Annual Meeting of the Association for Computational Linguistics, Melbourne, Australia, July 2018. 
King, D..  DeepMind s health team joins Google Health  [Blog]. https://deepmind.com/blog/announcements/deepmind-health-joins-google-health (2019, September). 
Lotman EM and Viigimaa M. Digital Health in Cardiology: The Estonian Perspective. Cardiology 2020; 145: 21-26. DOI: 10.1159/000504564. 
Maurer D. Face Recognition Technology: DOJ and FBI Need to Take Additional Actions to Ensure Privacy and Accuracy. In: Justice HSa, (ed.). 2017. 
McCall B. COVID-19 and artificial intelligence: protecting health-care workers and curbing the spread, The Lancet Digital Health, vol. 2 e166-167, April 2020. 
Merler M, Ratha N, Feris RS, et al. Diversity in faces. arXiv preprint arXiv:190110436 2019. 
Mesko, B., Gyorffy, Z., & Koll r, J. (2015). Digital Literacy in the Medical Curriculum: A Course With Social Media Tools and Gamification. JMIR Medical Education, 1(2), e6. doi:10.2196/ mededu.4411 
Milani L, Leitsalu L and Metspalu A. An epidemiological perspective of personalized medicine: the Estonian experience. J Intern Med 2015; 277: 188-200. DOI: 10.1111/joim.12320. 
Morley, J and Floridi, L. Policymakers must start asking difficult questions on the ethics of AI in healthcare. September 9, 2020. Available at: https://www.publictechnology.net/articles/opinion/policymakers-must-start-asking-difficultquestions ethics-ai-healthcare. Accessed September 14, 2020. 
Morley, J, Machado, CCV, Burr, C, et al. The ethics of AI in Healthcare: A mapping review. Social Science & Medicine 2020; 260: 113172. DOI:10.1016/j.socscimed.2020.113172. 
Pagallo Ugo, Aurucci Paola, Casanovas Pompeu, Chatila, Raja, Chazerand Patrice, Dignum Virginia, Luetge Christoph, Madelin Robert, Schafer Burkhard and Valcke, Peggy.. AI4People - On Good AI Governance: 14 Priority Actions, a S.M.A.R.T. Model of Governance, and a Regulatory Toolbox (November 6, 2019). A I 4 P E O P L E, 2019, Available at SSRN: https://ssrn.com/abstract=3486508 
Panch T, Mattie H and Celi LA. The  inconvenient truth  about AI in healthcare. npj Digital Medicine 2019; 2: 77. DOI: 10.1038/s41746-019-0155-4. 
Philip Pierre, Lucile Dupuy, Marc Auriacombe, Fushia Serre, Etienne de Sevin, Alain Sauteraud, Jean-Arthur Micoulaud-Franchi. Trust and acceptance of a virtual psychiatric interview between embodied conversational agents and outpatients. npj Digit. Med.. 2020-01-07. 3(1) 
Raji ID, Smart A, White RN, et al. Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing. Fat* '20 2020: 33 44. DOI: 10.1145/3351095.3372873. 
Shortliffe, E. H. (1999). The evolution of electronic medical records. Academic Medicine, 74(4), 414-419. Retrieved from https://pdfs.semanticscholar.org/d46d/1c4f5871d3c915d220c7e0350c2c7054583b.pdf [PDF] 
Ting Daniel S. W., Yong Liu, Philippe Burlina, Xinxing Xu, Neil M. Bressler and Tien Y. Wong. AI for medical imaging goes deep. Nature Medicine   May 2018 DOI: 10.1038/s41591-018-0029-3 
Vezyridis, P., & Timmons, S. (2017). Understanding the care.data conundrum: New information flows for economic growth. Big Data & Society, 4(1), 2053951716688490. doi:10.1177/2053951716688490 
Vinuesa R, Azizpour H, Leite I, et al. The role of artificial intelligence in achieving the Sustainable Development Goals. Nature Communications 2020; 11: 233. DOI: 10.1038/s41467-019-14108-y. 
Weizenbaum, J. ELIZA: A Computer Program for the study of Natural Language Communication between Man and Machine, CACM, Vol. 9, Issue 1, January 1966 
World Health Organization (WHO), Road Traffic Injuries (February 2020), https://www.who.int/ newsroom/fact-sheets/detail/road-trafficinjuries#:~:text=Approximately%201.35%20million%20 people%20die,road%20traffic%20crashes%20by%202020 
Appendix: 7 Key Requirements for Trustworthy AI (European Commission 2019a). 
Human agency and oversight 
AI systems should empower human beings, allowing them to make informed decisions and fostering their fundamental rights. At the same time, proper oversight mechanisms need to be ensured, which canbe achieved through human-in-the-loop, human-on-the-loop, and human-incommand approaches 
Technical Robustness and safety 
AI systems need to be resilient and secure. They need to be safe, ensuring a fall back plan in case something goes wrong, as well as being accurate, reliable and reproducible. That is the only way to ensure that also unintentional harm can be minimized and prevented. 
Privacy and data governance 
Besides ensuring full respect for privacy and data protection, adequate data governance mechanisms must also be ensured, taking into account the quality and integrity of the data, and ensuring legitimized access to data. 
Transparency 
The data, system and AI business models should be transparent. Traceability mechanisms can help achieving this. Moreover, AI systems and their decisions should be explained in a manner adapted to the stakeholder concerned. Humans need to be aware that they are interacting with an AI system, and must be informed of the system s capabilities and limitations. 
Diversity, non-discrimination and fairness 
Unfair bias must be avoided, as it could have multiple negative implications, from the marginalization of vulnerable groups, to the exacerbation of prejudice and discrimination. Fostering diversity, AI systems should be accessible to all, regardless of any disability, and involve relevant stakeholders throughout their entire life circle. 
Societal and environmental well-being 
AI systems should benefit all human beings, including future generations. It must hence be ensured that they are sustainable and environmentally friendly. Moreover, they should take into account the environment, including other living beings, and their social and societal impact should be carefully considered. 
Accountability 
Mechanisms should be put in place to ensure responsibility and accountability for AI systems and their outcomes. Auditability, which enables the assessment of algorithms, data and design processes plays a key role therein, especially in critical applications. Moreover, adequate and accessible redress should be ensured. 
5 
INSURANCE 
Authors 
Frank McGroarty 
Chairman Insurance Committee, AI4People; Professor of Computational Finance and Investment Analytics; Director of Centre for Digital Finance at Southampton Business School, UK 
Gianvito Lanzolla2 
Professor and Dean at Cass Business School - City, University of London, UK 
Nir Vulkan 
Chairman Banking & Finance Committee, AI4People; Associate Professor of Business Economics at Sa d Business School, University of Oxford, UK 
Paul Jorion 
Associate Professor of Ethics, Universit  Catholique de Lille, France 
Patrice Chazerand 
Director at DIGITALEUROPE 
Rui Manuel Melo Da Silva Ferreira Chief Data Governance Officer, Zurich Insurance Group (ZIG) 
Tilman Hengevoss 
Head Public Affairs EMEA Region at Zurich Insurance Group (ZIG) 
XeniaZiouvelou 
Innovation Officer and Research Scientist, Institute of Informatics and Telecommunications, National National Centre for Scientific Research Demokritos, & Member of the Scientific Committee on Data Policy and Artificial Intelligence, 
National Council for Research and Innovation (NCRI), Greece 
Executive summary 
The impact of AI technology onthe Insurance sectorseemslikely tobe in two overarching areas. The first concernsthe fair and transparent treatment of consumers. The second relates to risk assessment in underwriting and pricing aspects, in particular when hyper-personalisation of risk assessment is considered. 
Algorithmsare already being used for every part of the insurance business chain from customer acquisition/retention to risk pricing to claims assessment to investment management.There are obvious customertreatment aspectsinthefirstthree areas. Investment management is an important issue for insurers. It is obvious that the capital raisedfrom insurancepremia willneedtobemanagedinsomeway andisusually managed by aninvestment function within the insurance company who invest in the capital markets. What is perhaps less obvious is that most private sector pensions which are not managed directly by large corporate employers or linked to professional bodies, 
(i.e.the pension funds of SMEs) are also usually managed by insurance companies. However, investment management overlaps with the focus of the Banking and Finance committeeandcanbemore comprehensively addressed there. Thatsaid, given the growing prevalence of AI in investment decisions and in financial markets generally, we feel we shouldnotethatfinancial markets AIposesasystemic sustainability risk to Insurance via assetmarket crashes and volatility. Insurers canalso be entangled with broader financial market risk in another way. Inrecent decades,someinsurers (e.g. AIG) have engaged in reinsurance of structured products and in underwriting financial derivative risk(e.g. credit defaultswaps). AIalgorithmsare involved intherisk modellingthatdetermines thepricingoftheseandinthemarket behaviour that determineswhether theircontingentclaimistriggered. Again,theseissuesfitmore naturally with the remit of the Banking and Finance committee but we acknowledge thesystemic risktoinsurers ofthisAIexposure.  Convergence  triggered by AI adoptionacross banking and insurance might have adual effect. On the onehand, it might increase communication and cooperation. On the other hand, it might magnify further the systemic risk described above by enabling  groupthink . In the extreme, this groupthink effect can result in different insurers being overly aligned in the assumptions and models they usetoassessand price risk. This could arise through the common use of risk modelling software ordata services from singleorfew providers, orit could arise from insurance professionals coalescing onasingle best method ormodel. Model homogeneitycouldconcentrateandmagnifywhat would otherwisebesmallrisk pricing errors or isolated mistreatments of customers. 
 InsurTech  is fast emerging as a distinct area within the wider "FinTech revolution". The latter is causing disruption and are-think of financial services generally and is strongly assisted by AI technologies. In insurance, AI enables innovations such as micro insurance of instantaneous events, bespoke insurance fitting more closely anentity s trueriskprofile, betterbundlingandoffsettingofanindividual s risks,increased operational efficiency leading to cost reductions and greater demographic penetration of insurance services, benefiting the individual andsociety in general. It also opens the door to  crowd-insurance  which could compete with traditional insurers inasimilar way to how crowd-funding offers businessesanalternative way of sourcing capital to banks and capital markets. However, these benefits, which are greatly helped by the use of AI, are also accompanied by the risks introduced in the preceding paragraph. 
This committee would also like to highlight the opportunities and risks related to Big-tech  the likes of Google, Apple, Ant Financial -entering into the insurance space. TheunparalleledAIandCloudcapabilitiesofsuchcompaniescombinedwiththeir relatively poorer knowledge of the insurance legacy domains might interact tocreate insurancepolicieswhose (un-)intended consequencesare difficulttopredict. Asymmetric access to customer behavioral insights between Big Tech and the insurance industrymay alsotilttheplaying field.Finally, asymmetriesindatastorageand processing capacity may skew the rules when it comesto enforcing safety and security toalevel thatmatchescustomers expectationsandtrust.Assuch,thiscommittee suggests that regulators, financial stability authorities and broader stakeholders should systematically assessthe actual and potential scenarios triggered by Big-tech s entry in the insurance industry, and act accordingly to maximize societal welfare and ensure fair competition. 
Anadditional consideration for European insurers andpolicymakers isthe prospect of foreign insurers and tech companies arriving in the EU tocompete, with ready-to-go InsurTech products that they perfected in their home market, which may not have been subjected to same kinds of consumer protection regulation and legislation that we adhere to in Europe.  
For thepurpose of the current report, this committee will restrict itself to anEU, insurance-sector perspective. We believe that the risks of AI to the insurance sector can and should be anticipated, managed and mitigated. If employed carefully and judiciously, AIcanavoid perpetuating discrimination and candeliver good, sustainable social and economicoutcomes.Broader andbetterriskmanagementviainsuranceshouldaid financial inclusion and enhance financial wellbeing. Conversely, poor implementation ofAIcouldleadindividuals tobemore cautious intheirbusinessandeconomic activities, to discrimination and ultimately to social exclusion. 
Wehighlightthefollowing5specificissues: 
  Technical knowledge gap: 
There isarisk that regulators, senior management and other key decision makers in theInsurance sector, may lack of technical skills to fully understand the effect of technological innovation in their industry. In the past AI used to resemble humanthinking andreasoning. Butcurrent AIsystems suchasmulti-layered neuralnetworks, are muchmore opaque. To understand what these systems do and whether they pose a regulatory challenge, for example in terms of discrimination ofordeviating from agreed risk levels, regulators need to better understand the inner-workings of the AI system in question. A number of new techniques, which we review inthis report, are now available tohelp them in doing exactly that. These require some quantitative skills and training. It is important that regulators andotherkey usersare trainedinthesemethodsandhave amore thanbasic understanding of AI systems and how theywork. The sameapplies to insurance executives who need to understand and be accountable for theworkings of their systems. Furthermore, insurers, regulators and policymakers should adjust their recruitment strategies to attract people with these skills. 
From aconsumerperspective, abasic level of AIliteracy will need tobe acquired by the general population. Without this there can beno consumertrust inAItools,which couldendup damaging trustintheinsurancesectoritself. Consumersneedsufficientknowledge andconfidence tobeabletochallenge decisions andto understand whether they are fair. And the industry needs to have practices that are sufficiently transparent and explainable tobe able toprovide consumerswith understandable answers about how decisions have beenreached. 
  Social inclusion and fairness: 
Insurance has been framed as a socialization of risk: individual risk is pooled, and insurers compensate their losses with high-risk individuals by profiting from low-risk ones. Ifwe disclosetoomuch of the risk factors, it will be harder and harder to maintain this status quo. As they accumulate more and more data, it will become easier for insurers to quantify individuals true risk and to identify those customers most likely to make a claim. This poses a potential dilemma for society. If insurers were to solely prioritise profit over all other considerations, they would eitheravoid thosehigh-risk/high-claim-potential customers,leaving many uninsured, oroffer them high prices. A disproportionate number of those likely to findthemselves in thatsituation will be socially disadvantaged and poor, who may not be able to afford the insurance prices offered to them, again resulting in exclusion. The social consequences are obvious but this may also lead to knock-oneconomic consequences through uninsured people curtailing and suppressing their own economic activity. On the other hand, conscious behaviour of individuals takingonmore risk andassuming that the society will fund the consequences 
(i.e.free rider problem) goes against the fairness principle. Insurers and regulators need to consider ways of balancing these risks. 
  Ethical-by-design innovation: 
Engineers building AI systems may lack awareness of the ethical and other wider-implications ofthesystem theyare building. They are under increasing pressure todeliver technologies that make fasterand more accuratepredictions. Theymay be less aware of the wider impact these systems may have onindividual users(e.g. those who are turned downby the algorithm) and society overall. This failure to comprehend the broader societal impact is a trend across the tech industry. It is particularly disturbing insocial media where competitionfor users  attention have created more addictive systems which clearly harmsociety. Indeed, the AI4People 2020 report on AI inMedia and Technology Sector  found the following:  Thesectorismore directly user-facing compared toothersectorssuchasthe energy orautomotive sector, with,for example, socialmediaplatforms being essential for socialinteractionand informationsharing. Thismeansthatpeople might peg the confidence they should have in digital technology to how much they cantrustsocial media platforms. However, atthesametime, the MTS offers the opportunity toprovide AI with apromising front office,by realistically framing doom stories and possibly showcasing the advantages of cutting-edge technology. In thatway, people canlearnthe ropes of empowerment inanenvironment which is more familiar, or less forbidding than anything related to health or mobility.  
We propose regular training for engineeringandothertechnicalstaff working in insurance to highlight, among other things, the EU seven requirements for Ethical AI. Also, whilewe note the insurance industry s clear awareness of the potential for emerging technology (personalized apps and IoT devices) to  nudge  consumerstowards better/healthier behaviours, we advocate theuseof sandbox frameworks toexplore unintendedbehaviours oftheseAIsystems priorto deployment. We advise promoting the principal of ethical-by-design. However, it may bemore pragmatictoemphasise themore measurableconceptsof environmentally-friendly-by-design andsocially-beneficial-by-design. Thiswill require anintelligent approach toincentivisationandtoregulation, including self-regulation and co-regulation, in order to achieve trustworthy AI in insurance, withthedesired socialandgreen outcomes,withoutstiflinginnovation. In addition, we advocate that the industry adopts common information standards, to make insurers  systems and platforms interoperable, and data easily transferable1. 
1 EIRA (see TOGAF (2017)) could be a good starting point for developing insurance industry system interoperability. 
This is in the interest of customers, including business and employer customers, who are best served by being able to compare competitor insurance products with existingandeasestransfers ofcustomerdatabetween oldandnewinsurance providers. It alsofacilitates dataaggregation for regulators toassesssystemic risks. 
  Humans always in charge: 
AsAIsystems getbetter, theriskincreases ofinsurers becomingmore reliant andlesslikely tochallengethedecisionmadeby thesesystems. Where possible, AI systems should not replace human decision making but rather should be viewed as an additional tool in the decision-making process. Radiology provides avery good example: Here AIsystems are significantly betterthanhumansin identifying abnormal patterns in images. There were some about 6 or 7 years ago who predicted that these AI systems would replace radiologists all together. That clearly didnot happen,andthesesystems are now beingusedwidely by the professionals tomake betterdecisions.We believe thisisthemodelinsurers should follow, rather than replacing humans with machines. There isaprecedent for this in finance, in algorithmic trading which is aform of AI that has been in existence since the late 1980s. Here algorithmic models which take advantage of momentum(or trends) in financial markets have been widely used for decades. We know thatthesestrategiestendtodowell innormalmarket conditions. However insomeextreme market conditions (but notall) they cancrash badly often losing in matter of days much of the gains made in the preceding months2 and often interact with other autonomous algorithms in unanticipated ways. This phenomenon is known as momentum crashes . Hedge funds which follow these modelsblindly suffer. However, fundswhich useoversight riskmanagement strategieswhere final decisions onleverage levels are made by individuals, have done better and suffered less. AI systems used by insurers are more likely tobe customer-facing than market facing. However, we think the principle of human oversight of AI decision-making systems remains the same. 
To keep complacency in check, it may be worth noting here someof the proposals of the  Governance Innovation  report released by Japan METI3 at the OECD (in January 2019). For instance, this report proposes goal-based regulation  [which] will enable more flexible policymaking based on the [nature of the] risk. In other words, by requiring business to implement reasonable measures based on thenature/size ofthebusinessandtheimpactonconsumers,policiescanbe implemented flexibly, i.e. businesses with a strong influence on society are required to protect the interests of law more prudently, while businesses with smaller risks 
2 See https://www.aqr.com/Insights/Research/Journal-Article/Momentum-Crashes for more details 3 https://www.meti.go.jp/press/2020/07/20200713001/20200713001-2.pdfp. 42 
are allowed to take a less onerous compliance approach. In this age where changes are rapidly taking place and the need for revolutionary innovation is increasing, setting the rules in advance could frequently create adverse effects, therefore it would be better to have goal-based regulations as a basic policy.  
  Data risks: 
Sophisticated AI systems are  data hungry  in that performance and accuracy dependonbeing able to runonvery large data sets. In fact, the amount of data theaverage AIsystem requires seemstoincrease exponentially. Asmore interactions between individuals and insurers move online,newkinds of data is being collected on everything from mouse moves, to how long user stayed at what page,tohow many clicksshemade,toambientconversations, tolocation information, etc..This opens up issues of privacy and data ownership, and the laws andregulations in place to safeguard these. A further complication is posed by thefact that these data setsare increasingly being stored oncloud services. Recent incidents suggest this is areal and present dangerwhich all varieties of financial institution must act to guard against. 
Having outlined thesechallenges,we notethatinsuranceisalready ahighly regulated sector. Many of issues covered in the media such on so called "AI bias," where algorithms learn to discriminate against certain groups (e.g. gender, race), are already addressed by existingregulation. Furthermore, additionallayers ofregulation will increase the barriers to entry for start-ups and smaller firms. This canresult in unfair advantages for deep-pocketed, transnational insurance companies and tech giants who are now entering this space, which could be detrimental to competition and to consumer choice. 
We hope that the steps highlighted in this document can address and mitigate the risks posed by AI in Insurance, avoiding too much additional regulation and the costs and inefficiencies that are associated with it. Complex and convoluted regulation can result in the opposite outcome to the oneintended, especially in relation to consumer protection. This is because complex regulation canonly be navigated by those who can afford the lawyers tonavigate it. This cancauseconcentration of market share ina small number of providers, reducing consumer choice and increasing systemic risk. 
We note thatthe financial crisis of 2007-08 began with mortgages, arguably the mostregulated part of finance atthe time. At that time, the creation of sophisticated products, involving banks and insurers, on the back of mortgages resulted in the excess riskwhich culminated in the crisis. We hope to avoid arepeat of this in coming years where AI technologies will undoubtedly enable the creation of additional, unnecessary risks in the financial system. The steps we highlighted, especially around the technical training for regulators, we hope can serve as a big step in the right direction. 
In the rest of this document we consider the impact of AI technologies onthe Insurancesectorinlightof the EU7key requirements for trustworthy AI.For the purpose of the Insurance sector, we have mergedsomeof them together astheyare addressed by the same recommendations made in the report. These are: 
Requirements 1, 4 & 7: Human agency, oversight, transparency and accountability 
Requirement 2: Technical robustness & safety 
Requirement 3: Privacy & data governance 
Requirement 5: Diversity, non-discrimination and fairness 
Requirement 6: Societal-environmental well-being 
For eachwe provide usecases,areview of the research and literature in both academiaandby insurers andregulators before makingourrecommendations. We suggestanovel system of labelling toindicatetrustworthiness of AI applications. We evaluate the various regulatory framework options and we introduce aclassification framework for evaluatingthe necessity for regulation based onboth the risk and the consequential dependence of AI decision outputs. 
1. Introduction 
The aim of this paper is to create a Good AI Global Framework for the Insurance sectorwiththeconcrete objectives of:(a) considering theimpactthatthe7Key Requirements will have within the sector, and (b) establishing concrete and practical steps(concrete actionablerecommendations andhigh-level guidelinesfor distinct stakeholder segments) that the insurance sector must take to be compliant with respect to the 7 Key Requirements for a Trustworthy AI. 
The insurance sector is defined here in the broadest way possible, meaning a wide range of activities covering pensions and asset management as substantial key activities that insurance companies do, alongside more obvious insurance functions. 
Furthermore, this paper will notdiscuss insurance ethics in general, but rather the ethics of the AI-driveninsurance. It aims to distinguish between high-level guidelines and concrete actionable recommendations for firms. 
2. The impact of ai for the insurance sector Insurance sector overview and key stakeholder segments (value-chain) 
We canthink oftheinsurance stakeholder modelashaving theinsurer atthe centre. Around the centre sitanumber of other entities: premium payers, insurance beneficiaries, capital markets, insurance company staff, insurance company shareholders, regulators and government. Premium payers pay intotheinsurer inorder toget insurancecover. Beneficiaries receive apayout inthe event ofaclaim. The premium payer may alsobethebeneficiary, butthisisnotnecessarily thecase.Thecapital markets are another stakeholder because this is where the insurers invest the premiums in order to maximise their value so as to be able to pay future claims and also to sustain the insurance companies and reward their own shareholders. Insurance regulators are responsible for therobust and fair operation of the insurance industry. Government hasaninterest intheinsuranceindustrythatgoes beyond policymaking.Inmany instances, government may end up having to pay if aninsurance industry solution fails to deliver. 
Data processing and data-led statistical analysis has always been the core of the business of insurance undertakings. Digitisation enables the emergence of new types of data,which combined with increasingly powerful IT tools, algorithms and information systems, facilitate more predictive, descriptive andprescriptive analytical processes. AI tools and algorithms candiscover andtesthypotheses, make decisions automatically andaccesspreviously inaccessible datasets, such asunstructured data from pictures, videos and audios. 
Table 1: The Insurance Value Chain: Past and Future perspectives 

(Extending McKinsey 2020) 
Big Data andArtificial Intelligence has significantly impacted the insurance value-chain (Table 1). All value-segments have been transformed by Big Data, AI, and IoT (Internet of Things) among others. According to a recent study by EIOPA (2019), Big Data Analytics (BDA) tools are commonly used by insurance firms in the motorand health insurance segments. Such tools are generally focused onaspecific part of the insurancevalue chain and very few firms make useof them across all their processes. BDA toolsare mainly usedfor pricingandunderwriting (35%), claimshandling includingfraudprevention (30%)andsalesanddistribution(24%).Furthermore, although thevast majority of companies usein-house developed solutions, there are many others that use off-the-shelf  solutions from third party service providers and open source (i.e. freely available) tools. 
Unique aspects of the insurance sector (social value of insurance, cultural sensitivity of insurance, etc.) 
There are socialvalues of insurance that go beyond the immediate mutualisation of individuals' risks. Firstly, more economic activity will take place because insurance isavailable than when it is not. This is because at least someindividuals will moderate their activityin the face of uninsured risk. Secondly, insurance enables somegroups to subsidise others in socially beneficial ways -e.g. less vulnerable tomore vulnerable, richertopoorer, younger toolder, healthy toill, and among ethnic groups, different genders,etc.There are economicredistribution aspects tothis and there is also the effect ofagreater proportion ofsociety being insured which amountstoapositive externality. 
Opportunities and Risks of AI for the Insurance Sector 
Deployed responsibly andincompetitive markets, AIcouldprovide numerous benefits across all areas of the Insurance value chain (see Table 2). 
Table 2: AI in Insurance: Potential use cases across the Insurance value chain 

(Extending EIOPA BDA Thematic Review) 
WhataretheethicalimplicationsofinsurersusingAI? 
The issue of AI in insurance is the issue of processing big data.  Large societal benefits arise with the potential toreduce risks and increase insurability through the useofvast quantities of data. New approaches to encourage prudent behaviour canbe envisaged through big data, thus new technologiesallow the role of insurance to evolve from pure risk protection towards risk prediction and prevention. However, the useof bigdataininsuranceraises complex issuesandtrade-offswithrespect tocustomer privacy, individualisationofproducts andcompetition. Assessing thesetrade-offs requires complex value judgements, and the way they are addressed leads to different scenarios for the future development of the sector. , Anna Maria D Hulster, Secretary General, The Geneva Association (2020). 
Ethical AI Dilemmas in the Insurance Sector 
The insurance sector faces anumber of dilemmas. A central challenge is that business optimizing actions do not necessarily align with what is deemed ethically permissible (the trade-off between business optimizing and legitimacy is oneof the key trade-offs that AIposesto organizations). For example, although insurers have legitimate reasons touseAI in the way theydo, many of these behaviours and/or actionsare misaligned with what society in general or some in society findacceptable. In many cases, customers may have diverse views astowhat they seeasavaluable and ethical useof AI and data processing (CDEI, 2019). According to an online survey that was conducted by Deloitte in 2015 (2,955 insurance customerswithmotor, home and health policies), 40% of policyholders would allow insurers totracktheirbehaviour andassociateddata (telematics-based insurance), for amore accuratehealthcare insurancepremium, as opposedto49%who disagreed. The percentages for home insurance were 38% and 45%respectively whereas for motorinsurance 48% were willingtoshare their data versus 38%thatdisagreed. Theadoptionoftelematics-basedinsuranceandthe willingnesstoshare data with insurers differs between young and older customersas well. Digitally-savvy young customers (63% of customers between 25-34)were found to be more willing to share their data with home insurers in return for a more accurate premium, than older customers (38%). 
Another challenge is that many are convinced that AI will raise concerns for policyholders. This is reinforced by cases where insurers appeared to be creating innovative insurance serviceswithoutfully consideringtheethicalconsequences.For example, in2016 Admiral was criticised for attempting to use Facebook posts to analyse the personalities of car owners in order to set the price of their car insurance (Guardian, 2016). Finally, the insurance firm was forced to abandon its plans, within afew hours,asthe scheme breached Facebook s privacy rules regarding its users.The scheme was launched later with reduced functionality  anduserscouldlogintotheservicewithFacebook, withoutany intrusive attempttoanalyse theirposts.Althoughthisreversal was welcomed, it raises concernsregarding potential future attempts of other companies trying to use personal data in a similar way. Such encounters create challenges for users thatmay find it difficult toavoid opting in, asthe financial disadvantage of such an action may be quite significant that users are left without any other option than allowing companies to access their data. 
3. Use-case analysis regarding the 7 key requirements for trustworthy AI 
Inconsideringuse-casesfor the7Key Requirements for thecontextofthe Insuranceindustry, we found considerablerepetition inissueswe explore for the various requirements. Thiswas particularly thecasefor requirements 1,4and7, which, for brevity, we address collectively. 
3.1 Req. 1, 4 & 7: Human Agency & Oversight; Transparency; Accountability 
REQUIREMENT 1: Human agency and oversight 
Including fundamental rights, human agency and human oversight 
REQUIREMENT 4: Transparency 
Including traceability, explainability and communication 
REQUIREMENT 7: Accountability 
Including auditability, minimisation and reporting of negative impact, trade-offs and redress 
Therelevant fundamental rights that companies within the insurance sector should consider are:   
  Dignity and non-discrimination 
o All human beings are born free and equal in dignity and rights. ; Article 2:  Everyone is entitled to all the rights ( ) without distinction of any kind, such as race, colour, sex, language, religion, political or other opinion, national or social origin, property, birth or other status.  (Universal Declaration of Human Rights   Article 1) 
oCentre for DataEthics andInnovation onAIandPersonnel Insurance  
 Insurers are prohibited by law from discriminating against customers on the basis of their sex, ethnicity and several other characteristics.  
oEuropean Commission   Theuseof AI canaffect the values onwhich the EU 
is founded and lead to breaches of fundamental rights, including the rights to ( ) human dignity, nondiscrimination based on sex, racial or ethnic origin, religion or belief, disability, age or sexual orientation ( ).  
oEU Gender Directive: The EU Gender Directive prohibits gender-based differentiation of insurance premiums4. 
4 Council Directive implementing the principle of equal treatment between menandwomen in the accessto and supply of goods and services 2004/113/EC, 13/12/2004. 
  Related todignity, we believe itistherighttoanexplanation andsocial accountability: human beings should be entitled toanexplanation when anAI system makes a decision about whether they can get insurance and at what price. Ethical Guidelines for Trustworthy AI:  Whenever an AI system has a significant impactonpeople s lives, it should be possible to demand asuitable explanation of the AI system s decision-making process. (page 18). Transparency by design isafrequentfeature in AI systems developed by startups that operate in Europe 
-e.g., Lemonade and Akur8. 
Furthermore, in order to ensure thatconsumersare not unfairly discriminated, public authorities should independently monitor the data used by insurers and the output of AI systems. 
  Personal autonomy o AI could one day be used by insurers to advise customers on how to avoid risks, for example with chatbots suggesting healthy eating and exercise regimes. Some believe behaviour change initiatives like these would impinge on the autonomy of policyholders, while others say they could result in meaningful improvements in people s living standards.  (Centre for Data Ethics and Innovation on AI and Personnel Insurance). There are several examples of European start-ups which are using gamification asaproven waytonudge people into behaving. Nevertheless, the jury is still out there on assessing the real net effect of gamification on behaviors. 
  Personal and societal implications o AI is set to make risk assessments more accurate by revealing new predictors of risk. This could result in some groups paying more for their insurance premiums, possibly to thepoint where products become unaffordable. Yet the opposite may also be true, with AI-powered risk assessments showing individuals to be less risky than theyfirstappear (e.g. some youngdrivers). If large partsof societybecome uneconomical to insure, a wider debate will be needed on whether the state should intervene, and if so, on what terms. (Centre for Data Ethics and Innovation on AI and Personnel Insurance) 
  Personal data and privacy protection o While insurers may be tempted to store this data, perhaps in the expectation they will be able to put it to use in future, doing so raises several ethical concerns. One is the threat to people s privacy5, especially where datasets are at risk of a cyber breach. Another relates to fair compensation [for selling this data]. ( Centre for Data Ethics and Innovation on AI and Personnel Insurance) 
5 In the pharma industry, some companies are experimenting with a new  flipped  approach: patients own their data and give permissiontocompanies tousethem,onacase-by-case basis.Furthermore, patientscanwithdraw permissionanytime. Permission is often also linked to ad-hoc financial compensation.  
o Requirements aimed at ensuring that privacy and personal data are adequately protected during the use of AI-enabled products and services. For issues falling within 
theirrespective scope, the General Data Protection Regulation and the Law Enforcement Directive regulate these matters.  (European Commission) 
Insurers have the right toprivacy and data protection. Insurance AI toolsand systems must respect EUlaws onprivacy anddataprotection (TheCharterof Fundamental RightsoftheEuropean Union,articles7(private andfamily life), 8 (personal data) and 52, (2012/C 326/02), General Data Protection Regulation and the Law Enforcement Directive). Insurers privatelife should be protected in order to ensure citizen freedom, while at the sametimeconsumersshould be able to to access insurance policies that rely on non-intrusive data processing practices. 
  Oversight and accountability 
Inorder toensure legal compliance, algorithmic tools in the insurance context should go through, systematic assessment(s). These assessments, including impact and riskassessment,shouldbeconductedbefore thelaunch,butalsoduringthewhole product s lifecycle. This monitoring and assessmentprocess should be conducted not only by the insurance firms but also by dedicated public authorities. 
Accountability 
In2018,astudyby theFinancial Conduct Authority (FCA)6 was conducted examining the complexity of General Insurance (RI) pricing models and practices inthe UK retail home insurance market. 
Thestudyidentified anumber ofissuesrelating tofirms  pricing practices that present the most potential for significant harm and poor outcomes for consumers. Such asthat firms didnot have appropriate and effective strategies, governance, control and oversight of their pricing practices and activities, and asaconsequence theywere unable to reliably assess and evidence whether they are treating their customers fairly. 
Theyfound thatseveral insurancecompanies were unable toname adedicated member of staff who had ownership over their pricing strategy, which could include how AI-led risk assessments influence premiums. As the study notes:  appropriate governance and controls mechanisms are need to be underpinned by clear lines of accountability and responsibility sothatfirms appropriately consider and evaluate how pricing decisions impactconsumer outcomes. This includes considering whether the pricing structure and approach meetsourrequirements onfirms tohave dueregard totheinterests ofits customers and to treat them fairly.  (FCA, 2018). 
6 FCA (2018),  Pricing practices in the retail general insurance sector: Household Insurance , Thematic ReviewTR18/4, October 2018. 
  Human agency: 
Relevant governance mechanismsfor achieving humanoversight are: human-in-the-loop (HITL), human-on-the-loop (HOTL), orhuman-in command (HIC) approach. In the insurance sectorthis implies training staff in the appropriate technical skills and hiring technically competent into areas of the business that were not previously considered astechnical support. It also means trainingpeople inbusinessethicsandhaving auditprocedures thatcheck periodically whether measures are actually effective. The healthcare industry is experimenting with a new paradigm:  human-in-command , which is considered tobeamustwhenever sensitive data is atstake. Ultimately, in these models a human gatekeeper validates, or rejects, the AI-based recommendations. 
Arguably, humanagency inthe insurance sector is informed by humanagency insectors that cannot operate without being insured. In transportation, for instance, insurance premiums have long beenshaped by stable conditions whereby apilot would be the ultimate decision-maker inflying aplane, oradriver should be in control of his vehicle at all times lest he might qualify for fines. Nowadays, software may overrule the captain, as illustrated inB737MAX crashes, or Autopilot is behind Tesla s wheel. As aresult, humanagency in the insurance sectorwill of necessity reflect radically lower humanagency inindustries they work with, transportation, health,energy, etc. A possible consequence is for insurance companies to adjust to this trend passively. Another optionmight have them contribute to the works of the relevant sectors by evaluating early-on the insurability of particular paths considered: insurable scenarios would be deemed fine, those ineligible to insurance would carry ared flag orbe abandoned. For instance, will companies which are unable to get a quote by alegacy insurance company decide todesigntheir owninsurance  e.g., Tesla for its autonomous car business? 
Thismay betheideabehindtheco-creation modelpresented in METI s  Governance Innovation  report (cited earlier in the report7):  Mechanisms and systems for achieving the objective of law differ by company. Further, the shape of cyberspace is invisible, and information needed for evaluation of cyberspace is asymmetrically accumulated in businesses, therefore it is difficult for a third party to determinefrom the outside how businessesare actually ensuring compliance. Consequently, in order to encourage businesses to achieve sustainable innovation and protect social values set by law at the sametime,aco-creation model would be appropriate in which businesses that design/manage cyber-physical architectures todesign, disclose and explain their approaches and concepts of compliance to theregulatory authoritiesormarkets, timely receive feedback from them,and continue to evolve.  
7 Japan METI  at the OECD (2019). 
Furthermore, AI adoption across sectorshas other systemic consequences onhumanagency. AIadoptionispositively associatedtoconvergence in management attention in firms across industries including industries which are usually considered asunrelated suchasinsuranceandpharmaceutical (Lei, LanzollaandTsanakas, 2020). While convergence inmanagementattention might make communication and cooperation easier across firms and industries, here we want to note some potential pitfalls. First, it might increase  groupthink  and by implication, systemic risk. Second, we notehere thatconvergence might reduce the space for strategic differentiation for insurers thus potentially opening market opportunities from other market spaces, including big tech. 
Multi-stakeholder consensus on what constitutes a responsible use of AI and data 
Theindustry should engage with the publicinorder toreach aconsensusonwhat constitutesaresponsible useof AI and data. For example, in relation to personal data processing from social media; a joint decision should be made regarding the conditions underwhich itisacceptabletoprocess datafrom socialmediaplatforms ortouse algorithms to predict people s willingness to pay higher premiums. In UK the Financial ConductAuthority (FCA) hasdeveloped aFramework for assessingwhen price discrimination may be acausefor concern, that is when it canpotentially disadvantage someconsumerssignificantly, inparticularthemostvulnerableandleastresilient consumers. The proposed 6 question Framework aims at addressing the issue of unfair pricing practices in retail markets. In addition, it should also be examined whether tighter controls need to be in place on theuseofpersonalcharacteristicsinpricing.Iffor example, AIenablesinsurers to identify high riskcharacteristicsthatwere notpossiblebefore (e.g., chronic health conditions, etc.), thenthiscouldinturnresult tomore people facingunaffordable premiums. As such,  society should have asay in any decision onwhere toredraw the boundary between acceptable and unacceptable forms of discrimination  (CDEI, 2019). However, in the courseof this search for consensus, the industry should proceed with pro-active interventions (rather than re-active) toaddress obvious harms. Industry-driven measures should be setinorder toensure that AI and data are used for the publicandcommongood.Such measures may range from more accessibleprivacy noticesanddatadiscriminationaudits,toindustry-wide registers for third party suppliers of data (CDEI, 2019). Furthermore, all these measures should be supported by a sector-wide commitment to transparency. 
There needs to be  .... a sector-wide commitment to transparency. Without greater disclosure, insurers will struggle to build trust with customers and regulators will lack theinformation todesignproportionate regulatory responses.   .[Inaddition], greater transparency would helptodistinguish genuine threats from thosethatare overstated and would support the development of interventions that are proportionate to the risk inquestion, thereby allowing responsible innovation to flourish. (Centre for Data Ethics and Innovation on AI and Personnel Insurance) 
Explainability / transparency for insurance: 
Acentral issue is being able toexplain toconsumershow decisions about their individualpremiums orclaimshave beenreached. IntheUK,theInformation Commissioner s Office(ICO) istheregulator withresponsibility for upholding information rights in thepublic interest. They advise onhow firmscancomply with information and privacy legislation such asGDPR and the Data Protection Act. The ICO recently had an initiative called Project ExplAIn, which aimed to help organisations explain decision outcomes of AI systems8.They also developed an Auditing Framework for AI, which helps regulators to evaluate algorithmic fairness. 
Other stakeholders also have aninterest in transparency. These tend to be more macro-level andsystemic innature. Thesecanincludetheentire industry being dependentonthesameset of assets performing adequately in order to be able to pay future liabilities. Also, if all insurance companies usethesamemodelling approach, or even buy in the samethird-party models, there isarisk that mistakes in these models could be compounded and magnified. 
Need for Transparency in Insurance 
Theneed for transparency exhibitsvariations dependingonthetypeand significance of the AI insurance applications, and the extent to which there has been a change in the decision-making logic and/or the data sources utilised (Figure 1). 
8 It isworth noting that we have seensignificant progress inrecent years in explaining complex AI models, such asSHAP (SHapley Additive exPlanations) values and Local Interpretable Model-Agnostic Explanations (LIME), aswell asOptimalClassification Trees to improve accuracy and while addressing interpretability issues. 8 See https://search.coe.int/cm/Pages/result_details.aspx?ObjectId=09000016807c65bf 


Figure 1:Classification of AI Insurance applications, their significance and the associated need for transparency (extending GA, 2020) 
For someusecasesrelying oncomplex non-linearmodels (e.g. deep learning) or on the use of new external data sources explainability might be more difficult and affect customeracceptance.To offsetthesechallenges,somecompaniesare experimenting withalgorithms which reverse engineerthedecisionsfrom theoriginal algorithms, without limiting the complexity of the original algorithms themselves. These explanatory algorithmsmay not reveal all aspects of the decision process, yet they might shed light onsome  patterns  underpinning the decision. Nevertheless, although this is possible, it is still unclear whetherthese  reverse engineering algorithms will be sufficient for mitigating the explainability challenges. 
Existing legal requirements for transparency to(retail) customersshouldbe interpreted for theuseof AI in abalanced approach. The customer s righttoask for human(re-)validation ofthecorrectness ofthealgorithm-generatedresult and respective reasoning for thecorrectness oftheresponse shouldaddress customer acceptance issues. 
Providing an explanation is particularly important when a decision has a significant impactontheaffected individual.Therefore, thedegree towhich explainability is needed is highly dependent onthe context and the severity of the consequences when an output is erroneous or otherwise inaccurate (European Commission 2019). 
Interpretability ofalgorithmicoutcomesisimportanttoprovide meaningful explanation toaffected individuals;andisalsoindispensablefor assessingthe performance of AI systems and for their continuous improvement, and thus for sound data science. 
The implementation of interpretable models is encouraged, in particular if their outcomeshave asignificant impact oncustomers.When used for risk selection and pricing, trust in AI systems can be fostered by using data sources that are related to the insured riskinaway which isintuitively understandable tocustomers.Where itis difficult to explain algorithmic outcomes in an understandable way to consumers, there are othermeasures thatcanfoster customertrust, such astraceability, auditability and transparent communicationaboutasystem s capabilities(European Commission, 2019). 
Measures of transparency: 
  
Keeping records anddata:records oftrainingdatasets(selection process, characteristicsofdate); documentationontheprogramming andtraining methodologies; disclosure upon request in particular for inspection by competent authorities (European Commission) 

  
Internal guidelines and policies: Insurers canmitigate the risk by developing and implementing internal guidelines and policies to ensure a consistent approach to the transparency and explainability of algorithmic outcomes. Guidelines should helptoclarify how the benefits and risks of using AI should be assessed ona case-by-case basis. Actuaries, risk managers, data scientists and data protection officers, product development and digitization managers, innovation executives (justtomentionafew key stakeholders) shouldclosely cooperateinthe development and implementation of such guidelines and policies. 

  
Communications,Traceability andExplainability: Humans shouldbeinformed when theyare interactingwithAIsystems. Ifsomeonewants tochallengea decisionoraprice, the factors and sequence leading to that decision should be fully traceable and explainable in a way that a lay person could understand. 


Un-intended consequences at an implemention level 
Transparency has also several potentially negative implications such as sucking up thesystem  andtransactional behaviours  there is significant evidence of this in the management literature. Or, a blind faith in datagovernance might hinder the ability of an insurers of modelling what the industry call  emerging risks .  
Ethical Algorithm Audits 
Theneed for anethical audit framework for algorithmic development and deployment has been proposed by anumber of stakeholders from theresearch and the industrial communities. Such ethical  algorithm audits  (O Neil, 2016; Larsson etal. 2019) could function asAI auditing mechanisms, ensuring that the moral and ethical issuessurrounding the useof AI are being addressed, while identifying potential biases orflaws, depending onthe type of industry and globally accepted auditing procedures and standards. 
Larssonetal. (2019) suggest arole for professional algorithm auditors,whose jobwould be tointerrogate algorithms in order toensure they comply with pre-set standards. One example would be an autonomous vehicle algorithm auditor, who could provide simulated traffic scenarios to ensure that the vehicle did not disproportionately increase the risk to pedestrians or cyclists relative to passengers. 
The accountability and responsibility of AI systems and their outcomesisakey aspect of trustworthy AI.Hence, necessary mechanisms should be introduced in order toensure them in the insurance sector among others. As the backbone of privacy and data protection regulation ataglobal level, the principle of accountability is reflected in Europe s General Data Protection Regulation (EU), 2016/679 (GDPR), requiring data controllers to implement appropriate technical and organisational measures. Core elementsoforganisational accountability includeproportionate procedures, top-level commitment, risk assessment, due diligence, communication and training, monitoring andreview (TheBribery Act 2010,UKMinistryofJustice). Aswell assenior managementcommitmenttoimplementing aculture ofintegrity, transparency and compliance; the adoption of internal codes of conduct; implementation of whistleblowing systems; mapping risks and implementing internalcontrols and audits; and the training of staff oncorruption risks (French Anti-corruption Agency (AFA) Guidelines 2016). 
Furthermore, AI-based insurance might shape the subjectivity of policy holders in un-intended ways (e.g., Kellogg, Valentine andChristin,2020). For instance, policy holders might change their behaviors because of the constant perception of surveillance (when the policy is perceived topolice behaviors totrigger compliance with policy s expectations); constantvisibilityandlossofprivacy (when thepolicyenables mechanismsofsocialcomparison); andperception ofunfairness(when thepolicy holdercannot make senseofthedecisionstaken by theAIpowering thepolicy). Independently, and/or jointly, these mechanisms might trigger the emergence of new  gaming  behaviors and/or new social stratification (Lanzolla, Quy. Pesce, 2020). 
Auditability in the insurance sector context: 
Current regulation ensures appropriate governance ofoperationsandITrisks andadherence tointernalgovernance standards. Inthecontextofinsurance,robust internal governance is essential with clarity on roles, responsibilities and accountabilities including operational risk management obligations that insurers adhere to in their own best interest. 
In addition, AI audits should be put in place for the insurance sector. Such audits shouldbebothataninternal andexternallevel by independent authoritiesplacing emphasis onAI systems that affect (directly and indirectly) fundamental rights. At an internal level, strengthened quality assurance processes for model oversight and controls should be considered soasto demonstrate that internal governance systems are robust anduncompromised enough to address challengesresulting from theuse of non-linear AI models. More specifically, before the launch of insurance-specific algorithm-based systems, thorough assessment shouldbeconducted, including adetailedimpactand riskassessment.The performance of these systems mustbe continuously monitored and assessed,throughout the product s lifecycle, by the insurance company and relevant external independent authorities. Insurance companies must put in place the necessary internal governance mechanisms and measures toensure legal compliance of the AI-systems as well as necessary AI audits.  
Trade-offs in the insurance sector context: 
A number of trade-offs can be identified in the insurance context, as it can be seen in the Table 1. 
Issue	  Benefit  Cost  
Discrimination Risk profiling  Accuracy of Risk Classification  Equal Treatment  
Intrusiveness Privacy  Risk Reduction  Intrusiveness  
Secondary use Accuracy  Value of Data  Contextual Integrity  
IndividualisationPricing  Individual Pricing  Affordability  
Solidarity (Social Insurance)  Individualization  Equity  
Risk Pooling (Private Insurance) Individualization  Value of Insurance  

(Adopted from Geneva Association) 
Redress in the insurance sector context: 
Redress processes intheinsurancesectorare critical.Insurers shoulddevelop such processes that will essentially actasamechanismtocompensate for any harm caused by AI(AI4People, 2018). Such processes will foster public trust in AI. Reliable redress mechanisms for harms inflicted, costs incurred, orother grievances caused by the technology should be established, including a clear and comprehensive allocation of accountabilitytohumansand/or insurancecompanies. Aligned withtheAI4People, 2018report, the aerospace industry, could be seenasanexample, asit has aproven system ofhandlingunwanted consequencesthoroughly andseriously. Thedesign process of effective remedies should involve prompt and adequate reimbursement and redress for any harm suffered by the development, deployment oruseof AI systems, andmay include measures under civil, administrative, or, where appropriate, criminal law (Council of Europe, 2019). Therefore, algorithm-systems and tools in insurance shouldgothrough athorough assessmentbefore theirlaunch, including adetailed impact and risk assessment.In addition, the performance of these system should be regularly monitored and assessed throughout the lifecycle of the insurance product/ service/system, by theinsurancecompaniesaswell asrelevant, specialised public authorities. 
Establishing accountability 
 A 2018 study by the FCA found that several insurance companies were unable tonameadedicatedmember of staff who had ownership over their pricing strategy (which could include how AI-led risk assessmentsinfluence premiums). Centre for Data Ethics and Innovation on AI and Personnel Insurance (CDEI) 
RECOMMENDATIONS - Requirements 1, 4 & 7: 
1.AI systems should empower individuals to make informed decisions.At thesametime,proper oversight mechanismsneedtobeensured inthe insurancesector. Such mechanisms canbe achieved through human-in-the loop, human-on-the-loop, and human-in-command approaches. 
2.Internal guidelines and policies:Insurance companies need tobuildtrust through transparency. At an internal level they need to develop and implement guidelines and policies that will ensure transparency and safeguard the interests ofemployees, stakeholders, customersandthewidercommunity. Thisway insurers canmitigatetherisk,ensuringaconsistentapproach tothe transparency andexplainability ofalgorithmicoutcomes.Guidelinesshould help to clarify how the benefits and risks of using AI should be assessed ona case-by-case basis. Actuaries, risk managers, data scientists and data protection officers should closely cooperate in the development and implementation of such guidelines and policies. 
3.Ethical AI audits for algorithmic development and deployment in the insurance sector could ensure moral and ethical compliance while at the same time identifypotential biases or flaws. Such algorithm audits could be external (top-down) aswell asinternal(bottom-up,company-driven) auditing mechanisms that are performed at regular intervals. 
4.Accountability via governance frameworks (HLEGrecommendation). Robust internal governance withclarityonroles, responsibilities and accountabilities is covered by operational risk management obligations that insurers adhere to in their own best interest. Strengthened quality assurance for model oversight and controls should be considered todemonstrate that internal governance systems are robust and uncompromised enough to address challenges resulting from use of non-linear AI models. 
5.An AI system should be designed to be fully answerable and auditable. Inorder for this to be achieved, it is important to: (a) establish a continuous chain of responsibility  for all roles involved in thedesign and implementation lifecycle of the project, and to (b) implement a continuous  activity monitoring  soasto allow for oversight and review throughout the entire project lifecycle. 
6.Insurersshoulddevelopappropriateandeffectiveredressprocessesthat will involve prompt and adequate reimbursement and redress for any harm suffered by thedevelopment, deployment oruseofAIsystems, indirect alignmentwiththerelevant laws. Thorough assessments(impact,risk assessments,humanrightsassessments,etc.)ofAIinsurancesystems and toolsshouldbeconducted underdistincttimeperiods (pre-launch, post-launch) and throughout the lifecycle of the insurance product/service/system, by the insurance companies as well as relevant, specialised public authorities. 
Theinsurance sectorcannotbeconsidered inisolationofotherindustriesit enables. Changes in those industries will inevitably inform insurers  ability to go by the above recommendations. Whether in human agency, explainability oraccountability, insurers depend heavily on the modus operandi of other sectors. The other way around, theymay be used asa canary in the coalmine , able to tell what is insurable from what is not, which is more or less a way to tell what is socially correct and what is not, what is worth putting on the market and what is not. 
This is how  Governance Innovation  describes possible benefits accruing from a cross-sectoral approach:  Businessesthatappeartobedistinctly different from one another whenviewed from a physical space perspective may share many cross-sectorial commonalitieswhen seenfrom acyberspace operations perspective. For example, we believe thatcross-sectorial goals aswell asguidelines and standards canbe defined for areas such as data management (privacy, cyber-security), ID infrastructure construction, AI quality assessmentand continuous data collection method. Because it is currently difficulttoensure the predictability and explainability of the performance outputs of complex systems centered onsoftware such asAI that perform machine learning  usingconventional rule-based software authentication methods, in order to implement these systems inoursocietieswe may establish more flexibleevaluation criteria and constructtechnologicalfoundations for evaluations thatallow for AIandsimilar technologies to be introduced into the market to acertain level and allow performance improvements to be made through updates. Furthermore, asdescribed in 5.3.2, we can deependiscussiononthebehavior ofcomplex systems includingAIregarding incentivizedregimes for investigations performed by businesses,criminaldeferred prosecution agreement regimes, and the establishment of insurance mechanisms that cover potential dangers to society.  (page 67 of the Japan METIs report 2019). 
3.2 Req. 2: Technical robustness & Safety 
REQUIREMENT 2: Technical robustness and safety -Including resilience to attack and security, fall back plan and general safety, accuracy, reliability and reproducibility 
Potential forms of security-related attacks in the insurance sector include the following: 
   Potential forms of security related attacks: 
o While insurers may be tempted to store this data, perhaps in the expectation they will be able to put it to use in future, doing so raises several ethical concerns. One is the threat to people s privacy, especially where datasets are at risk of a cyber breach. Another relates to fair compensation [for selling this data]. ( Centre for Data Ethics and Innovation on AI and Personnel Insurance) 
Examples of Cybersecurity Incidents in the Insurance Sector 
The insurance sector has experienced avariety of cybersecurity incidents in past few years, includingwell-publicised databreaches atseveral U.S. healthinsurers. According totheInternationalAssociationofInsuranceSupervisors (IASIS) report (2016), such cybersecurity incidents include the following: 
Data breach: In 2015 inthe United States, Anthem Blue Cross Blue Shield and Premera Blue Cross (the largest health benefits company by membership in the US) discovered data breaches that began a year earlier. The breached data included member and applicant names, dates of birth, Social Security numbers, bank account information, claims data, member identification numbers, and someclinical data. The sophisticated cyberattack that lasted for nearly one year before it was discovered, potentially exposed the personal informationof up to 91 million policyholders  asmany asaquarter of the people in the United States9.The insurers had to react swiftly to mitigate reputational damage and to minimise litigation costs. They paid $260m for security improvements and remediation, and $117m in June 2017 to settle lawsuits from customers potentially affected10.Inaddition, during 2019Premera achieved HITRUST-certification that demonstrates the company s abilitytoidentify risks, protect data, detect attacks, and respond to security incidents. 
Distributed denial-of-service (DDoS) attack: DDoS attack threats by agroup of cyber extortionists known asthe  DD4BC  that had been targeting arange of firms including financial institutions in Europe, Australia, Canada, and the United States in order toextort money from them. DD4BC demanded ransoms, from specific targets, tobe paid in crypto-currency (bitcoin), in order tocancel the launch DDoS attacks. Two German insurance groups experienced this type of attack in mid-2015, receiving threats of a DDoS-attack on company web servers. The insurers refused, as they assessed the extortionists would have caused only minor damage in those instances, but these incidents could have been far more serious if theattacks had concentrated onmore critical systems. 
Cyber-attack: In the Netherlands, aninsurer was recently subject to the so-called  CEO hack,  aspecific form of phishing cyber-attack. Pretending tobe the CEO of a majorandwell-known commercial customeroftheinsurer, thecriminalstriedto persuadeemployees oftheinsurer totransfer moneyintoacertainaccount.The criminals had apparently researched certain operational details of the insurer. 
  Potential forms of attack.There are several categorization of threats, and they allmostly relate todatastored onservers and information exchanged in communications; unintended human actions facilitating cyber-attacks, outages ormalfunctioning, physical attack, orattacks to sensors orAI systems orupdate procedures. 
1.For ICT ingeneral,theEUCybersecurity Act establishesacertification framework, and allows the creation of tailored andrisk-based EU certification schemes . 
https://ec.europa.eu/digital-single-market/en/eu-cybersecurity-certification-framework 

9 Anthem, Inc., Statement Regarding Cyber Attack Against Anthem, via https://www.anthem.com/health-insurance/about-us/pressreleasedetails/WI/2015/1813/statement-regarding-cyber-attack-against-anthem; Premera Blue Cross, Premera Targeted by Cyberattack (17 March 2015),viahttps://www.premera.com/wa/visitor/about-the-cyberattack/?WT.z_redirect=www. premera.com/cyberattack/10 https://healthitsecurity.com/news/premera-reaches-proposed-74m-settlement-over-2014-breach-of-11m 
  Applicability of relevant Insurance Core Principles to Cybersecurity (ICPs):despitethefactthatICPs donotspecifically address cyberriskandcyber resilience theyprovide ageneralbasisfor managing cyber risksandinformation exchange. Relevant ICPs are: ICPs 7, 8, 9, 19, 21 (cyber risks), ICPs 3, 25, 2611. 
Protecting Customer Data through cybersecurity 
Data is creating a new horizon of opportunities for insurance companies. In order torealise its full potential, insurance firms mustrespect and protect theircustomers and their data. Therefore, insurance as a business of trust, needs to take all the necessary measures to protect data and demonstrate to customers the data privacy commitments, going beyond mere compliance with privacy and data breach laws. 
Focusing upon the protection of customerdatathrough cyber security, Zurich Insurance Group has established the Cyber Fusion Center asaninternal cyber threat intelligence group. The Center aims toprotect customers' data by combining cyber threat intelligence, response, forensics and vulnerability management teams. 
As cyber-attacks increase both in frequency as well as in severity, companies can make themselves more resilient by strengthening theircyber-risk strategies and practices atalllevels oftheinstitution andwithrespect torelevant third-party arrangements.However, thesystemic nature ofcontinuously evolving cyber threats, necessitatestheneed for collective action.Asfor insurers, cybersecurityincidentscan harm not only the ability to conduct business, compromise the protection of commercial andpersonaldata,andundermineconfidenceintheinsurancesector(causing reputational damage affecting the confidence of consumers, policy holders, investors, rating agencies and business partners)12.One example towards such collective action is the Centre of Cybersecurity13 of the World Economic Forum, which brings together experts from around the globe aiming to address systemic cybersecurity challenges and improve digitaltrusttosafeguard innovation, protecting institutions, businesses and individuals. 
RECOMMENDATIONS - Requirement 2: 
Insurers needto be alert to cybersecurity risks, to put relevant safeguards in place and should utilise the certification framework set out in the EU Cybersecurity Act. 
11ICP 7(Corporate Governance), ICP 8(RiskManagement andInternalControls), ICP 9(Supervisory Review and Reporting), ICP 19 (Conduct of Business), ICP 21 (Countering Fraud in Insurance), ICP 3 (Information Exchange and Confidentiality Requirements), ICP 25 (Supervisory Cooperation and Coordination), ICP 26 (Cross-border Cooperation and Coordination onCrisis Management) (International Association of Insurance Supervisors- IASIS, Issues paperoncyberrisk to the insurance sector (Aug. 2016)).12InternationalAssociationofInsuranceSupervisors (IASIS), Issuespaperoncyberrisktotheinsurancesector(Aug. 2016).13Centre ofCybersecurity oftheWorld EconomicForum https://www.weforum.org/platforms/shaping-the-future-of 
cybersecurity-and-digital-trust 

3.3 Req. 3: Privacy & Data Governance 
REQUIREMENT 3: Privacy and data governance 
Including respect for privacy, quality and integrity of data, and access to data 
Dataanddataprocessing constituteacore aspectoftheinsurancesector. Digitisation has enabled, insurance companies to enrich their traditional datasets (e.g. demographic data, exposure orbehavioral data, etc.), with newtypes of data such as Internet of Things (IoT) data, online data, bank/credit data, etc., and to perform highly advanced analytical processing. Across the insurance value chain, the data used (EIOPA, 2019) can: (a) include personal data (e.g. medical record) and non-personal data (e.g. hazard data), (b) they canbestructured (e.g. IoT data, survey) orunstructured (e.g. pictures oremails) and(c) canbeobtainedfrom internalsources (e.g. consumer provided data directly tothe firm) and from external sources (e.g. public databases, private data vendors, etc.) (see Table 2). 
Table 2: Traditional and New data sources in the insurance sector 

(Sources: EIOPA, 2019; Geneva Association, 2020) 
Theuseofdemographic data(e.g. age,gender, occupation, etc.) isawidely adopted practice among insurance firms. Demographic data are directly provided by theconsumersatthe pre-contractual stage and are subsequently complemented with external data sources such as national statistics offices or third-party data vendors (e.g. geo-spatialsocioeconomicdemographic classifications,suchaspurchasing power, family types, population density etc.). (EIOPA, 2019). Basedonarecent studyby EIOPA (2019),healthandmotorinsurancefirmscollaboratewiththird-party data vendors in order to acquire anonymised third-party data (at a postal code and granular level) that are used in technical models for pricing and underwriting purposes. Insurance firmsalsocollectgender data, which theyare notallowed tousefor pricingand underwriting purposes following the2011rulingoftheEuropean CourtofJustice against the pricing differentiation onthe grounds of sex(European Court of Justice, 2011)14. 
Digitisationenablesnewtypesofdata(e.g. IoT data). According toEIOPA (2019), the usage of such data is expected toincrease in the nextthree years in the insurance sector, primarily in the motor insurance sector (e.g. data collected via black boxes installed in cars or mobile phone apps including speeding data, miles driven, road types, harsh braking, etc.) as well as in other sectors such as the health insurance sector 
(e.g. 
datacollectedviawearables ormobilephone apps). Thisinformation often complemented with external data sources has and will facilitate pricing and underwriting 

(e.g. 
Pay-As-You-Drive (PAYD) orPay- How-You-Drive (PHYD) policies (Usage-Based Insurance)). 


Emerging sources of data in insurance and ethical concerns 
New sources of data are entering the insurance sector, such as data from wearables and telematic devices. In this emerging context, according to the Centre for Data Ethics andInnovation (CDEI), insurers may findthemselves collectingmore information about their customersthan is necessary todeliver theircore services.However, should insurers store this data with the expectation to put it to use in the future? 
This question raises several ethical concerns. One is the threat is people s privacy, especially where datasetsare atriskofacyberbreach. Anotherconcernsfair compensation,iffor example customerdataissubsequently soldontothird parties. Thisraisesaspectsofadequatereimbursement ofcustomersfor thevalue thatthey have created for the insurance company. 
Furthermore, the collection of additional data may also increase the probability of algorithmic bias during the training phase. In order to reduce these harms, the industry could prepare necessary standards, for example data storage standards, that could be jointly developed with relevant Insurance Associations or Standards Institutes, 
14 Case C-236/09, The European Court of Justice, 1 March 2011, http:// curia.europa.eu/ 
at a national and regionallevel, aiming to discourage insurance companies from storing data that is notdirectly linked totheir mission. Such standards, according toCDEI, couldinclude anexpectation for insurers toregularly review their datasets soasto determinewhether they are materialtothe ir core business practices, and if notto eliminate them from company records. 
  Existing EU rules on privacy and data: 
oControllers must, atthe time when personal data is obtained, provide the data subjectswith information necessary to ensure transparent processing about the existence of automated decision-making. The GDPR recognizes theoverarching principle offairtreatment ofconsumersinrelation to personal data processing, and enables consumers to demand the removal of their data from the insurer s databases ( right tobe forgotten , Art.17(2) GDPR). Inadditiontothestrong obligationsfor organisations,GDPR introduces several accountability tools to data protection rights such asthe principles of data protection by design and by default and newprovisions for company certification andindustry-wide codeofconductschemes (General DataProtectionRegulation (GDPR) andDataProtection Law Enforcement Directive). 
Privacy and data protection in the insurance sector: 
Thelevel ofgovernance ofdataandmodelaccuracy depends onthebusiness purpose (e.g. for medicalrelated services the scrutiny level must be much higher than for marketing purposes). In general, AI insurance applications in customer engagement may have lower significance than applications that are usedtodetermine payouts to policyholders. Applications in underwriting/pricing may exhibit the highest levels of significance, in particularif they could lead to theexclusion of customers and impact their privacy. For example, behavior change schemes could pose a threat to the autonomy of policyholders, with insurers gaining the power toinfluence their lives in multiple ways, from where they live tohow they drive tohow often they exercise. While one could argue that signing up to behavior change schemes is a choice, it would be relatively simple for insurers toturnavoluntary schemeintoamandatoryone.Refusing to participate in such schemes may also signal toinsurers thatcustomersare high-risk, since low-risk individuals would have every incentive to be monitored. 
In addition, other usesof AI that automatespecific tasks but do notchange the logicofdecision-making inany way (such asextractingrelevant information from documents inan automated way via OCR and NLP) are likely to exhibit low significance of impact. In contrast, any use of AI that changes the logic of decision-making (i.e. that applies a new model to existing data) may exhibit higher significance. The highest level of significance of impact may be in uses that change the logic of decision-making based on new data sources (see Figure 1). 
Ethical Review Boards 
Acknowledging the importance of the ethical useof algorithms and data, some insurers are increasing their level of disclosure in relation to these aspects. For example, the Zurich Insurance Group, launched a Data Commitment, and  UK Aviva insurance company, launched aCustomer Data Charter15 that aim to set out what happens to the information they collect oncustomers, and the corporate rules around how it is shared  aiming to protect personal data. Other insurance firms have established expert panels that focus upon their corporate policy. 
For example, AXA s Data Privacy Advisory Panel16 isaninternal team that is composedby dataandprivacy experts,businessandmarketing executives, legal practitioners and corporate responsibility officers as well as academics and independent advisors.Thepanelmeetsbi-annually inorder toconsiderAXA s useofdataand algorithms, as well as the firm s actions and commitments (e.g. data privacy commitments, including aspects that relate to the international exchange of data, among others). 
This company-centric, bottom-up approach aims to foster ethical practices and the ethicaluseof data and algorithmic systems. This approach entails the development of ethical advisory boards orpanels, integrating awide range of company-members from different departments. Such panels tend to have anadvisory, review role including for example the review of insurance applications, the review of potential violations of the internal ethical code, aswell asthereview of newsoftware that contains AI elements andthedevelopment ofactualmeasures, amongothers.Theycanbelinked toan industry-wide Code of Conduct scheme, which is encouraged under Article 40 of the GDPR as well as company certification schemes. 
RECOMMENDATIONS - Requirement 3: 
1. 
Compliance with data protection standards.Insurers shouldensure compliance with the General Data Protection Act and the Data Protection Act, among other legislation, placing emphasis on personal data storage and being clear on the legal basis for processing data (CDEI, 2019). 

2.
 Disciplined internal oversight ofthedatasciencepipelinesandstrong businessdataownership andaccountabilitycansupporttheaccuracyofdataand mitigate privacy and data risks. 


15Zurich s datacommitment,https://www.zurich.com/media/magazine/2019/earning-trust-to-unlock-the-power-of-data; Aviva s customer data charter, https://www.aviva.co.uk/services/about-our-business/about-us/customer-data-charter/16 AXA s data privacy advisory panel was set up during July 2015, https://www.axa.com/en/about-us/data-privacy 
3. The formation of ethical advisory/review boards canbeseenasabottom-up approach to establishing ethical AI practices and Trustworthy AI. The members of these boards should have sufficient diversity of expertise such as technical, ethical, legal andexpertsonthesubject matter(domain ofapplication). Insurancefirmsaligned with the HLEG recommendations canadapt their corporate responsibility charter, Key Performance Indicators (KPIs), codes of conduct orinternal policy documents to add the striving towards Trustworthy AI. 

4.Thefair, ethical and transparent use of data is a priority.Data analytics governance frameworks canbe utilized in order to create trust and ground the useof data in common ethical principles (EIOPA, 2020). 
3.4 Req. 5: Diversity, non-discrimination and fairness 
REQUIREMENT 5: Diversity, non-discrimination and fairness 
Including the avoidance of unfair bias, accessibility and universal design, and stakeholder participation 
Fairness inakey andhighly important principle for responsible AI, thatis associatedwithdistinctvalues -suchasfreedom, dignity, autonomy, privacy, non discrimination, accessibility, equality and diversity, among others. These values often needtobeinterpreted incontext,includingtheculturalcontext,which makes it impossible to provide auniversal standard of fairness (GA, 2020). At ageneral level, distinct dimensions of fairness can be distinguished such as procedural and substantive (European Commission 2019): 
  
fair process  consumersfair treatment across thewhole process; where acore aspect of fairtreatment is the ability of customers to challenge and seek effective redress against decisions affecting them (European Commission 2019). In the context of the insurance sector there are market conduct requirements to ensure fair treatment of customers irrespective of the technology used (such as Insurance Core Principle 19(ICP19) oftheInternational AssociationofInsurance17) (GA, 2020). 

  
fair decision making  AI-driven decision making should be fair sothat it does notunfairly discriminate and disadvantage individuals orgroups of individuals (European Commission 2019, 2020; aswell asother ethical AI guidelines for 


e.g.OECD 2019). Thus,non-discrimination and avoidance of unfair bias are 
17 The Insurance Core Principles (ICPs) developed by the International Association of Insurance Supervisors (IAIS) provide aglobally accepted framework of principles, standards, andguidancefor theregulation and supervision of the insurance sector. In the contextof fairness, the IAIS-Insurance Core Principle 19 (ICP19) statesthat  The supervisor requires that insurers and intermediaries, in theirconduct of insurance business, treat customers fairly, both before acontractisentered into and through to the point at which all obligations under a contract have been satisfied.  
core aspectsoffairness.Inaddition,theequalandjustdistributionofboth benefits and costs, constitute another key aspect of fair decision making (fairness of outcome) (European Commission 2019). 
At anAI system level, diverse dimensions of fairness canbe distinguished along itslifecycle, sothatitmeetsaminimumlevel ofdiscriminatorynon-harm. These include:data fairness  usageoffairandequitable datasetsonly, design fairness   inclusionofreasonable feature, processes, andanalytical structures inthemodel architecture, outcome fairness  prevent the system from having any discriminatory impact, and implementation fairness   implement the systems in an unbiased way. 
In the insurance context, this requirement can be associated with a low to medium risk depending onthe insurance use-case.For example, for someusecasessuchasLife & Health, hyper-personalization, pricing and underwriting, if fairness and discrimination aren t properly assessed at every step of the value of chain. 
Avoiding bias (i.e. unwanted discrimination) is atechnical question that canbe resolved with sound methodology for AI and algorithms. Methods to avoid unwanted bias inAI should be developed and applied in line with scientific progress. For example, in order to limit adverse impacts of enhanced pricing algorithms on customers, such as biasand implications for affordability, insurers canimplement acomprehensive post-monitoring system. Models are scrutinized by diverse teamsfrom different functions, and system output is tested using different tests to validate that the models are in line with expectations of the modelling team, business partners and regulators. 
Where regulatory approval of ratesisrequired, regulators canbe provided with dedicated tools to track the performance of the system. Particular attention should be given to potentially high-risk applications (e.g. robo-advice for asset allocation or long term financial products), which could be considered for ex-ante regulatory approval. 
Fairness issues can also be addressed by a broader ethical discussion and adherence to a proper conduct regime and transparency on underlying value-based decisions. The existingconductregime for thefinancialservicessectoralready provides arobust principle-based ethical framework (such asactuarial ethics). However, interpretation to resolve ambiguities is required. 
To monitor and mitigate bias requires the quantification of fairness. While academic research onappropriate fairness metricsisstillevolving and shouldbeencouraged18, insurers need to identify context-specific fairness definitions for each use of AI. 
18 Singapore Monetary Authority Services (MAS) launched in May 2020 aframework for financial institutions to promote theresponsible adoption of Artificial Intelligence and Data Analytics (AIDA). It willcommencewith the development of fairness metrics in credit risk scoring and customer marketing. https://www.mas.gov.sg/news/media-releases/2020/fairness metrics-to-aid-responsible-ai-adoption-in-financial-services 
Practical frameworks for assessing fairness require asuitable balance to be found between four (potentially competing) objectives19: 
  
businessmodelrequirements sustainable andprofitable product provision, which is in the long-term interests of customers,subject tomarket conditions and competitive dynamics; 

  
ensuring consumer value meeting the fundamental demand for the product; 

  
consumerprotection ensuring good outcomes and avoiding the exploitation of vulnerable customers; 

  
consumerchoice considering the alternatives thatcustomersmay ormay not have, hence avoiding the exploitation of customers with limited alternative 


Roles and responsibilities with respect to monitoring and mitigating bias should be clearly defined. As bias canenter decision-making at various stages, it is important to raise awareness at different management levels through appropriate educational and training programs. 
- ICO20 advises organisations on how they can adhere to the Data Protection Act andtheGDPR, among otherlegislation.It cutsacross every sectorandaffects the majority of organisations, including within the insurance industry. Recent and relevant ICO initiatives include Project ExplAIn, which will assist organisations asthey attempt toexplain theresults ofAIdecision-making; andthedevelopment ofanAuditing Framework for AI, that will guide the regulator s efforts in examining algorithms for fairness.  (Centre for Data Ethics and Innovation on AI and Personnel Insurance). 
Avoidance of unfair bias in the insurance sector: 
Centre for Data Ethics andInnovation on AI and Personnel Insurance   Insurers are prohibited by law from basing pricing and claims decisions oncertain protected characteristics, including sexand ethnicity. However, other data points could feasibly actasproxies for thesetraits,for example withpostcodes signalling ethnicity or occupation categories signalling gender. This means that AI systems canstill be trained ondatasetsthatreflect historicdiscrimination,which would leadthosesystems to repeat andentrench biaseddecision-making. APropublica investigation intheUS found that people in minority neighbourhoods onaverage paid higher carinsurance premiums thanresidents ofmajority-white neighbourhoods, despite having similar accidentcosts.While the journalists could notconfirm the causeof these differences, they suggest biased algorithms may be to blame. 
19 Oxera s  Fair ground: a practical framework for assessing fairness , Oxera Agenda March 2019.20 The Information Commissioner s Office is the UK s principalregulator for upholding information rights in the public interest. 
Online car insurance and Discrimination Claims 
Onlinecarinsurancecompanies usepredetermined algorithms to assessthe risk of a user filing a claim against their policy. In 2018, a public backlash21 started building against large global firms like Admiral, Marks & Spencer, Bell, Elephant and Diamond, asitwas found that insurance quotes for drivers with traditional English names, like  John , were far lower thanquotes of the same for drivers with non-English names, such as  Mohammed  for example, for identical insurance details. 
The insurance company Admiral was under scrutiny asitwas found that when applying for quotes via the price comparison website GoCompare, the sameinsurance for a2007 Ford Focus inLeicester was priced at  1,333 for  John Smith  and  2,252 for  Mohammed Ali . 
This was further validated, after obtaining sixty quotes ranging across ten different cities through a number of price comparison websites including GoCompare. According to the newspaper that obtained the quotes, in all cases the difference was often hundreds ofpounds.Following theseconcernsandcomplaints, theUK s Financial Conduct Authority (FCA) put under close scrutiny the pricing practices of UK insurers and intermediaries, placing emphasis in dual pricing and discrimination22. 
FCA s InterimReport ofthegeneralinsurancepricing practices (FCA, 2019) identifiedanumberofconcerns.Inrelation todifferential pricing,FCA s research found evidence of differential pricing between, long standing customers who continually renewed with theirexistingprovider (tended topay higher asocalled loyalty premium ) and new customers. Furthermore, FCA, set out concerns about how pricing in these markets leads to consumers who do not switch or negotiate with their provider paying high prices for their insurance. 
In relation to discrimination against protected characteristics FCA (2018) report, found noevidence of firms engaging in direct discrimination, it voiced concernsabout the potential use of data based on race/ethnicity within firms pricing models. According toFCA, firms pricing models were found tobe using datasets, including third party datasets (that were used without always undertaking the necessary due diligence soas toensure that data exclude factors that may lead to discrimination based on protected characteristics) that could  contain factors that could implicitly or potentially explicitly relate to race or ethnicity23.  
21 Money Saving Expert (2018). Accessed via: link. 22  Dual Pricing in Insurance   the beginning of the end? , April 10, 208, Retrieved from: link 
23  When firms were asked how they gained assurancethat the third-party data they used in pricing did notdiscriminate against certain customers basedonany of the protected characteristics under the Equality Act 2010. Many firms could not provide this assurance without first contacting the third-party provider. Further, some firms responded that they relied on the third-party provider to comply with the law and undertook no specific due diligence of their own to ensure that the data were appropriate to use.  (FCA, 2018, p. 15). 
Like any organisationusing algorithms to make significant decisions, insurers must be mindful of the risks of bias intheir AI systems and take steps to mitigate unwarranted discrimination. However, there may be someinstances where using proxy data may be justified. For example, while carengine size may be aproxy for sex, it is also amaterial factor indetermining damage costs, giving insurers more causetocollectand process information related toit. Another complication is that insurers oftenlack the data to identify where proxies exist. Proxies canintheory be located by checking for correlations betweendifferent data points and the protected characteristic in question(e.g. between thecolourofacarand ethnicity). Yet insurers are reluctant tocollectthissensitive information for fear of customer believing the data will be used to directly discriminate against them. As a general principle, AI systems should be trained to guarantee all human beings fair and equal treatment with no distinctions among age, gender, race, etc.. 
Use cases for Requirement 5: 
Flood Insurance: A UK initiative that balances fairness, regulation and accuracy 
Inthe UK, flooding is recognized asacommonnatural disaster and flood damage coverage is available to residential customers and small businesses aspart of the standard terms of property insurance. During the past few years advances inthe useof Geographic Information Systems, remote sensing and simulation modelling and detailed hydrological floodmodels,aswell astheabilitytoaccessnew high-quality data, have significantly improved the ability of insurers toassessflood risks. This resulted inahighly segmented home insurance market, with an informal cross-subsidy between low- and high-risk homes24. 
However, the increasing affordability problems for the individuals at high risk, lead to adebate about fairness. So, should those athigh risk pay apremium tomatch,even if unaffordable, or should they be supported by cross-subsidy from the rest of the population? 
Following theseriousUKfloodingin2000(affecting 10,000 properties in700 locations and caused  1 billion of damage25), the UK government and the Associationof British Insurers (ABI) drew up a Statement of Principles (incorporated in the Gentlemen's Agreement26). This agreement entailed that ABI members would continue to offer insurance at existing rates to properties at high risk of flooding, if the government continued to invest inflood defences. After the expirationof the Statement of Principles (2013), the fairness debate spurred againleading tothe emergence of the Flood Re partnership,asanot-for profit fund owned and managed by the insurance industry. 
24 Cullen, M. (2015) The ABI view: Sharing risk orsmoothing bad luck, Insurance Times, 19 October 2015. Available at: 
https://www.insurancetimes.co.uk/the-abi-view-sharing-risk-or-smoothing-bad-luck/1415939.article25 Environment Agency (2001), Lessons learned Autumn 2000 floods, March 2001. Available at:https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/292917/geho0301bmxo-e-e.pdf26 The unwrittenGentlemen'sAgreement between the UK Government and private sectorinsurers, facilitated the private sectorflood insurance operations inthe UK since the early 1960s (Huber M. (2004), The Breakdown ofaGentlemen s Agreement, Discussion Paper No. 18, London School of Economics andPolitical Science, ESRC Centre for Analysis of Risk and Regulation, 2004. Available at: http://eprints.lse.ac.uk/36049/1/ Disspaper18.pdf.) 
Flood Re27isajoint initiative between the Government and insurance industry that aims to make the flood cover part of household insurance policies more affordable. This flood re-insurance company essentially provides flood insurance coverage to the vast majorityofhouseholds(domestic properties) andsmallbusinessesdeemedat significantriskofflooding28 andwhere nopublic plans are inplace todefend the property (The Government andtheAssociationofBritish Insurers, 200829).The scheme that enables insurance companies toinsure themselves against losses because of flooding, became operational in 2016 with a25-year lifetime, becoming the first scheme of its kind anywhere in the world. 
RECOMMENDATIONS - Requirement 5: 
1.Fairness via the mitigation of discrimination and bias in the insurance sector is challenging, butjointefforts, initiativesandpartnershipsbetween stakeholders,like Public Private Partnerships (e.g. Flood Re), etc.canincrease our social resilience, solidarity (insurability) and social responsibility. Monitoring isanimportant parameter that facilitates the discrimination and bias mitigation processes. The Dutch Insurance Association has developed a  solidarity monitor  toassess thespread of insurance premiums and individual insurability across time. 
2.Insurance companies can eliminate biases with testing and monitoring (auditing mechanisms).Insurers shouldtestandcontinuously monitorAI systems for unwanted consequences, such asunfair biases, in which casethey shouldremove datasetsthatcreate orpreserve them,andtake human-led corrective action. In addition, AI models should be scrutinized by diverse teams from different functions, and system output should be tested using different tests tovalidate that the models are in line with expectations of the modelling team, business partners and regulators. Where regulatory approval of rates is required, regulators canbe provided with dedicated tools to track the performance of the system. Internalroles andresponsibilities withrespect tomonitoringand mitigating bias should be clearly defined. 
27 Flood Re, https://www.floodre.co.uk/28 This is generally defined asmore thana1.3%or1 in 75 annual probability of flooding  BasedonThe Government and the Association of British Insurers 2008,  Revised statementof principles onthe provision of flood insurance , July 2008. Available at:https://www.abi.org.uk/globalassets/sitecore/files/documents/publications/public/migrated/flooding/statement 
of-principles-england.pdf29 The Government and the Association of British Insurers (ABI) 2008,  Revised statement of principles onthe provision of flood insurance , July 2008. Available at: https://www.abi.org.uk/globalassets/sitecore/files/documents/publications/public/migrated/flooding/statement-of-principles-england.pdf 
3.Particular attention should be given to potentially high-risk applications 
(e.g. robo-advice for asset allocationorlong-term financialproducts), which could be considered for ex-ante regulatory approval. 
4.Educational and training programs: Asbiascanenterdecision-making at various stages, it is important to raise awareness at different management levels through appropriate educational and training programs 
5.In additiontodeveloping auditingmechanismsfor AIsystems, asolidarity mechanism to deal with severe risks in AI intensive sectors should be developed (AI4People, 2018).Thoseriskscouldbemitigatedby multi- stakeholder mechanismsupstream. Pre-digital experience indicatesthat,insomecases,it may take acouple of decades before society catches up with technology by way ofrebalancing rights andprotection adequately to restore trust. The earlier that users and governments become involved  as made possible by ICT  the shorter this lag will be. (AI4People, 2018). 
3.5 Req. 6: Societal and environmental wellbeing 
REQUIREMENT 6: Societal and environmental wellbeing 
Including sustainability and environmental friendliness, social impact, society and democracy 
AIsystems shouldbeusedtobenefitallhumanbeings,current andfuture generationsaswell asthe natural life. Consistentwith the principles of fairness and prevention of harm, the broader society, other sentient beings and theenvironment should be considered asstakeholders throughout the AI system s life-cycle (AI HLEG, 2019). AI systems should be usedtoenhancepositive social change andencourage sustainability and environmental responsibility of such systems. 
Sustainable andenvironmentally friendly AIintheinsurancesector:The environmental wellbeing issue is an emergent topic for the insurance sector, for example the EU agenda ondigitalization for atransitiontoamore sustainable future is using AItopredict andmitigate climatechangerelated risks.Assuchitisimportantto encouragemeasures securing the sustainability and environmental friendliness of AI systems inthe insurance sector. For example, selecting alow energy consumption method and increasing the environmental sustainability of an AI system in insurance. 
Social impact of AI in the insurance sector:In addition, the social impact of AI systems in the insurance sector should be enhanced. For example, AI applications can 
helptoextend insurance cover tonewandpreviously uninsured orunderinsured customer segments or to expand the range of risks for which insurance cover is available (Geneva Association, 2020). As such, AI canfacilitate the expansion of the scope of risk pooling, which lies at the core of the economic and societal role of insurance (see Table 3), according torecent study by the Geneva Association. The social impacts of AI systems at an internal (i.e. impact on workforce) and external level (i.e. customers) should be encouraged and constantly monitored. 
Table 3: Socio-economic benefits of AI for Insurance 

(Source: Geneva Association, 2020) 
Society and democracy: Theeffects (direct and indirect) ofAIsystems on societyatlargeaswell asdemocracyshouldbeconsidered andassessed.Special emphasis should be given toelectoralcontexts(e.g. amplification of fake news, etc.) andcaseswhere AIsystems intheinsurancesectormay have anegative impact. Dedicated actions and measures should be taken inorder tominimize the potential harm. 
Automated Decision-Making in Insurance: Examples from Europe 
According to the recent Algorithmwatch report30 there are anumber of examples of automated decision-making in Europe. In the contextof the Insurance sectorthe following examples were identified across different EU counties. 
Denmark: Car Insurance 
Profiling and automated decisions are present in the insurance sector in Denmark. These activities are regulated via data protection laws andoverseen by the Danish data protection authority, Data Tilsynet. In the context of car insurance, insurance companies offer rebates if drivers install abox (a type of telematics carinsurance, often called black-box carinsurance31) that measures speed, acceleration, deceleration and g-force and inturndrivers are offered afixed 25% rebate for installing the box (Spielkamp, 2019). In other cases, car insurers created a mobileapp32 (that is currently unavailable) that included driving instructions and measurements resulting in quarterly, monthly, or even more frequent adjustments to the car insurance premiums. 
Finland:BenefitprocessattheSocialInsuranceInstitution 
The Social Insurance Institution of Finland (Kela) is settling benefits under the nationalsocialsecurityprogrammes (such benefitsincludehealthinsurance,state pensions, student financial aid, housing allowances, and basic social assistance among others). Decision automation in Kela, has relied for decades in traditional automated information systems. However, AI, machine learning, and software robotics are seenas anintegral part of their future ICT systems (including chatbots for customer service, automated benefit processing, detection (or prevention) of fraud ormisunderstanding, and customer data analytics) (Spielkamp, 2019). 
Netherlands: Credit/Risk Scoring 
An increasing number of private companies offer credit scoring services that are used by numerous clients including health care insurance providers. The credit scoring companiesprovide anautomatedindicationregarding thecreditworthiness ofa potential customer. As such, the clients of the credit scoring services, such as insurance companies,canuseadditional automated decision-making process to decide whether, for example, apotential customer canhave insurance. These credit scoring companies existalongside anofficialand independent financial registration office(Central Credit 
30 A recent report by the Algorithmwatch  anon-profit organisation promoting algorithmic transparency -in cooperation withBertelsmannStiftung, supported by theOpen Society Foundations, provides acomprehensive studyofthestateof automated decision-making in Europe, listing examples of automated decision-making already in use (Spielkamp, 2019). 31 Black box car insurance involves the installation and use of a type of telematics equipment  known as a black box  which usesGPS to monitor and set carinsurance premiums based onthe driving habits of the individual. It is oneof the most well-known types of telematics car insurance, in addition to:  Plug-and-drive  (are devices that also use GPS technology, but instead of installing ablack box, that carinsurer provides adevice that individuals canplugs into their car s charging port directly) and smartphone app  (mobile app that onceinstalled it cantrack the driving habits of the individual). In US, the insurance company- American Automotive Association, is providing aspecialised smartphone based tool  AAADrive  in the AAA Mobile App that measures the driving habits of individuals and produces asafe driver score. Qualified insurance customers have the ability to view and receive ascore of their driving behavior and earnpotential discounts of up to 30% ontheir auto policy (Avi Ben-Hutta 2019,   Introducing AAADrive , Available at: link).32 See article  Mobilen giver gode bilister rabat hos Nem Forsikring  (2016). Available at: link. 
RegistrationOffice or BKR- Bureau Krediet Registratie), however inmany cases the amount of data that they collect far exceeds the amount available at the BKR (Spielkamp, 2019). 
Slovenia: Insurance 
Algorithmic systems are used in order to facilitate insurance agents in anumber of cases. The biggest Slovenian insurance company Triglav, is using algorithmic systems in order to assist its insurance agents in recommending appropriate insurance products to customers, to detect fraud and to assessinsurance risks. However, these systems are only used as counsellors and the final decision is taken by their human agents, according to a spokesperson. 
Sweden: Automated Home Insurance 
TheSwedish, homeinsurance start-up company Hedvig usestechnologyto automate many insurance processes, such as pricing and filling of insurance claims. The Hedvig app usesavoice input in order toautomatically write and send the claim so thattheinsurancecompany canautomatically process theclaimanddisbursethe payment. 
RECOMMENDATIONS - Requirement 6: 
1.AIsystemsshouldbenefitsocietyandthenaturalenvironment,nowand in the future.Therefore,  we need to adopt a culture of AI systems development thatisbothsocially-good-by-design and environmentally-friendly-by-design  (Ziouvelou and McGroarty, 2021). Towards that end, we shouldusefinancial incentives toleaddevelopment anduseofAItechnologiestowards socially preferable (not merely acceptable) andenvironmentally friendly (not merely sustainable but favorable totheenvironment) outcomes(Floridietal., 2018). This is important both atanoverall EU level and atindividual national levels. This will entail putting inplace structures and methods for assessing the socially goodness and environmentally friendliness of AI systems and projects. 
4. Recommendations for the insurance sector 
4.1 Recommendations for each of the 7 Key requirements The table below provides an overview of the key requirements for a Trustworthy AI and the associated risk levels from the industrial perspective. 
Table 4:IndustryPerspective onTrustworthy AI:Key Requirements, Associated Risks and Mitigation 
7 Requirements for Trustworthy AI  Associated Risk for the Insurance Sector (Risk Level)*  INDUSTRY PERSPECTIVE  
R1 - Human agency and oversight  Low  Risk mitigation: - engaging with various stakeholders representative of different societal voices to reach consensus on fair and responsible use of AI applications, giving particular emphasis to business use cases with a potential impact on vulnerable customer (e.g. use of mandatory behavioral change schemes)- training and skill development programs to help internal staff to embed ethical considerations within evolving business practices  
R2 - Technical robustness and safety  Low to Medium33  Risk mitigation: - implementation of state-of-the-art cybersecurity standards and control frameworks to ensure that AI applications are resilient against both overt attacks and more subtle attempts to manipulate data or algorithms  
R3 - Privacy and data governance  Low to Medium (depending on the use case)  Risk mitigation:- disciplined internal oversight of the data science pipelines- strong business data ownership and accountability can support the accuracy of data.- High dependence between the level of governance of data and the model accuracy with the business purpose (e.g. for medical related services the scrutiny level must be much higher than for marketing purposes).  
R4 - Transparency  Low to Medium (depending on the use case)  Risk mitigation: - developing and implementing internal guidelines and policies to ensure a consistent approach to the transparency and explainability of algorithmic outcomes. These guidelines need to assist the assessment (risks/benefits) on a case-by-case level.- When using more complex / non-linear AI models, to ensure wider customer acceptance it is critical to adopt highly interpretable and explainable techniques for non-technical audiences (e.g. optimal classification trees)  
R5 - Diversity, non-discrimination and fairness  Low to Medium34 (depending on the use case)  Risk mitigation: - enforcing internal guidelines and policies which continuously refine and validate use of AI applications against established ethical values, paying particular attention to sensitive customer characteristics such as race, ethnicity, gender, nationality, income, sexual orientation, ability, and political or religious belief.  
R6 - Societal and environmental wellbeing  Low  Risk mitigation: - designing AI applications that contribute to a positive and sustainable societal outcome, by ensuring that internal guidelines and policies consider the principles of solidarity and inclusiveness (e.g. social and financial inclusion, affordability) in the development of insurance products and services  
R7 - Accountability  Low  Risk mitigation: - Robust internal governance with clarity on roles, responsibilities and accountabilities - Strengthened quality assurance for model oversight and controls so as to demonstrate that internal governance systems are robust and uncompromised enough to address challenges resulting from use of  

non-linear AI models. 
33 Medium, if cybersecurity risks aren t properly mitigated.34 Medium risk, for someusecases(e.g. Life & Health, hyper-personalization, pricing and underwriting) if fairness and discrimination aren t properly assessed at every step of the value of chain. 
4.2RecommendationsfordiverseStakeholdersegments 
Recommendations to: developers/users of insurance AI 
Somerecommendations toinsurers for theresponsible andethicaluseofAI within the organisations include the following: 
  Ethical AI corporate culture: 
oInternal guidelines and policies: Insurers canmitigate AI risk by developing andimplementing internalguidelinesandpoliciesthatwillensure a consistentapproach totheethicalandtrustworthy AI,includingthe transparency and explainability of algorithmic outcomes. 
oEthical AI audits:Aninternal ethicalalgorithmauditmechanismfor the design,development anddeployment ofAIsystems ininsurance,could ensure that the moral andethical issues surrounding the use of AI are being addressed, while identifying potential biases or flaws, depending on the type of industry and globally accepted auditing procedures and standards. 
  Ethical AI training: Theresponsible design of AI entails a number of decisions that are made by engineers. As such it is important to raise awareness at different managementlevels through appropriate educationalandtrainingprograms. Contributing this way to ethical decision making at anindividual and corporate level and realising responsible AI-driven innovation-by-design. 
Recommendations to: governments and regulators 
  
There is an urgent need to set up national and international regulatory frameworks toensure democraticgovernance ofartificialintelligencesoastoprevent its misuse but at the same time facilitate responsible AI innovation. 

  
Particular attentionshouldbegiven topotentially high-risk applications (e.g. robo-advice for asset allocation or long-term financial products), which could be considered for ex-ante regulatory approval. 

  
Inorder to ensure the adequate implementation of ethics into AI there is also a need to (re)examine the  emerging  role that the regulator of the future should have. The regulator of thefuture can be seen as an entity that works closely with businessactorsfrom theinsuranceandothersectors,inorder tocertifyAI products andservices while atthesametime protect the public (WEF, 2019). In this perspective, the role of governance is shifting and expanding as a concept. The dynamics of AI bring about the need for an agile, anticipatory AI governance 

approach. Thatisanadaptive, human-centred, inclusive andsustainable policymaking approach, anchored in the notion that policy development is no longer limited togovernments but rather is anincreasingly multi-stakeholder effort (WEF, 2018). 

  
Multi-stakeholder consensus should be reached on what constitutes a responsible useof AI and data. This involves the active engagement of the public, private sectorsaswell asresearch andacademicstakeholders and the civil societyat large, but also thedemocratisation of AI across these segments.  

  
At aninfrastructure level, abig challenge for the insurance sector relates to the standardization of the exchange of data with 3rd parties such as banks (i.e. what kind of data is involved, etc.). Facilitating data exchanges is acrucial component of AI-driven innovation in insurance.  


Recommendations to: customers 
  Actively engageindiscussionswithdiverse stakeholders andcommunitiesin order toexpress what constitutes aresponsible useof AI and data from their perspective.  
4.3 Generic Recommendations 
An EU Ethical and Trustworthiness AI Label 

The operationalisation of ethically-sound AI applications, could involve thecreation ofaEuropean Union AIEthical and Trustworthiness Index that could function as an  AI ethics label  (similar to the European Union Energy Label) that would rate andclassifythedifferent AIapplications basedontheirAI Trustworthiness level (at a requirement level), while considering thecontextualparameters.Thisratingcouldbeimplemented before theuseofanAI system (and could even be used asa guideline during itsdesignphase) andshouldbeconstantly monitored andupdated.Furthermore, therequired level to ensure thatanAI application is ethical and trustworthy would be highly dependent onthe application context(i.e. industrial context, B2C, B2B, B2P, etc.) and thus the associated risk of the current and future use of the system. Such alabelling scheme would provide useful information, in 
Figure 2: AIEIG AI Ethics Label(Source: AIEIG, 2020) 
aneasytocommunicateway tothe end usersasthey choose between products and servicesfrom different companies,totheEuropean policymakers, regulators and standard-setting organisations to constantly update and enhance their requirements for ethical and trustworthy AI systems as well as to encourage companies to design, develop, implement and invest inethical and trustworthy AI applications-by-design and -by default. 
An example of this approach is the VCIO model and AI ethics label that has been developed by the AI Ethics Impact Group35 (AIEIG, 2020) and which combines four conceptual parameters, namely values, criteria, indicators and observables in order to evaluate AI (Figure 2). 
New models of Governance 
Artificial Intelligence will profoundly change all industrial sectors, transforming ourbusiness and social interactions. Equally profound is and will continue tobeour need for newprinciples, algorithms, and policies soasfrom theoneside to accelerate the positive impacts of AI and from the other to minimise the negative (expected and unexpected) consequences. Towards this aim, newmodels of governance are needed that will move beyond traditional reactive governance towards anticipatory models of governance. Such innovative governance models will be agile and adaptive innature. They will be human-centric, inclusive and sustainable by design (agile governance). Such models will be centered around collaborative, multi-stakeholder policy development processes, ratherthangovernment-centric functions. Theywillembracechangeand empower rapid, on-going readiness and adaptiveness (proactively and anticipatory). 
Balancing between top-down, bottom-up and middle-out AI approaches/policies 
Inorder to seize the opportunities that Artificial Intelligence offers to society, we are confronted with adaunting challenge: how canwe promote the development and deployment ofAI-driven innovation andenhanceexistingbusinesspracticesinthe insurance sector aswell asin other sectors, while at the sametime limit the risks and failures? How canwe ensure citizenprotection-by-design, andincrease trustand confidence inAI? How canwe balance innovation and trustworthiness in AI, without compromising ourethical standards and fundamental rights? Furthermore, the lack of specific and verifiable AIprinciples and measures may threaten the effectiveness and enforceability of AI ethics guidelines. 
35TheAIEthics ImpactGroup isaninterdisciplinary consortiumledby VDE Associationfor Electrical,Electronic & Information Technologies and Bertelsmann Stiftung. 
Analysis shows thatabalancedapproach couldbeofbenefitfor ensuring ethically sound  design, development, implementation and evaluation of AI systems (by-design) in the insurance context among others. Aiming to find the framework that will provide the right mix that will from theoneside prevent(proactively) the potential negative effects to fundamental rights and human-centric ethical standards and from the other side provide afuture-proof and innovation-friendly framework. Consequently, such an approach would entailtheeffective integrationofdifferent typesoflegalregulation namely:  top-down  (legal regulatory action),  bottom-up  (self-regulation) and  middle out  (co-regulation and coordination mechanisms for the governance of AI36) policy actions, considering the contextual parameters of the different AI insurance applications and the associated risks (current and anticipated). These different levels of legal regulation are aligned with AI4People s Report on Good AI Governance (Pagallo et al., 2019b), and include: 
I.Traditional legal regulation - top-down  approach -including both hard law and soft law actions. such as the Opinions of the Art. 29 Working Party and, nowadays, of the European Data Protection Board (EDPB) in the field of data protection, as asetofrulesorinstructionsfor thedeterminationofevery legal subject ofa system. Theseare therulesthataimtodirectly govern socialandindividual behaviour, and mainly hinge on the threat of physical (and financial) sanctions as a means of social control (Kelsen 1949); 
II.Self-regulation - bottom-up  approach -including thedifferent forms ofself-regulation inall its variants. According to Directive 2010/13/EU,  self-regulation constitutes a type of voluntary initiative which enables economic operators, social partners,non-governmental organisationsorassociationstoadoptcommon guidelines amongst themselves and for themselves  (Recital 44)37.An example of suchabottom-up approach, in the context of AI, is the final Assessment List for Trustworthy AI38 (ALTAI), presented by the European High-Level Expert Group onAI duringJuly 2020, which is intended for self-evaluation purposes. Sector-specificconsiderationscanaddrelevant elementstotheALTAI list,for each specific AI system. This type of actions acts in a complementary manner and does not substitute other legislative requirements. 
III.Co-regulation - middleout approach  incorporating bothelementsoftop-down legal framing and bottom-up empowerment of individual actors. According toDirective 2010/13/EU,  co-regulation gives inits minimal form alegal link 
36 The  middle-out  layer (Pagallo et al., 2019a; 2019b) includes everything that lies between the top-down and bottom-up approaches and is associated with forms of co-regulation. 37 Directive 2010/13/EU of the European Parliament and of the Council of 10 March 2010 on the coordination of certain provisions laid down by law, regulation oradministrative action in Member States concerning the provision of audiovisual media services (Audiovisual Media Services Directive).38 The ALTAI is also available in a web-based tool version, available at: link. 
between self-regulation and the national legislator  (Recital 44)39.In this context, theregulatory role isshared between stakeholders andthegovernment orthe nationalregulatory authorities orbodies (Pagallo etal., 2019b). In the current EU legal framework, this  middle-out  layer is mostly associated with forms of co-regulation models, such as the GDPR (Pagallo et al., 2019b)40. 
AclassificationframeworkforAIapplications 
The following framework is based on Ziouvelou and McGroarty (2021). In order to facilitate the creation of novel AI insurance applications and services that will serve thecommon benefit and welfare and the customerneeds in anethical and trustworthy manner, a classification framework for AI systems and applications is needed. 
Basedonthe two axis that focus onthe level of risk and the dependence onthe decisionoroutcomeof the AI application, four distinct categories emerge that have different levels of significance (high, medium, low), as shown in Figure 3. Furthermore, two additional subcategories emerge within Category 1 and Category 3 denoting the extreme andopposite cases, namely: subcategory 1a  where we have the lowest risk level and the lowest possible dependence on the decision and at the other end, subcategory 3a  where we have highest possible risklevel associatedwiththehighest possible dependenceonthe decision. The highest class, 4 inthis case, serves to classify contexts where noAI system should be applied. For algorithmic decision-making systems that fall between these two extremes, asubdivision of at least three further classes seemsto make sense to reflect increasing system requirements adequately. 
-Category 1: AI systems that belong inthis category have a low risk level and our dependenceontheirdecisionislow. ThiscategoryentailsAIsystems oflow significance that necessitate the lowest need for transparency and the lowest need for intervention. Assuch bottom-up  policyactions may sufficiently cover the trustworthiness requirements of applications that belong in this area. -Subcategory 1a: This subcategory depicts oneof the extremes asit entails AI systems that have the lowest possible risk level and the lowest possible dependence on their decisions. As such, these systems have a very low overall significance and anequally low associated impact for humans and nature. Theycanbe parts ora basic stand-alone AI systems, which once assessed and classified in this subcategory they may necessitate no further action. 
39 See previous note. 40 According to Pagallo etal., (2019b) in the GDPR  the mixed approach togovernance revolves around the  principle of accountability thatthrough amixofprimaryandsecondary legal rulesaimstostrike abalancebetween guaranteeingcompliance with both the principles and the top-down rules of the system, while leaving room for self-regulatory measures.  
Figure 3: A classification framework for AI applications/systems and associated policy action (Source: Ziouvelou and McGroarty, 2021) 

-Category 2:AI systems that belong in this category have ahigh-risk level and ourdependence ontheirdecision is low. This category includes AI systems of mediumoverall significance that necessitate avery high need for transparency as well asintervention. Consequently, they call for top-down legalregulations and co-regulation actions (middle-out). -Category 3: This category depicts AIsystems of high overall significance that exhibitahigh-level of risk and ourdependenceontheir decisions is high. Such systems require thehighestlevel oftransparency andintervention. Thus, top-down policy actions are needed for AI systems in this category.  -Subcategory 3a: Thissubcategoryentails AIsystems thatexhibit thehighest possiblerisklevel andthehighestpossibledependenceontheirdecisions. Consequently, these systems have avery high overall significance and impact for humans and nature. As such AI systems, assessed and classified in this subcategory they should not be permitted. -Category 4: AI systems that belong in this category have a low-risk level and our dependenceontheirdecisionishigh.Such systems have amediumoverall significanceandnecessitateahighneedfor transparency andintervention. Consequently, they necessitate both top-down legal regulations as well as middle-out actions. 
References 
Spielkamp, M. (2019). Automating Society: Taking Stock of Automated Decision-Making in the EU. BertelsmannStiftung Studies, AW AlgorithmWatch gGmbH, 2019. 
Centre for Data Ethics and Innovation (2019). AI and Personnel Insurance, CDEI Snapshot Series. 
Council of Europe (2019). Unboxing Artificial Intelligence: 10 steps to protect Human Rights. By the Council of Europe, Commissioner for Human Rights, May 2019. 
Deloitte (2015). Insurance Disrupted. General insurance in a connected world. 
European Supervisory Authority for the Insurance Industry (EIOPA) (2019). Big Data Analytics in Motor and Health Insurance: A Thematic Review. Luxembourg: Publications Office of the European Union. 
European Commission (2019). Ethics guidelines for trustworthy AI. High-Level Expert Group on Artificial Intelligence (AI HLEG), April 2019. Retrieved from: https://ec.europa.eu/futurium/en/ ai-alliance-consultation/guidelines 
European Commission (2020). On Artificial Intelligence   A European approach to excellence and trust. Retrieved from: https://ec.europa.eu/info/sites/info/files/commission-white-paper-artificial intelligence-feb2020_en.pdf 
European Commission (2020). Assessment List for Trustworthy Artificial Intelligence (ALTAI) for self-assessment. High-Level Expert Group on Artificial Intelligence (AI HLEG), July 2020. Retrieved from: https://ec.europa.eu/digital-single-market/en/news/assessment-list-trustworthy-artificial intelligence-altai-self-assessment 
Fang, Lei, Gianvito Lanzolla, and Andreas Tsanakas. "Digital Technology Adoption and Changes in Management Priorities." Academy of Management Proceedings. Vol. 2020. No. 1. Briarcliff Manor, NY 10510: Academy of Management, 2020 
Financial Conduct Authority - FCA (2018) Pricing practices in the retail general insurance sector. 
Financial Conduct Authority - FCA (2018),  Pricing practices in the retail general insurance sector: Household insurance , Thematic Review, TR18/4, October 2018. 
Financial Conduct Authority - FCA (2019), General insurance pricing practices, Interim Report, Market Study, MS18/1.2, October 2019. 
Floridi, L., Cowls, J., Beltrametti, M., Chatila, R., Chazerand, P., Dignum, V., ... & Schafer, B. (2018). AI4People   An ethical framework for a good AI society: Opportunities, risks, principles, and recommendations. Minds and Machines, 28(4), 689-707. 
International Association of Insurance Supervisors (IASIS) (2016), Issues Paper on Cyber Risk to the Insurance Sector, Financial Crime Task Force, August 2016. 
Kellogg, Katherine C., Melissa A. Valentine, and Angele Christin. "Algorithms at work: The new contested terrain of control." Academy of Management Annals 14.1 (2020): 366-410. 
Lanzolla, Gianvito, Danilo Pesce, and Christopher L. Tucci. "The digital transformation of search and recombination in the innovation function: Tensions and an integrative framework." Journal of Product Innovation Management (2020). 
Larsson, S., Anneroth, M., Fell nder, A., Fell nder-Tsai, L., Heintz, F., and Cedering  ngstr m, R. (2019). Sustainable AI: An inventory of the state of knowledge of ethical, social, and legal challenges related to artificial intelligence. AI Sustainability Center. 
OECD (2019). Recommendations of the Council on Artificial Intelligence (adopted on May 22, 2019). Retrieved from: https://legalinstruments.oecd.org/en/instruments/OECD-LEGAL-0449 
O Neal, C., (2016). Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy, New York, NY: Crown. 
Pagallo, U., Aurucci, P., Casanovas, P., Chatila, R., Chazerand, P., Dignum, V., ... and Valcke, P. (2019a). AI4People-On Good AI Governance: 14 Priority Actions, a SMART Model of Governance, and a Regulatory Toolbox. 
Pagallo, U., Casanovas, P., and Madelin, R. (2019b). The middle-out approach: assessing models of legal governance in data protection, artificial intelligence, and the Web of Data. The Theory and Practice of Legislation, 7(1), 1-25. 
TOGAF, (2017). An introduction to the European Interoperability Reference Architecture (EIRA) v2.1.0. Retrieved from: https://joinup.ec.europa.eu/sites/default/files/distribution/access_url/2018- 02/b1859b84-3e86-4e00-a5c4-d87913cdcc6f/EIRA_v2_1_0_Overview.pdf 
The AI Ethics Impact Group (AIEIG), (2020). From Principles to Practice, An interdisciplinary framework to operationalise AI ethics. Retrieved from:  https://www.ai-ethics-impact.org/resource/ blob/1961130/c6db9894ee73aefa489d6249f5ee2b9f/aieig---report---download-hb-data.pdf 
The Bribery Act (2010), UK Ministry of Justice. Retrieved from:  http://www.justice.gov.uk/ downloads/legislation/bribery-act-2010-guidance.pdf 
The Geneva Association (GA) (2020). Promoting Responsible Artificial Intelligence in Insurance. Jan 2020, International Association for the Study of Insurance Economics. 
The Guardian (2016). Facebook forces Admiral to pull plan to price car insurance based on posts, by Graham Ruddick. Nov. 2, 2016. Retrieved from: https://www.theguardian.com/money/2016/ nov/02/facebook-admiral-car-insurance-privacy-data 
Universal Declaration of Human Rights. Retrieved from: https://www.ohchr.org/EN/UDHR/ Documents/UDHR_Translations/eng.pdf 
World Economic Forum (WEF) (2018). Agile Governance: Reimagining Policy-making in the Fourth Industrial Revolution. WEF White Paper, April 2018. 
World Economic Forum (WEF) (2019). A  Governance: A holistic approach to implement ethics into AI. WEF White Paper Jan. 2019. 
Ziouvelou, X. and McGroarty, F. (2021), A classification framework for AI applications/systems and associated policy action. Working paper, Centre for Digital Finance, University of Southampton. 
6 
LEGAL SERVICES INDUSTRY 
Authors 
Burkhard Schafer 
Innovation Officer and Research Scientist, Institute of Informatics and Telecommunications, National National Centre for Scientific Research Demokritos, & Member of the Scientific Committee on Data Policy and Artificial Intelligence, 
National Council for Research and Innovation (NCRI), Greece 
Cornelia Kutterer 
Senior Director, Rule of Law & Responsible Tech, European Government Affairs at Microsoft 
Elisabeth Staudegger 
Professor at Universit t Graz, Austria 
Evdoxia Nerantzi 
Policy Manager, European Government Affairs at Microsoft 
Jacob Slosser 
Carlsberg Foundation Postdoctoral Fellow at University of Copenhagen, Denmark 
Jamie J. Baker 
Associate Dean and Director of the Law Library; Professor of Law at Texas Tech University School of Law, USA 
Mireille Hildebrandt 
Research Professor on 'Interfacing Law and Technology' at Vrije Universiteit Brussel, Belgium 
R n Kennedy 
Lecturer in Law, School of Law, National University of Ireland Galway, Irelands 
1. Introduction: scope and remit of the report 
1.1. Methodological and terminological preliminaries 
The aim of this report is to distil a number of ethical principles to assist developers of  legal technology  and the users(such aspublic administration decision-makers and thelegalprofession) who commissionit.We are addressing aspecificAI-driven technology that is intended for use in the legal sector, giving  AI  a broad interpretation thatincludes symbolicAIandrule-basedsystems aswell aslegal ontologies, legal information retrieval ordata driven approaches such asmachine learning and neural networks. Since the terms  legal tech  and  legal technologies  are now commonly used, we willusethese to refer to those usesof AI that are germane to the delivery of legal servicesandthejusticesystem, excluding  generic  technologiesthatmerely assist lawyers with tasks, such asbilling and other back office activities. Our focus isonthe useof artificial intelligence in the senseof symbolic logic developed to translate legal normsinto computer code, aswell asmachine learning developed for legalsearch or e-discovery. We use  legal technology  with quotation marks to emphasise that we mean theusein the legal domain, rather than anevaluative attribute of the digital artefact (legal vs illegal or ethical and unethical AI). 
One challenge for suchaproject is the sheer scope of applications of AI in the justicesystem. Theyrangefrom intelligentinformation retrieval toolstodecision supportsystems insentencing; from automatedcompliance for businessprocesses ( code aslaw ) toAI tools that help visualising evidence in trials; from chatbots that help citizens to file a complaint or generate documents to tools that predict the outcome ofcasesfor litigation risk assessment.Many of these tools canbe framed under the heading of  legal search , andare already integrated in legal practice. Without claiming acomprehensive overview we direct thereader toexamples suchasWestlaw Edge, Lexis-Nexisandmany more thatare specifically relevant in Europe.Whilesomeof the recommendations below will be morerelevant to somesuch applications than others, for all of them responsible use requires a clear focus on the moral underpinnings of law and the rule of law. This is why our focus ison thejustice system in the broad senseof thatterm(including legislature, judiciary, public administration, public prosecutor, police and professional legal advice), because through them and their interdependencies law and the rule of law acquire concrete form. 
Onthetechnological side, thewiderangeofapplications meansthatawide variety of approaches to AI are used, from  good old-fashioned AI  (GOFAI) in chatbots, to machine learning in prediction-based approaches, each with their own risk profiles and ethical challenges. On the application side, this meansthatwhat is atstake for individualscandiffer dramatically, from determiningaprisonsentencetomissed opportunities to file asuccessful compensation claim for aminor product defect, oran automated check if all data used in abusiness process have appropriate consent forms associated with them. While the severity of the impact is oneelementtotake into accountinany assessmentof the impact onlaw and the rule of law, two thingsare importanttoremember. First, inthejusticesystem, there isnofixed andcontext independent matrix of  severity . To stay with theexample, getting legal aid for small claims litigation may be amajor issue for thoseonalower income, and have asimilar impactonthemasahigh impact commercial litigation could have onawell-resourced party. Harm,in both cases, may go beyond the economically tangible and deprive them of the dignity they deserve ascitizens and human beings. Second, ethical assessment cannotstopattheevaluation of single transactions oruse-events. Some of the most problematic ethicalconsequencesoftechnologyinthelaw willaccruewhen such missedopportunitiesaccumulate,orsmalldisadvantages becomeasystemic and endemic feature. 
The wide scope of AI applications in the legal domain also meansthat the central questions of any ethics of AI, i.e.  how are existing power imbalances, injustices and biasesperpetuated, amplified orchallenged,andwhat newpower relationships will emergeduetonewplayers enteringthefield? , may have quitedifferent answers in different contexts. Transactional law, for example, a lawyer using a software tool for the drafting of contracts for well-resourced parties with access to independent high-quality legal advice, obviously poses very different ethical issues from a program that determines the eligibilityfor legal aidorsocial security payment for indigent claimants, orwhich supports decision making in criminal trials. 
Technology isnever neutral. Every useofAItoolsinthelegalfieldcanhave consequences that impact law and the rule of law. Legal information retrieval systems may lead to aloss in accessto the law for somegroups, or asoften claimed  toan overall increase ofaccesstojustice.It canmarginaliseuncomfortable butnecessary minority opinions, or amplify them. Computational decision support in criminal trials canamplify orreduce human biases in decision making. Document assembly systems canreduce costs for indigent litigants, but canalso encourage frivolous litigation that increases demand for legal solutions and hence costsfor the courts, orbe used asa pretext for the government to reduce legal aid even further. One key principle that we propose below isa rule-of-law riskassessment thatissensitive tothesevarious parameters. 
As noted above, this report focuses onthose issuesthat set law asadomain apart from other applications. It should therefore beread in conjunction with the general guidelinesonresponsible development andemployment ofAIthatAI4People has developed. These lay out principles that any ethically sound development of AI ought toobserve, while thisdocumentwillfocus ondutiesthatare specifictothelegal domain and create obligations that go above and beyond those ethical concerns identified in the first two AI4People reports that apply to all uses of AI. 
It isalsoimportanttoreiterate thatethicalconsiderationsstartwhen legal compliance has been achieved. None of the ethical principles stated in what follows should be misunderstoodasanalternative tocompliance with applicable laws, oras pre-empting further regulation. Aspiring to ethical excellence is only possible when it is driven by genuine concern for the wellbeing of others, not when it is instrumentalised to prevent appropriate control through democratically legitimised regulators backed up by sanctions. Thisisparticularly importantfor applications thatmay have severe implications for the fundamental rights of citizens. Where such an infringement can be foreseen, appropriate legislation andinstitutional safeguards ratherthancallingfor adherence to ethical guidelines will be necessary. 
Finally, we needtoremain mindful of the dangerof technological determinism and solutionism. Aswe will see below, some of thecurrent debates surrounding ethical useof AI have along history, ahistory that also traces failed attempts to solve difficult issues of fairness through merely improved technology. We should therefore not start from the assumption that the adoption of aproposed  legal technology  is agiven and the only concernis to shape them in ethically less harmful ways. Rather, the question of their introduction always requires keen attentiontothe preliminary questions (1) what problem they supposedly solve, taking into account (2) what problems they will notsolve and (3)what problems theymay create. Deployment shouldnotbethe default. Spiekermann-Hoff talks in this contextof  the artof omitting , the prudential decision to not develop now, or not to develop at all, a certain application of a technology. 
1.2. AI in Law as a unique challenge for AI ethics 
The use of artificial intelligence in the justice system creates significant opportunities toaddress known shortcomings and failings in the administration of justice, but also poses uniqueand serious dangers, not only for individual citizens, but for the rule of law ideal more generally. 
Any attempttobuildanethically responsible approach tothedevelopment, deployment and post-deployment monitoring of AI in the justice system and the legal services industry thus faces a number of challenges that are specific to the legal domain. An ethos of  move fastand break things  is ill ateasewith conceptions of justice and the rule of law thatevolved slowly over centuries,and which are anchored in complex social understandings of law, justice and its role in democratic society. 
At thesametime,recent events have also brought into stark focus thatnotall aspects of legal traditionsare worth preserving, and that many of them perpetuate deep injustices. 
Technology canplay apositive role inaddressing theseproblems. Thisisnot limited to legal institutions that do not work as intended or became corrupted, but also concerns legal institutions that work as intended and are imbued with particular ethical significance. Sometimes normative justifications of existing practices have been back-projected on legal institutions that arose for very different historical reasons; sometimes practices and institutions arose astheresult of ethical concernsthatwere valid at the time of their creation, but are now obsolete. This means that not every conflict between  legaltechnology  andthe internal logicoflegal institutions, ortheirfunctioning,is necessarily wrong. Any claim that a practice or institution serves justice deserves scrutiny and criticism. Even such a hallowed and central proposition such as  equality before the law  is known toleadtosubstantive injusticewhen applied tooformalistically  the famous prohibition preventing rich and poor alike from sleeping under bridges, ora prohibition oneithermenandwomen breastfeeding in public. At the sametime, legal institutionsindemocraticsocietiesdoreflect deeply heldethicalcommitmentsand communal practices centred around shared values. An appropriate ethical assessment of AI in law therefore needs both  awillingnesstocritique established features and institutions of the law that are in need of change, and respect for the values and ethical commitments that are inscribed in many of these practices and institutions. 
Legal systems across theworld struggle with high costs of litigation and efficient enforcement of rights, and with often unconscionable delays in the administration of justicesothat justice delayed does indeed often become justice denied. Even in rich countries, large parts of the population are oftenexcluded from affordable legal advice. Absentarealistic ability to challenge discriminatory orotherwisewrongful application of the law, abusive practices against vulnerable citizens can become institutionalised. 
Here technology canbeaforce for good, and not using its full potential in itself becomesanethical failure. Law regularly deals with citizens who are in particularly vulnerable conditions, and subject tosevere power and information imbalances. The employment of AI in law couldreduce these imbalances by making information more easily accessible,butcouldalsoamplify andentrench themfurtherintothevery infrastructure oftheadministrationofjustice, making themeven lessresponsive to social needs and more difficult to change through the political or judicial process. 
Legal systems are also shaped by the need to reconcile deeply held and conflicting intuitions about justice. The application of general legal normsarticulated in natural languageinevitably requires interpretation andmore orlessdiscretion, which is however itself guided by the underlying principles of the rule of law. The dual imperative of equality before the law  and  doing justice toparticulars  (Bankowski and others), following the rules but where appropriate showing mercy and empathy, thusrequires afurther balancing act that ensures the adaptiveness and the contestability of the law. Theantinomian characterofthisdualimperative isafeature ratherthanabug (Radbruch), and we must be mindful of the danger that the success of even a beneficial and, on its own terms, ethically sound legal technology risks  resolving  this antinomian character by silencing ormarginalising competing intuitions about the just society and stiflemore substantial social change. Finally, while other fields of AI application can lookatlaw asanexternalmeanstocontrol abuse, the transformative effects onlaw itself make thismore difficult, and raise the stakes substantially. Were AI systems to undermine the law, its ability to control their application would be lost. 
Publicexcitement about newtechnologies risks that lessons from the past can easily beforgotten. Legal systems by contrastare always also,for betterorworse, repositories ofpast experiences andacollective memoryofproblem solving. While there has been a recent upsurge in interest in legal AI, the idea has a long pedigree, with early systemsdeveloped in the 1970s and 80s. While most of them were expert systems thatfocussed onexplicit, symbolicrepresentation ofandreasoning withrules, applications ofmachinelearningcanalsobefound from the1990sonwards, for instance in the Split-Up system developed by Zeleznikow. In 1996, the UK became the first country to legislate for fully automated decision making in the Social Security Act. Revisiting some of the ethical debates of the time is instructive, both to see the continuity of ethical concernsthatwere raised then and now, but also tobereminded of issues that are not as visible in the current debate. 
Lookingattheparliamentary debate, we canidentifythree different typesof concerns:
 a.How canthelegislature ortheexecutive be certain that the computer program 
faithfully replicates the law   are the decisions it reaches correct?
 b.If theyare not, how cancitizensknow, andwhat are theremedies attheir 
disposal?
 c.Even ifthedecisionswere correct inallcases,andcouldbecommunicated 
adequately to the citizen, isn t the decision to useit for social benefit applicants, 
and only for these, sending a symbolic message that some of the most vulnerable 
members of society do not count, that the state has given uponthem and now 
only manages themthrough acoldmachinery, where itshouldembraceand 
integrate them fully? 
It isespecially thethird aspectoflegaltechnology thatisoftenmissingin contemporary debates, which focusses onquestions of technical correctness, debiasing of algorithms and explainable AI. It reminds usof the fact that simple technological solutions alone will not resolve complex social problems. While improvements in our ability to  get the results right  are of course welcome and important, and also create an ethical obligation to usethe best available tools, there remains avery real danger that they will nonetheless amplify and entrench existing biases and injustices. The principles therefore try to reach amore holistic appraisal of  legal technology , and urge caution of their usein situationswhere thevalue of  being judged by one s peers  also means  being judged by one s fellow humans . 
Despite great expectations in the transformative potential of  legal technology  in the 80s and 90s of the previous century, and significant uptake insomeapplications such as tax law, the first wave of  legal AI  did not result in the substantial transformation of the justice sector that somehad anticipated (see e.g. Susskind). Many of the current technologiesshare many ofthefeatures oftheseearlierattempts.Legalchatbots represent knowledge in very similar ways to earlier expert systems, data-driven pattern analysis in courtdecisions, and earlier ideas of modelling discretionary legal decision making. Undoubtedly, there have been significant improvements in the technical tool kitavailable todevelopers (see Bench-Capon 2015 for ahistorical timeline), both in terms of software and algorithms, and the hardware necessary to process large data sets atever increasing speed. However, someof the reasons why this time we may seea much greaterimpact on the justice system, have less to do with changes in the way legal knowledge is representedand made computational, and more with wider social and technologicaldevelopments, someofthemwithethicalsalience.More datais born digital , including data generated by the justice system. This creates newopportunities but also dangers for accessto justice, ascommercial AI systems canalsocreate walled data gardens. Coding has become easier, and in particular, platforms that support  no code automation  enablemany more people tobuild simple legal applications. While the development of earlier systems typically took place in university environments or particularly large and well-resourced law firmsorpublic administrations, this has now becomepossiblefor small start-ups, individual practitionersorcitizen-activists. Platforms suchasJosefpromise  legal automationfor everyone  and have significant potentialtosupportthework ofNGOs andotheractiviststhattrytoaddress shortcomingsinthecurrent justicesystem. While thisdemocratisationof legal technology  has significant potential to contribute to the  good AI society , it also means the loss of gatekeepers, bringing with it problems of quality control and rogue players. The principles account for this through a risk assessment framework that matches the danger of harm with arange of duties of quality assurance.Finally, the ubiquity of smartphones together with developments such asonline banking has increased both our willingness and expectation to carry out more complex tasks which would previously have involved expert assistance onourown, at atime of ourchoosing. It has changed thelandscape ofthediscourseondigitalexclusion andthedigital divide, butnot eliminatedit.Thismeansgreat care hastobetaken toensure thattheunderlying infrastructure for legaltechnologydoesnotamplify inequalitiesandexclusion, somethingatleastasimportantasthe fairness ofthealgorithmsthemselves, and inseparably connected with it. 
2. Foundational Principles for Responsible use of AI in law 
In this section we develop a series of principles that concretise the specific ethical concernsthat should inform both the development and the useof AI in law onahigh level of abstraction that applies tothewhole range of technologies and usecases.As theyconcernthe foundations of law and the rule of law theycannotbereduced to ethics, indeed they concern the nature of modern positive law as informed by the rule of law. 
2.1. Principle of Respect for the integrity of law and the rule of law 
We identifiedrespect for the rule of law asthe key ethical concernthat sets law apart from other domains of AI application. It creates a way of thinking  orinternal logicthatiscentraltolegalreasoning andisnot always alignedwiththeway AI developers conceptualise the world. With  integrity of the legal system  we mean respect for law asaseparate discourse that cannot be replaced ortaken over by other modes ofthought.We musttherefore firstclarifyandmotivate what we meanwiththe centrality of this concept, and its dynamic and often contested nature. 
Different technologies  fit  in different ways tocompeting conceptions of justice and competing legal philosophies. Widespread adoption of comparatively simple rule-based expertsystems andsimilar technologies thatfirst entered the scenein the 1980s and 90s, andface aresurgence through legal chatbots ontheonehand, compliance support tools  onthe other, may seemtoenhanceformal equality, but could shift the balancetowards formalism simply becausetheyare unabletoaccommodatemore nuanced and discretionary reasoning. This can lead to injustices in casesthat do not fit their pre-defined categories. Data-driven approaches, by contrast, could in theory allow decisionmakers suchasjudgesorpublicsectoradministratorstodo justiceto particulars  and consider much more nuanced and much more varied factors in their decision making than currently possible, and for instance tailor sanctions much more tothe individual circumstances ofacase.However,  personalised law , when taken to the extreme, conflicts with the promise of equality before the law, may destroy social bonds and allows extraneous factors to influence outcomes. 
Every legalsystem tries tobalancetheseconflictingvisions,equalityand predictability, through rule adherence andresponsiveness to individual differences and contexts.Oneoverarching danger oflegal technology, even when itsoutcomesare seemingly benign, is thatsubstantive social debate and democratic deliberation about the nature of justice and how toachieve itare pre-empted by what is technologically possible. This can be seen in proposals of  mechanising  legal and administrative decision making, not because there is adesire in society for amore uniform application of rules that passed public debate and scrutiny, but because this is the only thing that is currently technologically feasible, or worse, feasible given cost constraints. 
Furthermore, oncecomputing infrastructures are put in place at significant cost, thesemay beresistant tochangeinlinewithsocialattitudes, again side-lining the democraticdecision-making process. Thisalsorefers tothedifference between (1) legality as part of the moral backbone of the rule of law, which is intimately linked with theargumentative nature ofthelaw thatnevertheless provides for legal certainty (Waldron), and (2) the computational legalism that emerges in the contextof both rule-based and data-driven AI in law (Diver). 
For the purpose of this report, this poses oneof the multiple challenges that need tobe taken into consideration when assessing the ethical implications of aproposed technology. Preserving the integrity of alegal system (Dworkin), its internal logic and values, isanimportantconsideration.At thesametime, disrupting structures that perpetuate injustice can be ethically mandated, eventhough it can raise in turn questions ofdemocratic legitimacy and accountability ifdoneby software developers ortheir clients. 
Turning theseabstractideasintomore concreteactionablerules,we have the following overarching Principle of Respect for the integrity of law and the rule of law: 
Any use of AI must respect the integrity of the legal system, the values inscribed therein, 
andadhere topractical and effective respect for theruleof law. Disruptionsof the internal logic of law are permissible only if they in turn are justified by anoverriding 
ethical mandate. 
2.2. Principle of purposiveness 
As both use and non-use of AI in the justice system may create risks for individuals and society, anticipatory evaluation of potential impacts are central for ethically robust development of AI in law. From this we can derive an overarching general requirement of anticipatory impact assessment:Any decision onthe development and deployment of AI in law needstostartwithanassessmentof potential risks for thevalues of the justice system, for the groups likely to be adversely affected, and, where applicable, for the human rights of those who will interact with the technology, be it assubjects of a decision,asusers,asdata engines oraslegal knowledge experts. As arequirement of the democratic state under the rule of law, such impact assessments should be publicly accessible to allow scrutiny by civil society. 
We do not adhere to the innovation mantra that the burden of proof is onthose opposingtheintroduction of legal technology . We therefore propose thatthefirst stage of any impact assessment consists of aclear and comprehensible answer tothe three questions related in the introduction. We name this the principle of purposiveness, which should be integrated in the anticipatory impact assessment: 
Principle of purposiveness: 
Any proposed legal technology should be explicitly upfront about 
(1)
 what problem it supposedly solves, 

(2)
 what problems it will not solve 

(3)
 what problems it may create.  


Only withthisdegree oftransparency attheinitial stageofaproject cana meaningfulethicalevaluation take place. Thisevaluation thenhastodetermine,in addition to the utility of the technology to achieve these goals, the following: 
a.How will the technology in the short term impact onthe character of all the 
stakeholders thatare affected; how doestheirrole andself-understanding 
change? 
b.Which human,social,economicaletcvalues are positively ornegatively 
affected? 
c.What are the essential societal values and priorities that need to be protected 
from this development? 
2.3. Principle of respect for (the situated nature) of the rule of law 
This principle inevitably poses the questionof the standard against which such an impactassessment should take place. EvenwithinEurope, for historicalreasons, there is substantial divergence regarding how legal systems operate, and how justice is best served. 
To give an example, in some legal systems, the jury is seen as essential not just for efficient decision making, but as a requirement of justice. A random selection of citizens 
who candecidewithoutfear ofrepercussions, and(therefore) withoutpublishing reasons, isseenasthe only way toconstrain theexercise of arbitrary power by the executive, creating public acceptability of and trust in the justice system, and ensuring fairness to the individual by giving them at least the chance to be judged by people who astheir peers canrelate totheir context, values and life decisions. For legal systems withoutatraditionofjury trials, orsystems withrecent memoryofabuseofthe  popularvoice  in the administration of justice, juries canappearasthe opposite. The randomness of their selection, the  black boxed  nature of their reasoning, and the fear ofmanipulation ofemotionalresponses orbiascreates fears very similartothose levelled against (some forms of) AI. 
Whether ornotaproposed future AI application for law raises ethical concerns andconforms withPrinciple 1,itwillalsodependontheseconceptionsand understandings of justice. An AI that profiles jurors to determine what arguments they are most likely to be susceptible to would be aserious ethical concernin those systems that imbue the anonymity and secrecy of the juror deliberation with normative qualities, butneutral (irrelevant) oreven beneficially increasing transparency in those systems that followed a different trajectory, inscribe different historical experiences and consider black box character of jury deliberations an ethical concern. 
Conversely, systems thatprofile judgesandtheirpreferences may beseenas particularly problematic in those continental systems that consider legal formalism also asanethical mandate, asthey potentially introduce extra-legal considerations into the way in which lawyers plead their cases. This may explain the recent French ban on such systems  but may be considered ethically beneficial from the perspective of those legal systems thatconsider jurisprudential realism notjustasadescriptive accountof the operation of law, but as anaspect of sound and competent legal advice that lawyers owe totheirclients.Similarly, even withinasystem thatvalues formal rationaldecision makingasembodiment of justice, profiling judges canbe beneficial if it were to show unjustifiable biases in theadministration of justice  not to exploit them for the benefit of an individual client, but to criticise and remedy them. 
There are thusnouniversal precepts for AI ethics in legal technologies. Instead, the principles of democracy and the rule of law require jurisdiction specific, context sensitive concretisation. However, the European Human Rights framework, in particular the right to a fair trial, the right of judicial review of administrative decisions, and the rightstoprivacy, freedom ofinformation andnon-discriminationprovide non negotiable boundaries for the margin of discretion that national jurisdictions have to concretise legal protection. This allows ustoformulate aprinciple of respect for the situated nature of rule of law: 
Principle of respect for the situated nature of the rule of law: 
The use of AI in law ought to take account of historically grown, socially and culturally embedded practices of adjudication, and divergent conceptions of justice within the contours of the European fundamental rights framework. 
Legal harmonisation canhave ofcoursebenefits. But the decision toalign legal practicesacross borders has tobe driven by democratically legitimated decisions, not by considerationofconvenience oftransnationally operating legaltechnology  companies promoting a single product across diverse legal markets and traditions. 
2.4. Principles of fair distribution of impact at individual and societal level 
LegalAIdoesnotdivideneatly intoethically soundapplications thathelpto address known shortcomingsontheonehand, and harmful applications that create unjustifiable risks on the other. Often, the long term social and ethical consequences of  legal technology  will be difficult to anticipate, aslegal professionals, governments and citizens in turn adjust to the new technology. 
To illustrate by way of example, at first sight, developing atool that gives reliable, fast and free legal advice to underserved groups, seems like a clear case of beneficial use asdescribed in the first paragraph. However, the availability of such tools could in turn be used to justify a further reduction in legal aid, which may affect also those cases that due to their complexity are lesswell suited for this technology and potentially threaten ahard- won achievement of the rule of law, the right to counsel. In the long run, this could lead to an overall reduced access to qualitative legal support for indigent claimants, for whom even inferior software products are deemed  good enough . 
Conversely, faster and cheaper accessto tools that facilitate litigation canonthe one hand give people the ability for redress that they had been excluded from previously. It canhowever also lead to amore litigious society, and far from reducing the burden on the legal system, create an increase in demand. 
At least one clear ethical duty applies to both developers and those who commission such tools. Where the development of a  legal technology  is driven mainly or exclusively by acostreduction rationale, there isaparticularly strong dutytocommunicate transparently, explicitly andhonestly any trade-offsintermsofquality, accuracy, comprehensiveness and similar quality criteria that apply to professional legal advice. 
Especially where efficiency threatens totrump quality, animpactassessment should mark outwho will benefit from costreduction and who will pay the price in 
terms of legal protection. Such an assessment should include an assessment of potential mitigation strategies that can move risks away fromalready vulnerable or disadvantaged groups.     
Taking risks cannotmeantaking risks with other people s rights and interests, particularly in the legal services industry. 
This can be further concretised by the 
Principleoffairdistributionofrisksandbenefits: 
 Legal technology  is shaped in contexts with significant power differentials. Responsible development and employment of AI reflects onthesestructural conditions, and protects against unfair redistributions of risks and benefits. More ambitiously, AI in law should 
aim to reduce existing power imbalances and redistribute risk to those best placed to mitigate it. 
The application of this principle will be highly contextdependent. However, if linked explicitly to the human rights framework, we canensure that the protection of the  awkward minorities , oroncomputational terms, the  difficult edge cases  are always foregrounded. 
Therationaleofthehumanrightsframework istoprotect individualsand minority groups from the dictatorship of the majority  and from exploitative business models of private enterprise alike. Justice is apublic good that cannot be traded asif it were part of the logic of economic markets. For AI tools that are basedonstatistical patternrecognition and analysis, this is aparticular challenge, due to the implications of mathematical optimisation that may obscure outliers and result in the tyranny of  stochastic majorities . Responsible AI in law willrequire amore fine-grained analysis toensure equalrespect and concernfor each individual citizen (Dworkin) atevery step of the design process. 
Where historically disadvantaged groups oratypical individuals are putatrisk, the introduction of a legal technology  should be reconsidered if effective safeguards and remedies cannot be provided. Community involvement is crucially important here, enablingthosewho willsuffer theconsequencesof legal technology  tovoice their concernand contribute their lived experiences  designingwith rather than merely for them. 
This leads usto the problem that sometimes, anAI tool canbe ethically neutral, andeven beneficial,when used by anindividual lawyer for the benefit of anindividual client, and yet causesignificant harm oncetheuseof the very samesystem becomes widespread oruniversal. In these cases, the AI fails to abide by the Kantian imperative (and also Jonas  extension of this principle to techno-social systems) that demands that unless an action can be universalised, it is not ethically sound. 
To illustrate this point, consider a system that onthe basis of past court decisions predicts the likely outcome of acourtcase.A lawyer using such asystem may useit to discourage a client from bringing litigation, or contest a case, where there is insufficient chancetoprevail. This canprotect the client fromunnecessary expenses orworse. It canalsorelieve the pressure onthe justice system thatcanthenusescarce resources for more meritorious cases, also benefiting others asaresult. Despite these apparent beneficialtraits, general useofthesamealgorithmcouldleadtohighly undesirable consequences for the justice system and the rule of law. Some fact constellations would become unlikely toreach the courts, simply because in the past, caseslike thesewere unsuccessful. As aresult, the legal system ossifies and becomes irresponsive tosocial change. 
Thismeanswe need to extend the principle of fair distribution of risks and benefitswithaprincipleoftransversalimpactassessment,that anticipates risks for the common good of practical and effective legal protection: 
The rule of law and the concept of legality transcend the binary relation between individual and state, or lawyer and client, and constitutes a common good. Evaluating ethical risks of employing AI in law must therefore consider long term detrimental 
impact on third parties and the cumulative effect on our ability to live lawfully. 
3. Principles for Responsible Development of AI in Law 
Legal technology does not just raise ethical issues at the point of application, they also raise issues in the way they are developed. Unlike the laws ofnature (including such things asrulesonhow to diagnose anillness,oneof the earliest expert systems), legal rules are not true orfalse, but authoritative ornot authoritative. Their authority, especially in democratic societies, in turn is closely linked to the way in which they are generated in arule-governed legislative process. Ethical issues canarisewhen program developers shortcut or usurp this legislative process. 
Inparticularinapplications thatserve as compliance tools and eitherstrictly enforce oratleastnudge individuals and businesses intolaw-conforming behaviour, ethical issues arise from themoment whensoftware developers translate legal rules into code.  Regulationthrough software architecture  (Lessig) asanewform of governance through legal technology raises its own set of ethical concerns that we will turn to now. 
3.1. Principle of procedural transparency 
As the expression  Code is Law  indicates, we canthink of someforms of  legal technology asanewform ofregulation. However, the process of translating natural language into computer code inevitably also involves, like every translation, aprocess ofinterpretation andchange of meaning. Ambiguous legal phrases may have tobe disambiguated, decisions have to be made onhow much of the original meaning needs tobeoperationalisedand how muchmustbeleftanalysed intheparameters,and choices will have to be made if implicit logical connections in the text ought to be made explicit in the code. The problem here is not somuch that the code may be wrong   this issue will be dealt with elsewhere  but that the programmers have to take on tasks that inthe past were reserved for the legislator (e.g. clarifying laws through statutory instrumentsorcodesofpractice) orthecourts(interpreting anddisambiguating legislation). 
This raises questions about the legitimacy of such law-making by private actors, the danger it poses tothe process of democratic control of legislative andexecutive powers, and the possibility for citizens to participate in adeliberative democracy. We therefore argue that legislation by way of code is inherently creating ethical risks and canbe incompatible with the core tenets of democracy and the rule of law. First, it can violatetheprocedural precepts ofgoodlaw-making thatensure publicdebateand accountability. Second, itcancreate newaccessbarrierstothe law. Special skills are now neededtounderstandwhat the law says, and also todeterminewhether agiven application is a true representation of the law. The use of arcane and complex language innatural language law making has been recognised by initiatives suchastheplain English campaign. The values of open administration of justice and equal accessto the law, which drove this development towards more accessible legal language, risk being undone through code-based compliance tools, if these do not also afford interrogation in natural language. 
Even thoughevery form of automated decision-making is in this sensecarrying high risks, we propose asetof principles that should apply whenever delegated legal powers are exercised by way of policy rules whose application may be automated. 
H.L.A.Hart, in The Concept of Law, distinguishes between primary rules of conduct and secondary rules that regulate how primary rules are created, enforced and given legitimacy.  Legaltechnology , especially intheform ofautomateddecisionand compliance systems, tends to incorporate primary rules only, which threatens to cut the nexusbetween laws andtheconditionsthatensure theirlegitimacy. Responsible development of  legal technology , by contrast, is aware of the wider constitutional and social rules that confer legitimacy to laws, respects the values enshrined in constitutional settlements, and abides by the procedural rules and safeguards that control ordinary law-making. 
We focus here in particular ontwo of Hart s rules, the Rule of Recognition and the Rule(s) of change. Hart describes this rule as tosay that agiven rule is valid is to recognize it aspassing all the tests provided by therule of recognition and soasarule of the system. We can simply say that the statement that a particular rule is valid means that it satisfies all the criteria provided by the rule of recognition.  
In democratic societies, this meansin particular that the law emanates ultimately from the legislature and elected officials, albeit in somecasesindirectly though powers that have inturnbe conferred according toconstitutional law. It canalsomeanthat procedures about public involvement were observed, and that the law maker canbe heldresponsible both through democratic processes and through judicial review by the courts.AIdevelopers are notsubjecttodemocraticandcourtcontrol, norare they recognised aslegitimate norm-givers. Where AI is used to automate the application of legal rules, there is therefore a heightened demand for transparency and accountability, toensure thatdevelopers donotby accidentusurpfunctionsthatonly  recognized authorities can exercise . Equally, the right of the public to participate in legislation and hold legislators to account must not be diminished, and legislators in turn must not be abletoavoid publicdiscourseandscrutiny by turningpoliticalquestionsinto technological design decisions. 
Just asthetraditionallegislative process creates anauditablepapertrailfrom committee discussions to plenary debates totheeventual drafting of the law thatcan be used in judicial review, soshould legislation through coding. This is the rationale behind the 
Principle of procedural transparency: 
Throughout the development process of  legal technology , decisions about design features that are functionally equivalent to interpretation, augmentation or limitation of the law ought to be documented, including a documentation of who made the decision and on what authority. This should happen in language accessible to all stakeholders, including civil society and their representatives, and not just specialists. 
3.2. Principle of respect for the legislative process 
Connected to this notionof transparency is the next principle that we propose as a key aspect of a new theory of prudential digital law-making by code. It aims to mirror the procedural safeguards, limitations ontheexercise of power, and the right of civil society to contribute to legislative processes that we find in traditional law making. 
Thismay meanatsomepointinthefuture thecreation ofanewsui generis system for legislative drafting, including thepossibilityby legislators to legislatein code , andsimilarformal rulesthatwould change theway inwhich modernstates generate laws. In the absence of such rules, a number of ethical constraints can however be identifiedthat flow from the Rule of Recognition and the principles of democratic accountability and the sovereignty of the demos. 
Principle of respect for the legislative process: 
Public sector organisations that commission legal technology must not use it as a way to prevent democratic rule-making, scrutiny and accountability, or limit established rights of the public to participate and be heard in the legislative process. They remain ultimately responsible that  legal technology  matches in form and function the laws it implements. 
This aims to prevent the often-observed tendencyby public bodies to change the nature ofaproblem from onethatrequires public debate to onethat is turned into a technical coding issue resolved behind closed doors by specialists. It also aims to avoid actingultra vires by delegating legislative power tocommercial software developers, and to ensure transparency and public scrutiny of the norm-making process. 
3.3. Principle of unfettered public participation 
The purpose of the previous principle was toprotect public participation rights in the legislative process. The underlying value is expressed directly in the 
Principle of unfettered public participation: 
The right of the public to contribute in the norm-setting process, and the right of 
communitiestobeheard inpublicsectorrule-making thataffect them,mustnotbe 
reduced or circumvented by a process of building legal technology and  legal by design  environments that would rule out disobedience. 
Participatory, community-led designprocesses willfrequently besuperiorto achieve responsible  legal technology  thatalignswithhumanrightsanddemocratic principles. In most public sector  legal technology  projects, it is an ethical demand. But itwillfrequently bedesirablealsoinmostotherAIinitiatives inthelegaldomain. Ethics for theresponsible development of  legal technology  cannot be reduced to aset of rules, in the way asoftware specification manual might. Rather, responsible  legal technology  is fundamentally asetof processes thatensures thevoices of all affected individuals and groups are heard and attentively listened to. 
3.3.1. Principle of transparent and adequate compensation 
Participatory design processes canberesource-intensive. Theycanalsoplace significant burdens on thecommunities and groups it tries to involve. This is exacerbated by the fact that the mostvulnerable communities are regularly also facing economic andotherbarrierstoefficientpoliticalengagementandcontribution.Anethical emphasis on community-led design practices and other forms of substantive involvement of affected parties should notfurther increase the burden onthem. To benotamere aspiration, but a reality, also requires the provision of adequate resources, compensation, andrecognition for thelabourofthesegroups.Onekey achievement for modern democracieswas the introduction of payment for parliamentarians. This provided the infrastructure thatwas needed sothat active and passive votes also aligned in practice. From this historical experience we get the   
Principle of transparent and adequate compensation: 
Development and employment of AI in law does not just passively listen to the voices of 
allindividuals andgroupsthatare affected bytheoperation of  legal technology , it 
actively seeks and involves these voices. Through their labour these communities add value to the eventual product, value and labour that has to be adequately compensated and acknowledged. 
3.3.2. Principle of diversity and representation 
Insomesituations, substantial community involvement willnotbepossibleor necessary, for instanceinvery low riskapplications. Even then,there shouldbe independentethicalscrutiny andadvicewhere possible.While having diverse and representative ethicaloversight isarequirement for allAIdevelopment, ittakes on particularimportancefor AIapplications inthedomainoflaw. Principles of representativeness, non-discrimination, inclusivityand substantial as well as procedural fairnessare alsocore requirements of the rule of law in democratic societies. This is expressed in the 
Principle of diversity and representativeness: 
The groups and individuals who through their labour and expertise inform the development of  legal technology , also and in particular through ethical advisory boards and other governance structures, should be representative of the society that the technology will 
serve and reflect its diversity. 
3.4. Principle of non-discrimination 
While the previous principles focused on the process of creating  legal technology , thequestionofdiversity, discriminationandbiastakes onparticular importance in legal contexts. Thatwe do not deal with this issue more prominently is due, first, to its importance for all AI applications and not just the domain of law, and it has therefore beenaddressed already intheprevious AI4People reports. Second,theprevious principles thatrequire transparent decision making, and principles that require that diverse voices are heard in the design process, should already minimise the danger of products that couldleadtobiasedordiscriminatorydecisionmakingby itsusers. However, due to its centrality for the rule of law, and also because there can be different forms of discrimination, it is restated here explicitly. It is connected with the rule of recognition inthatbiasedanddiscriminatorylegalpracticesthreaten thevery foundations of a legal system. 
Principle of non-discrimination: 
Biased and discriminatory practices are incompatible with the rule of law ideal and its promise of justice for all.  Legal technology  must demonstrably ensure through the choice of proper design methodologies (e.g. vetting of input data), choice of technology (algorithms that are interpretable and/or have been debiased), testing (both during design and after deployment), and an analysis of other forms of disparate impact on communities, informed by their lived experience that they do not unjustly discriminate against individuals and communities. 
This principle goes beyond mere mitigation against biased decision making, such as reliance on data that encodes social prejudices in, for example, the detection of crime or the imposition of penalsanctions. We include other forms of potentially discriminatory practices, such asfor instanceexclusion from digital legal information due toaspects of the technology that make it unusable for people with disabilities. Equally covered by the principle can be graphic design choices, for instance for the avatar of a legal chatbot, which currently more often than notreflect stereotypical assumptions about gender roles ( assistants  tend to have voices, images and animations that present female, while  legal expert AIs  suchasROSS present malevoices andappearanceetc), physical attractiveness or are chosen predominantly from major ethnic groups. This technological amplification ofexistingsocialbiasesisundesirable,even when usedintheprivate sector, but itbecomes anethical afront when used by public sector legal technology . Asadefault, anthropomorphising  legal technology  should be avoided or minimised to facilitate the exercise of critical judgement by the citizen who is interacting with them. 
Ultimately theprinciple ofsubstantive equalitybefore thelaw flows from the overarching human rights principle of human dignity. Of all the human rights ideas this is the most difficult to concretize, and aswe saw in the historical introduction oneof the most persistent concerns in  legal technology  that is driven by a cost rationale. 
3.5. Principle of temporal contestability 
Finally, we turn to asecond meta-rule identified by Hart, the Rule(s) of Change. Rules of change ensure that legal systems donotremain static, but react tochange. While there are different approaches across Europe, especially when itcomesto constitutional law, they all contain limits onhow much, and for how long,acurrent legislator canbind their successors. Rules of change describe the necessary process and the powers that are conferred to various actors to affect such a change. 
The danger with  legal technology  aspart of the infrastructure is that it canbe resistant to change, creating unwieldy legacy systems that are difficultorimpossible to change, modify orabandoneven when the democratic process orjudicialreview by courtsmake thisnecessary. Themostprominent example of this problem inrecent times has been blockchain technology and the degree of immutability of rules inscribed on the chain that they bring. 
This gives us the 
Principle of temporal contestability: 
All  legal technology  projects should anticipate that laws change in response to changing societal norms.  Legal technology  must not lock in or constrain future legislators, and must permit challenges and changes to current laws through the democratic process or through judicial review.  
 Designingfor change meansavoiding designdecisionsthatimpacton parliamentarysovereignty andremaining adaptive andflexibleatleasttothesame degree as traditional laws are. 
4. Principles for Responsible Employment of AI in Law 
The next set of principles looks at the point in the lifecycle of  legal technology  where the technology is rolled out and in use.Now, the problem of potentially wrong advice, decisions orrecommendations becomes relevant. The first group of principles looksattheuseof  legal technology  in public administration, the second focuses on usesby the court system. The principle of continuous empirical evaluation also applies to legal practitioners, and for them will be taken up and concretised in the Principle of Collective Responsibility. 
Although some principles are more relevant to particular fields of application, and are presented inthisthematicfashion, they are nonethelessvalid for allpractical deployments of AI in law. The public sector is responsible for itsuseof AI, and must ensure that this useremains justiciable. Practising lawyers are ethically accountable for thetoolsthat they adopt, andmustensure thatthey support accesstojustice.The judiciary must rememberthat they have arole in monitoring their own responsibility andaccountability for the adoption of AI in courts, and ensuring that the public and private sectors are aware of ethical expectations inthis regard. The boundaries between thecategories inwhat follows are porous, andtheethicaldutieslistedapply toall involved in the application of legal technology , butsomemay bemore pressing in specific contexts. 
4.1. Accountability: Principles for Responsible Legal AI in Public Administration 
Theprevious principles addressed ethicalconcernswith legal technology  that would have tobe considered even if, hypothetically, the technology works perfectly. Thenext set of principles deals by contrast with one of the most obvious concerns, the possibility that a given program comes to incorrect results and as a consequence causes harm, including but not limited to the denial of a right to the subject of a decision. 
4.1.1. Principle of continuous empirical validation 
One of the characteristicsthat sets law apart from other applications of AI is that what counts asa correct  answer canbe radically contested in legal settings. How do we know that the system performs correctly, at least onitsown terms? If acitizen has been harmed through anincorrect decision that was caused by the useof technology, how would they be able toknow? Assuming thatnosystem is perfect, when are the benefitssogreat thattheymake takingtheriskofoccasionalmistakes ethically acceptable? How finegrained should that analysis be? Can it be acceptabletousea system thatinmostcasesoutperforms ahumandecisionmaker, butsystematically makes more mistakes when dealing with the claims of a small subgroup? We will revisit these issues below, but we note as a general principle the 
Principle of continuous empirical validation: 
Ethical use of AI in law requires rigorous and realistic testing both pre- and post-
deployment, with robust methods toshare findings about problems between users, and 
users and regulatory agencies. 
The last part of this principle, the mandate to share findings, will also play arole in the Principle of Collective Responsibility that we will discuss below. We do note here though that this principle and itscorollaries have beensuccessfully usedin medical research anddevelopment for a long time, without imposing impossible burdens on the industryonceappropriatesystems are in place. In medical research, pre-registration of tests, and meta-analysis of shared results through e.g. the Cochranereview,are well established,asare post-deployment systems to report adverse effects of drugs. Similar mechanisms to share experience are needed at least for those data-driven AI applications in the justice system that carry higher risks.  Higher risk  applications include all those where AI systems directly assist decision makers such as judges. 
While the demands made of AI systems in law regarding their accuracy will vary between contexts(we canandshouldjudgelegalinformation retrieval systems differently from sentencing support systems, and bothfrom achatbotthatguides citizens through aprocess of form filling), benchmarks need to be created that allow thosewho useandcommission legal technology, and those adversely affected by it, to evaluate the reliability of  legal technologies  in termsofreal world reliability (rather than accuracy onthe data). For those applications that imply risks for fundamental rights and freedoms, this benchmarking ought to be carried out by independent bodies. 
4.1.2. Principle of contestability 
As noted above, in law, the question of what counts as the  correct answer  can be inturncontested. This meansthat it will notbe sufficient tocertify that the AI has somedesirable formal properties. Rather, decisionsmustbe explained in away that allows their contestation.What type of explanation orjustification do we owe tothe citizens who are affected by the use of an AI system? What other communicative duties are there  for instance towards a possible supervisory or appeals body, or especially in cases of trials and similar procedures to the public at large? 
In contemporary debate, these issues are often conceptualised asanaspect of the correctness of the decision-making, and are seenasatechnical challenge that canbe 
addressed through interpretable orexplainable AI.Ifthesubjectofanautomated decision,orauserof legal technology, cangetanexplanation of its decision, mistakes can be identified and, in theory, addressed and rectified. 
In the legal field, this idea is closely connected to the adversarial nature ofmany legal settings, especially when theyare alsoembeddedintoahierarchical system of judicialreview and appeals. Legal decision making, in many contexts, centres around adual notionof  contestability . Horizontal contestability meansthat in anadversarial setting, both sides deserve to be heard, to bring their arguments to the table, examine and challenge each other s position. Vertical contestability meansthat legal decisions can regularly be challenged and submitted to a higher authority. This can take the form of judicial review in administrative decisions,orappeals in trial situations. AI in law, especially inthecaseof black-boxed  machinelearningsystems, canpotentially underminethiscentralconception.To counterthis,we postulatetheprinciple of contestability as a concretisation of the principles to respect the internal integrity of the legal system and the rule of law ideal. 
Principle of contestability: 
The use of AI must not limit the right to contestation of a decision where the legal system provides such a right. Contestation requires typically that appropriate reasons for a decision are given. 
4.1.3.Principlesofjustificationandexplanation 
 Explainable AIinlaw  isoneaspectthatsupportstherighttocontestation.  Explainable AI  typically makes theway in which adecisionwas reached transparent. However, in the legal field we are notjust interested in how adecisionwas reached. Rather, we want a justificationthat meets certain standards and criteria. In epistemology, this is sometimes expressed asthe difference between the context of discovery and the contextof justification. We do neither expect nordesire that human judges explain a decision in terms of their personal background, socialisation or life history, as important asthesemay befor theway adecisionwas reached. Neither dowe demandan explanation in terms of neurological data from the judge s brain at the point of decision making. Rather, we demandajustification that meetscertain legal standards, e.g. by reference to the applicable law and governing cases. From this we get the 
Principleofadequatejustification: 
For applicationsof AIinlawtobeethicallydefensible itisnotsufficientthat an  
explanation of the decision is given. Rather, to be valid the explanation has to respect the internal logic of the legal system in question, and give appropriate reasons that match the duties to justify decisions that we currently impose on human decision makers such as judges or public administrators. 
However, we may allow insomecasesalsotocontestadecisionthatcanbe justified in this way, butwhere the procedure toreach the decision was irredeemably flawed. A decision by ajudgewho failed to recuse themselves even though they were related tooneofthepartieswould beatypicalexample. Incaseslike this, back engineering aformally valid justification of the outcome of the decision is not enough and cannot remedy the flaw in the decision-making process. Similarly, we should expect from AIs alsotobe explainable in the sensethatflawed reasoning processes canbe identified,andwhere appropriate remedied, even incaseswhere theoutcome(by chance) is correct.  
Principleofsufficientexplanation: 
AI in law must be explainable enough to allow adequate allocation of responsibility to enable sanctions and redress where there are errors or misconduct and to vindicate the 
duty toprovide remedies in those cases where AI applications may have caused unjustified 
harm. Where, exceptionally, such an explanation is not possible, comprehensive no-fault compensation or similar safeguards are necessary. 
The aim of this principle is preventing any  responsibility gap  and to ensure that parties harmed by an AI are not facing burdens of proof they cannot discharge.   
4.1.4.Principleofpracticalandeffectiveredress 
Neither explanations norjustifications in the above senseare sufficientontheir own to address concernsabout false decisions. Knowing that the decision of anAIwas wrong, and why it was wrong, do notin itself right the wrong that was committed. There isadangerthat transparency  isweaponised toshiftresponsibility from the software developer to customers, users or the people subjected to autonomous decision making. Especially in legal contexts and the differences in power, material and intellectual resources and social capital that often permeate them, knowing that aninjusticewas committed is often not sufficient if there are nopractical and effective meansto obtain redress. This leads to the 
Principleofpracticalandeffectiveredress: 
Any evaluation of the ethical soundness of a  legal technology  project needs to identify the remedies that will be provided to challenge decisions, to ensure that incorrect decisions 
can be detected, making sure that these remedies are adequate to provide redress and serve as a deterrent to negligence or misbehaviour, and that those harmed are given 
appropriate information and support to exercise their rights effectively. 
4.2. Justiciability: Principles for Responsible AI in the Judiciary 
A particularly sensitive useof AI in the justice sector is its useby courts and the judiciary. This is not just due to the high stakes for the citizens involved, but the social and symbolicvalue of  seeing justice being done . Thenextprinciples therefore adda number of principles that are specific to AI in the courtroom. 
4.2.1. Principle of public justice 
It would be amistake toreduce the issue of explainable AI to the detection and correction of mistakes. Even if, hypothetically, it could be formally proven thatagiven AI application in the domain of law was always 100%correct, there would still be a need to give reasons for its decision in many contexts. 
The trial in particular is aprocess of  public holding to account  (Duff and Duff). In giving public reasons for adecision,ajudge,orasimilar decision maker inapublic body, doesnotjustcommunicate thereasons for thedecisiontothepartydirectly affected, to enable them to contest the decisions. The aim of the trial is also to enable them to understand, and in some way accept, why justice demanded this outcome. 
Furthermore, by giving public reasons, trials make the control of the justice system acollective dutyandpossibilityfor allcitizens, particularly by making iteasierto detectinstances ofcorruption, bias, andprejudice. Trials alsopublicly reaffirm the values of thecommunity, and in this way also allow criticism and evolution of the law in response to changing social attitudes.  Government with the consent of the governed  requires involving the public in both, scrutiny of the application of legal rules in a given case,andscrutiny oftheacceptabilityofthelegalrulesgiven theresults thattheir application leads to. The idea of the public trial is thus not only central to the rule of law, it also intertwines democratic participation and citizenship. 
 Legaltechnology  canunderminethisprinciple inanumberofways. It can weaken the obligation of state actors to explain and justify their decisions, for instance when they use software owned and controlled by a private sector enterprise that shields behind tradesecret orcopyright law. The above principle was meant to address this. It canalsoexclude citizensfrom theirrole asobservers of the justice system, by using communication tools that are inaccessible for them. AI can, in these situations, actas anassistive technology, thatsupportscitizenswiththeirparticipation,through automated translation for instance, or as assistive technology for citizens with visual or hearing impairments. Conversely, itcanlimitaccessto and publicity of the trial  for instancewhen for the purpose of evidence evaluation, judges orjurors  see  acrime scenere-enactment intheirvirtualreality helmets,anexperiencethatcannotbe replicated for observers (Cieslak). 
Finally, itcancreate economic obstacles of accessing legal information through business models that rely onintellectual property rights over these legal documents. From this we derive the 
Principle of Public Justice: 
Public justice, notably the publicness of the trial, is a cornerstone of the fair trial. The use of  legal technology  is ethically harmful if it increases the obstacles for the public to get involved with, and learn about, the justice system. Responsible legal technology, by contrast, aims to lower these obstacles, while preserving legitimate privacy interests of those involved in the proceedings. 
4.2.2. Principle of equal access to justice 
As noted above, one important aspect of the idea of public justice is access to legal materials. One reason why Lon Fuller s  King Rex  in his seminal  The Morality of Law  failed to build alegal system was by preventing accessof citizens to the legal texts that ruled them. In our context, these can be primary sources generated by the public sector suchasstatutesandregulations, courtdecisions, policy documents, ordocuments generated by parties such assubmissions, pleadings and briefs. The idea that asavery minimum, citizens need tobeabletodeterminewhat the law is, i.e. have accessto primary legal sources, remains patchy, unaffordable for many and subject to numerous obstaclesinpractice.Technology hassignificant potential tomitigatethisproblem. Many legal systems have madesubstantialimprovements ofpublishingstatutesand also, in varying degrees, court decisions online. Intelligent and user-centric Information Retrieval systems canhelpeven those with less experience to find material relevant to them. Automated translation tools cangive accessto the law torecent immigrants and speakers of minority languages.  
Technology can, however, alsocreate newbarriers, and increase inequalities for accesstojustice. These canbe caused by design features of the technologies, such as accessibility problems caused by inadequate consideration of the needs of userswitha wide range of disabilities,digital skills, and unequal accessto computers. They can also becreated through acombination of business models and IP law, when walled gardens are created for legalinformation. Theabove publicity principle cantherefore be sharpened to 
Principle of equal access to the law: 
 Legal technology  should strive to maximise access to legal sources for all. It should in 
particular be used to widen access to groups that have historically faced significant access 
barriers. It must not lead to new obstacles, including technological and economical obstacles. Design for accessibility that considers a wide range of disabling factors, in close 
consultationwiththeaffected communities,iscentral for ethical legal technology . 
Alternative modes of access have to be preserved for those who cannot be accommodated this way. 
Becauseaccessto the law is mediated by language, this principle also speaks to the principle of respect for local traditions thatwe introduced earlier. Law and language are intimately connected, as are languages and the way inwhich cultures express themselves. Inthe Europeancontext, this has beenrecognised inthe EuropeanCharter for Regional orMinority Languages, Article 22 of the Charter of Fundamental Rights that prohibits discriminationongrounds of language, and institutions suchasthe EuropeanLanguage Equality Network. Just as there is a danger that  legal technology  could impose conceptions and solutions thatwork only for some(typically larger) legal systems onothers, there is aconcernthat speakers of majority languages are betterserved infields such asnatural language recognition. At the very least usersofrecognised minority languages must not be disadvantaged by the introduction of  legal technology . 
The ability to speak one s own languagewhen interacting with the state, the legal system and public authorities, and in turnget services in one s own language, is an important aspect of full democratic participation for speakers ofrecognised minority andregional languages. For ourpurposes, it connects the question of  legal technology  more broadly tothat of participation in the democratic process, the relation between citizen and legislature and the ideal of full and active citizenship. 
4.3. Responsibility: Principles of Responsible AI in Legal Practice 
Apart from their use in the public sector, the legal profession is the main user of  legal technology . The unique characteristics of the legal profession constitute another factor that sets AI in law apart from many other domains. This needs to be accounted for in a suitable ethics framework. 
Inmost jurisdictions, practicing lawyers are members of aregulated profession, which necessarily also meanstheyare subjecttothe ethical and professional rules of 
their respective regulatory body, which is instituted by law. These rules in turn can be enforced with sanctions, and must be qualified as legal norms (delegated in the relevant Act of Parliament). Someof the principles in this document could become part of these rules and in that way obtain legal status. This highlights the special nature of professional ethics in the domain of law as  ethics rules with teeth  that blur the line between ethical rulesandenforced regulatory standards. Astheregulatory bodiesandprofessional societies will have toplay acriticalrole in ensuring beneficial useof AI, someof the principles directly address the regulatory bodies, asthey will have to play a central role in the development, use and monitoring of AI applications in law. 
4.3.1. Principle of equivalent application of professional ethics rules 
Thisprinciple reflects notjustthecentralityoflaw andtheruleoflaw for democratic societies, andwith that the particularly severe risks that canbecreated. It alsoreflects the special function that lawyers play in society asaregulated profession, and the dual private/public status they have in many jurisdictions as both representatives of their parties but also officers of the court. This function has always been understood asgeneratingasetofstandards of professional ethics  andconcomitantobligations that go above and beyond those of ordinary citizens orunregulated businesses. As an example, lawyers have aduty of confidentialityvis- -vis their clients that goes above and beyond those owed by all businesses under data protection law. A particular ethical concern with the use of AI in the legal domain is that while law is a regulated profession, computing, in the main, is not, oratleastnotcurrently. Thiscreates dangers for the usersof  legal technology  when these are notoperated by regulated lawyers, but are created andprovided by commercial entitiesthatcurrently donotprovide similar levels of protection and redress. From this we get the 
Principle of equivalent application of professional ethics rules: 
Where  legal technology  takes on roles traditionally carried out by lawyers as members 
of aregulatedprofession, theprotection for theaffected citizenshouldnotallowany 
circumvention of the core principles that govern professional conduct of practicing lawyers, notably (but not limited to) the norms set out by the International Bar 
Association concerning: independence, honesty, integrity and fairness, conflicts of interest, confidentiality and professional secrecy, clients  interests, lawyers  undertaking, clients  
freedom, property of clients and third parties, competence and fees. 
This will also include, for instance,availability of indemnity insurance and access to independent complaints mechanisms of the type often provided by the law societies and similar professional organisations. Any danger to misrepresent the interaction with a legal technology  asprivileged legal advice, such astheuseof legal symbolism, an avatar with the insignia of alawyer etc, must be strictly avoided. This principle should not be misunderstood as buttressing the monopoly of lawyers for legal service delivery. It does however ask for equivalence in protectionfor people who receive legal advice through a machine, whoever the owner-operator of the program may be.  
4.3.2. Principle of ultimate responsibility 
Regulated professions are distinguishedby theautonomy thattheirmembers enjoy:  professionals are autonomous insofar astheycanmake independent judgments abouttheirwork  (Bayles). Thismeans,inparticular, thefreedom toexercise their professional judgement.However, thisautonomy isnotunlimitedorself-serving. Professional autonomy canonly be exercised in anethically sound way if members of the profession 'subjecttheir activities and decisions toacriticalevaluation by other members of the profession. (Hoogland and Henk). From this understanding of law as a regulated profession, we can derive two further principles. 
Principle of ultimate responsibility: 
The choice to use or not to use a given AI as a tool is an exercise of professional judgement. As members of a profession, lawyers have particular ethical responsibility for the tools they choose for the discharge of their responsibilities. 
This includes adutytocarryoutsufficient training, and remain up todate,to understand the potential, limitations and risks associated with the AI tools they use. Where lawyers rely onthird party certification, they have adutytounderstandthe limitations of these parties, including the independence of the certification body. 
4.3.3. Principle of collective responsibility 
The previous principle does notentail that developers of AI software are freed from their ethical duties. Rather, both their obligations and that of the lawyers using an AI exist in parallel. Nor does it meanthat lawyers need to acquire anunderstanding of AI that equals that of the experts who build the system. It does meanhowever that the choicefor oragainstanAItoolhasethicalandprofessional salienceandcreates a responsibility that cannotsimply be shifted to the developers orsuppliers. In practical terms,itmeanslawyers usingAIneedtohave appropriate andcurrent knowledge, training and understanding of the system that they are using, demonstrated e.g. through continuous professional development activities. For law societies and similar regulatory bodies, this meansthat competency in AI has tobecomeanelement of professional competencyrequirements. Part of this competency has also to be anunderstanding of one s limitations inknowledge, whenit is safe to use technology despite these limitations, and how and where to get the right type of support. Law societiesand other such bodies whose independence from software manufacturers is guaranteed by statute are alsowell placed todevelop systems of certificationthat canhelp their members indeciding on appropriate and safe tools that advance instead of diminish law and the rule of law. 
The central role of professional bodies is furthermore reflected in the 
Principle of collective responsibility: 
The safe and ethically sound use of AI in the legal domain is a collective responsibility of lawyers and their professional bodies. This means a duty to share experience especially of problems and errors encountered. 
This principle flows from the requirement (noted above) that members of a profession should subject their activities and decisions to a critical evaluationby other members of the profession . Post-market reporting of problems with drugs inthe medical field could be a model for this type of sharing, which could be administered by the professional bodies. Theseare also best placed todetermine the right balance betweenthetransparency and openness of this process, and the necessary confidentiality that enables frank disclosure. 
4.3.4. Principle of technological neutrality 
Being member of a regulated professiondoes not only create additional professional duties towards one s clients. The European UnionDirective onRecognitionof Professional Qualifications (2005/36/EC)defines professions as thosepracticedonthebasisof relevant professional qualifications inapersonal, responsible and professionally independent capacityby thoseproviding intellectual and conceptual servicesinthe interest of the client and the public . Crucial here is the last part that establishes a duty to the public atlarge. Lawyers asofficers of the court, and professional organisations as state-recognised regulators, have duties that go beyond the relationbetweenthemand their clients. They have responsibility towards the integrity of the legal system as a whole. Ethical problems canariseinparticularwhere there ispotential conflict between the ethical and professional duties owed to a client, and those owed to the general public. 
From this we derive the 
Principle of technological neutrality: 
If lawyers orgovernmentofficials employ  legal technology  to provide alegal service or to make legally relevant decisions, their duties as professional lawyers remain unqualified, 
and do not shift to the system developers. The use of technologies does not alter what those subject to law may expect of legal professionals. 
This principle does not say that technology is ethically neutral. In ourview,  legal technology never is.Rather, itmirrors theprinciple oftechnological neutrality in Internet law, that states that what is illegal offline remains illegal online. Together with thePrinciple ofrespect for thedemocraticprocess andthePrinciple ofequivalent application of professional ethics rules introduced above, it ensures that the useof AI in law maintains consistently at least the same standards and protections that we expect from human decision makers. In addition, it acknowledges that  legal technology  may require more and different legal protection compared tothat prevailing in the offline world. This implies that technological neutrality may require compensatorymeasures to ensure equivalent protection. 
5. Concluding remarks 
Most of the AI for People s  global AI frameworks  have beenstructured onthe seven key requirements for trustworthy AI, proposed by the High-Level Expert Group onArtificial Intelligence (HLEGAI): (1) human agency and oversight, (2) technical robustness and safety, (3) privacy and data governance, (4) transparency, (5) diversity, non-discrimination and fairness, (6) environmental and societal well-being and (7) accountability. Theframework for thedevelopment anduseofAIinlaw hasnot followed thisleadinitsstructure, toaccountfor thediversity ofthelegaldomain. Nonetheless, each of the Principles ultimately speak to this primary framework. They emphasise theprofessional responsibility oflawyers andtheirregulatory body (HLEAGAI 1), propose a system of evaluation and post-deployment review (HLEAGAI 2), emphasise the specific legal demands onexplainability (HLEAGAI 4), foreground therole of law in promoting avision of equal citizenship (HLEAGAI 5), discuss the need for arobust system of  making good  in caseof harm (HLEAGAI 7). Together, they protect the Rule of Law as cornerstone of a democratic society, and thus ultimately allserve HLEAGAI 7. Because of this central connection between afunctioning justice system and the democratic ideal, the principles do not just address anarrowly defined  legal services industry  but rather target legal practice in all its forms (law firms, the judiciary and the office of the public prosecutor, the legislature and public administration), software developers and digital publishers in the legal domain, and also those subject toajurisdictionandthus entitledtolegal protection againsttheviolationoftheir human rights. 
Thereason for thisresistance to limit the ethical principles for legal technology toeither developers orcommercial legal practice should be clear: the introduction of AI into the practice of law cannotbe left to the legal services industry alone, and the underlying assumptions that legal protection is acommodifiable service whose price should depend on negotiations in a competitive economic market is a category mistake. Fair, transparent and effective economic markets themselves depend on the legal systems that institute, sustain and develop their constitutive constraints (such ascontract and property, but also including public law obligations and the institution of an independent judiciary capable of deciding and enforcing such constraints). Law therefore defines the contoursandinnerworkings ofeconomicmarkets andshouldnotbeframedasa commodity itself, dependent on the whims of a market meant to decide its fate. 
This, it should be made clear, applies toboth the decision touseordevelop a suggested technology, andthe decision not to use or develop a suggested legal technology. Justice systems across theglobe are at breaking point. Years of underfunding after the financial crisis of 2008, and araisedawareness of systematic and historical injustices, have resulted in anerosion of public trust.Appropriate technologies have significant potentialtoincrease accesstojustice,reduce economic, educational, geographic and cultural barriers toaccesslegal advice. They canhelp monitoring and evaluating the performance of key stakeholders, enable new forms of public participation in the justice system, anddemocratiseaccesstolegalsources asanenablerofcivicparticipation. Both of the ethical obligations to use/develop and not to use/develop a  legal technology , asappropriate for the particular contextinwhich alawyer finds themselves, require reflection onthe underlying values that the rule of law embodies,which is what this report ais to facilitate. 
Moreover, whereas thelaw isinformed by aspecificsetofethicalprinciples, notably justice, legal certainty and purposiveness, it differs from ethics in not depending on the ethical inclinationsof whoever has the power to decide (the sovereign). The law is firmly grounded in theability to unilaterally impose its will upon those subject to its jurisdiction,which makes theideaofdependingontheethicalintentofthosewho serve thelaw ratherhazardous. Theparadox oftheruleoflaw meansthatlegal protection against the state in the end depends onthe police force of that samestate, and that this can be effective and meaningful if checks and balances have been instituted. Without countervailing powers in the heart of constitutional democracies, there isno ruleoflaw butruleby law by persons (the reign ofunconstrainedsovereignty). A global AI framework should commit to upholding the rule of law and to preventing the development and useof AI for amore effective rule by law by persons (usually called a dictatorial regime). 
Countervailing powers and checks and balances, however, require careful crafting. They demand aproper understanding of the integrity of the law, which entails both its consistency and its adaptiveness asintegrated in the concept of legal certainty. This necessitates keen attention to legal normsasproviding foreseeability, accessibility and proper safeguards thatprotect against rigid orunreasonable application. Law isan argumentative practicethatnotonly decidesconflictsbasedonbinding, publicly available rules and principles but also enables anadversarial debate over thecorrect interpretation of such legal norms.The advent of AI may catertodreams ofaself-driving law that is both unambiguous and personalised. The task of the lawyer isto resist such fundamental misunderstandings about the nature of law and the limits of technology. The lawyer s task is towork towards anintegration of AI that supports checks and balances andmaintains the rule of law rather than making efficiency the holy grail of legal services. 
That being said the guidelines offered above do interact with the key requirements summedupbelow. The integrity of the law thatistypifiedby theprinciples of purposiveness, respect for the rule of law and a fair distribution of impact, imply acuity withregard tothe principle of technical robustness (if a legal technology  does not operate as claimed it cannot not serve the law s purpose), transparency and accountability (core torespect for the rule of law), and the principle of diversity, non-discrimination and fairness (which stipulates afair distribution of impact). As to development of AI in law,we have assertedtheprinciples ofprocedural transparency, respect for the legislative process, unfettered publicparticipation, transparent andadequate compensation,diversity andrepresentation, non-discriminationandtemporal contestability. Theseconnectwithhumanagency andoversight (which assumes procedural transparency to be meaningful, aswell asrespect for the legislative process, unfettered public participation, diversity and representation to be truly inclusive asto human agency, and temporal contestability to ensure effective oversight), with technical robustness (which requires procedural transparency to ensure mathematical verification and empirical validation aswell asdiversity and representation to prevent training on datasetscontaining distributions that are theresult of unfair bias), with privacy and data governance (as theyrequire procedural transparency todetect infringements of privacy; respect for the legislative process to prevent government from beingreplaced by governance and non-discrimination and temporal contestability toassertthatnot justany datagovernance regime is fit for purpose in aconstitutional democracy), with transparency (both inaprocedural senseandinthesubstantive senseofenabling adequatecompensation), andwithdiversity, non-discrimination andfairness(as explicitly addressed under diversity and representation aswell asnon-discrimination). Asconstitutionaldemocraciesneedtocopewithenvironmental andsocietalwell being, developers of  legal technologies  should ensure that their effectiveness does not depend on disproportional impact on the environment (which aligns with the principle of respect for the legislative process that should not be hijacked by data- or code-driven approaches that endanger the sustainable development goals (SDGs) asadvocated by the UN). Obviously the principle of accountability only makes sensein the context of ajurisdictioncapable of holding those who violateits legal normsto account, whether big players, government or individual citizens. As to the employment of AI in law we have detectedeleven principles that are constitutive of legal practice in a constitutional democracy, grouped under theheadingsofaccountability (focusing ontheuseof responsible AI in public administration), justiciability (focusing on the use of responsible AI withproper judicial oversight) and responsibility (focusing on the use of responsible AI in legal practice). We refer thereader tothecontentof the relevant chapter for a detailed analysis of the relevant principles, such as those of justification and explanation, practical andeffective redress, equal accessto justice and equivalent application of the rulesofprofessional ethics. Many oftheseprinciples canbecorrelated oneway or another withthe HLEGAI global AI guidelines, but again, they are notmerely ethical principles but mostly binding legal precepts and their granularity provides for concrete legal protection rather than ethical guidance. 
Mapping the key performance indicators oftheHLEGAI againstourown framework demonstratesmany cross-references (and many more canbedetected). This does not imply, however, that we could have done equally well by framing the use of AI in law basedonthose principles. We would have startedonthewrong footing by assuming that general principles of the development, deployment and useof AI are fit for purposeregarding thearchitecture of constitutional democracies. Instead, we have startedby laying thefoundations (integrityoflaw), subsequently addressing the relevant challenges for developers, followed by stipulations for the actual employment of AI in legal practice. This should pinpoint the road forward, refusing to accept  legal technology by default(technological solutionism), while takingapragmaticand principled approach to the integration of  legal technology  in legal practice (opting for responsible AI). 
Summary of Principles 
Foundational Principles for Responsible AI in law 
1.Respect for the integrity of law and the rule of law 
Any use of AI must respect the integrity of the legal system, the values inscribed therein, 
and adhere to practical and effective respect for the rule of law. 
2.Principle of purposiveness 
Any assessment of the ethical valence of a proposed legal technology should be explicitly upfront about 
(1)
 what problem it supposedly solves, 

(2)
 what problems it will not solve 

(3)
 what problems it may create.  


3. Principle of respect for (the situated nature) of the rule of law 
The use of AI in law ought to take account of historically grown, socially and culturally embedded practices of adjudication, and divergent conceptions of justice within the contours of the European fundamental rights framework. 
4. Principles of fair distribution of impact at individual and societal level 
 Legal technology  is shaped in contexts with significantpower differentials. Responsible 
development and use of AI takes account of these structural conditions, and protects 
against unfair redistributions of risks and benefits. More ambitiously, the use of AI in 
law should aim to reduce existing power imbalances and redistribute risk to those best placed to mitigate it. 
5. Principle of transversal impact assessment 
The rule of law and the concept of legality transcend the binary relation between individual and state, or lawyer and client, and constitutes a common good. Evaluating ethical risks of using AI in law must therefore consider long term detrimental impact 
on third parties and the cumulative effect on our ability to live lawfully. 
Principles for Responsible Development of AI in Law 
6. Principle of procedural transparency 
Throughout the development cycle of  legal technology , design decisions that are functionally equivalent to interpretation, augmentation or limitation of the law ought to be documented, including a documentation of who made the decision and on what authority. This should happen in language accessible to all stakeholders, including civil society and their representatives, and not just specialists. 
7. Principle of respect for the democratic process 
Public sector organisations that commission the development of legal technology must not use it as a way to prevent democratic scrutiny and accountability, or limit established rights of the public to participate and be heard in the legislative process. They remain ultimately responsible that  legal technology  matches in form and function the laws it implements. 
8. Principles of unfettered public participation 
The right of the public to contribute in the norm-setting process, and the right of 
communitiestobe heard in public sectorrule-making that affect them,mustnotbe 
reduced or circumvented by a process of building legal technology and  legal by design  environments that would rule out disobedience. 
9. Principle of transparent and adequate compensation 
Development and use of AI in law does not just passively listen to the voices of all 
individuals and groups that are affected by the operationof  legal technology . Developers 
should actively seeks and involves these voices. Through their labour these communities add value to the eventual product, value and labour that has to be adequately compensated and acknowledged. 
10. Principle of diversity and representativeness 
The groups and individuals who through their labour and expertise inform the development of  legal technology , also and in particular through ethical advisory boards and other governance structures, should be representative of the society that the 
technology will serve and reflect its diversity. 
11. Principle of non-discrimination 
Biased and discriminatory practices are incompatible with the rule of law ideal and its promise of justice for all. Developers and operators of  legal technology  must demonstrably ensure through the choice of proper design methodologies (e.g. vetting of input data etc), choice of technology (algorithms that are interpretable and/or have been debiased), testing (both during design and after deployment), and an analysis of other forms of disparate impact on communities, informed by their lived experience that they do not unjustly discriminate against individuals and communities. 
12. Principle of temporal contestability 
All  legal technology  projects should anticipate that laws change in response to changing societal norms. The deployment of  legal technology  must not lock in or constrain future legislators, and must permit challenges and changes to current laws through the democratic process or through judicial review. 
Principles for Responsible Use of AI in Law Accountability: Principles for Responsible Development and useof AI in the Public Sector 
13. Principle of continuous empirical validation 
Ethical use of AI in law requires rigorous and realistic testing both pre- and post 
deployment, with robust methods to share findings about problems between users, and 
users and regulatory agencies. 
14. Principle of contestability 
The use of AI must not limit the right to contestation of a decision where the legal system provides such a right. Contestation requires typically that appropriate reasons for a decision are given. 
Principles of justification and explanation 
15.Principleofadequatejustification 
For applicationsof AI in law tobe ethically defensible it is notsufficient that  an  
explanation of the decision is given. Rather, to be valid the explanation has to respect the internal logic of the legal system in question, and give appropriate reasons that match the duties to justify decisions that we currently impose on human decision makers such as judges or public administrators. 
16.Principleofsufficientexplanation 
AI in law must be explainable enough to allow adequate allocation of responsibility and of the duty to provide remedies in those cases where AI applications may have 
caused unjustified harm. Where, exceptionally, such anexplanation is notpossible, 
comprehensive no-fault compensation or similar safeguards are necessary. 
17.Principleofpracticalandeffectiveredress 
Any evaluation of the ethical soundness of a  legal technology  project needs to identify the remedies that will be provided to challenge decisions, to ensure that incorrect decisions can be detected, making sure that these remedies are adequate, and that those harmed are given appropriate information and support to exercise their rights 
effectively. 
Justiciability: Principles for Responsible Use of AI by the Judiciary 
18. Principle of public justice 
Public justice, notably the publicness of the trial, is a cornerstone of the fair trial. The use of  legal technology  is ethically harmful if it increases the obstacles for the public to get involved with, and learn about, the justice system. Responsible use of legal technology, by contrast, aims to lower these obstacles, while preserving legitimate privacy interests of those involved in the proceedings. 
19. Principle of equal access to justice 
The use of  Legal technology  should strive to maximize access to legal sources for all. It should in particular be used to widen access to groups that have historically faced 
significantaccessbarriers. It must not leadtonewobstacles, including technological 
and economical obstacles. Design for accessibility that considers a wide range of 
disabling factors, in close consultation with the affected communities, is central for 
ethical  legal technology . Alternative modes of access have to be preserved for those who cannot be accommodated this way. 
Responsibility: Principles of Responsible use of AI in Legal Practice 
20. Principle of equivalent application of professional ethics rules 
Where  legal technology  carries out operations traditionally carried out by lawyers 
as members of a regulated profession, the protection for the affected citizen should not 
allow any circumvention of the core principles that govern professional conduct of practicing lawyers, notably (but not limited to) the norms set out by the International 
Bar Association concerning: independence, honesty, integrity and fairness, conflicts of interest, confidentiality and professional secrecy, clients interests, lawyers  undertaking, 
clients  freedom, property of clients and third parties, competence and fees. 
21. Principle of ultimate responsibility 
The choice to use or not to use a given AI as a tool is an exercise of professional judgement. As members of a profession, lawyers have particular ethical responsibility for the tools they chose for the discharge of their responsibilities. 
22. Principle of collective responsibility 
The safe and ethically sound use of AI in the legal domain is a collective responsibility of lawyers and their professional bodies. This means a duty to share experience especially of problems and errors encountered. 
23. Principle of technological neutrality 
If lawyers orgovernmentofficials employ  legal technology  to provide alegal service 
or to make legally relevant decisions, their duties as professional lawyers remain 
unqualified, and do notshifttothe system developers. The useof technologies does 
not alter what clients or public may expect of legal professionals. 
Bibliography
The references are a curated listing of the most relevant state of the art literature onthe ethical dimensions of  legal technologies , with no claim to being comprehensive, along with some other works referred to in the report. 
Allhutter D and others,  Algorithmic Profiling of Job Seekers in Austria: How Austerity Politics Are 

Made Effective , (2020) 3:5 Frontiers in Big Data Ashley, KD, Artificial Intelligence and Legal Analytics: New Tools for Law Practice in the Digital Age (Cambridge University Press 2017) 
Baker JJ, 'Beyond the Information Age: The Duty of Technology Competence in the Algorithmic Society' 

(2017) 69 South Carolina Law Review 557 Bankowski Z, White I, and Hahn U (eds), Informatics and the Foundations of Legal Reasoning (Springer 2013) 
Barocas, S and Selbst AD,  Big Data s Disparate Impact  (2016) 104 California Law Review 671 
Bayles MD, Professional Ethics (Wadsworth 1981) 
Bench-Capon T and others,  A History of AI and Law in 50 Papers: 25 Years of the International 
Conference on AI and Law  (2012) 20 Artificial Intelligence and Law 215 
Brownsword R,  Technological Management and the Rule of Law  (2016) 8 Law, Innovation and Technology 100 Burk DL,  Algorithmic Legal Metrics  (2021) 96 Notre Dame Law Review 1147 Campbell RW, 'Artificial Intelligence in the Courtroom: The Delivery of Justice in the Age of Machine 

Learning' (2020) 18 Colorado Technology Law Journal 323 
Chishti S and others, The Legaltech Book: The Legal Technology Handbook for Investors, Entrepreneurs and Fintech Visionaries (Wiley 2020) Cieslak M,  Virtual reality to aid Auschwitz war trials of concentration camp guards  (BBC News, 20 
November 2016) <https://www.bbc.co.uk/news/technology-38026007> accessed 22 February 2021 
Citron DK, 'Technological Due Process' (2008) 85 Washington University Law Review 1249 
Compagnucci, MC and others, Legal Tech and the New Sharing Economy (Springer 2019) 
Custis T and others, Westlaw Edge AI Features Demo: KeyCite Overruling Risk, Litigation Analytics, 
and WestSearch Plus , in Floris Bex and others, Proceedings of the Seventeenth International 
Conference onArtificial Intelligence and Law, ICAIL  19 (Association for Computing Machinery 2019) 
Deakin S and Markou C (eds), Is Law Computable?: Critical Perspectives on Law and Artificial 
Intelligence (Hart Publishing 2020) 
Dignum V, Responsible Artificial Intelligence: How to Develop and Use AI in a Responsible Way 
(Springer 2019) Diver L, Digisprudence: The Design of Legitimate Code (2021) 13 Law, Innovation and Technology forthcoming 
Dubois C, How Do Lawyers Engineer and Develop LegalTech Projects? A Story of Opportunities, 
Platforms, Creative Rationalities, and Strategies , (2020) 2 Law, Technology and Humans Duff A, and Duff RA, Punishment, Communication, and Community (Oxford University Press 2001) Dworkin R, Taking Rights Seriously (A&C Black 2013) Fuller L, The Morality of Law (Yale University Press 1969) 
Greenleaf G, Chung P and Mowbray A,  Building Datalex Decision Support Systems: A Tutorial on 
Rule-Based Reasoning in Law  (2017) UNSW Law Research Paper No. 17-68 

Hartung M, Bues MM and Halbleib G (eds), Legal Tech: A Practitioner s Guide (Hart/Nomos 2018) Hildebrandt M and Tielemans L,  Data protection by design and technology neutral law  (2013) 29 Computer Law & Security Review 509 
Hildebrandt M,  Data-Driven Prediction of Judgment. Law s New Mode of Existence?  Forthcoming in Collected Courses of the Academy of European Law (Oxford University Press) 
Hart HLA, The Concept of Law (Oxford University Press 1961) Hoffmann-Riem W,  Legal Technology/Computational Law  (2021) 1 Journal of Cross-Disciplinary Research in Computational Law 
Hoogland J and Henk J,  Professional Autonomy and the Normative Structure of Medical Practice  
(2000) 21 Theoretical Medicine and Bioethics 457 Jonas H, The Imperative of Responsibility: In Search of an Ethics for the Technological Age (University of Chicago Press 1985) 
Kennedy R, 'E-Regulation and the Rule of Law: Smart Government, Institutional Information 
Infrastructures, and Fundamental Values' (2016) 21 Information Polity 77 
Law Society, Lawtech: A Comparative Analysis of Legal Technology in the UK and in Other 
Jurisdictions (The Law Society 2019) 
Law Society, Technology, Access to Justice and the Rule of Law: Is Technology the Key to Unlocking 

Access to Justice Innovation? (The Law Society 2019) Livermore MA and Rockmore DN (eds), Law as Data: Computation, Text, & the Future of Legal Analysis (Santa Fe Press 2019) 
Mohun J and Roberts A,  Cracking the Code: Rulemaking for Humans and Machines  (OECD 2020) 
Morison J and Harkens A, 'Re-Engineering Justice? Robot Judges, Computerised Courts and (Semi) Automated Legal Decision-Making' (2019) 39 Legal Studies 618 Pasquale F, New Laws of Robotics: Defending Human Expertise in the Age of AI (Harvard University 
Press 2020) Pistor K, The Code of Capital: How the Law Creates Wealth and Inequality (Princeton University Press 
2019) 
Radbruch G,  Legal Philosophy , in Kurt Wilk (tr and ed), The Legal Philosophies of Lask, Radbruch, and Dabin (Harvard University Press 2014) Spiekermann-Hoff S, Digitale Ethik: Ein Wertesystem f r das 21. Jahrhundert (Droemer 2019) Surden H, 'Machine Learning and Law' (2014) 89 Washington Law Review 87 Susskind R, The Future of Law (Oxford University Press 1996) Tomlinson J,  Justice in Automated Administration , (2020) 40 Oxford Journal of Legal Studies 708 
Waldron J,  The Rule of Law and the Importance of Procedure , (2011) 50 Nomos 3 Whalen R, Computational Legal Studies: The Promise and Challenge of Data-Driven Research (Edward Elgar 2020) 
Wyner A and Casini G (eds), Legal Knowledge and Information Systems JURIX 2017: The Thirtieth Annual Conference (IOS Press 2017) 
Yeung K,  Algorithmic Regulation: A Critical Interrogation  (2018) 12 Regulation & Governance 505 
Zalnieriute M and Bell F, 'Technology and the Judicial Role' in Gabrielle Appleby and Andrew Lynch (eds), The Judge, the Judiciary and the Court: Individual, Collegial and Institutional Judicial Dynamics in Australia (Cambridge University Press 2020) 
Zeleznikow J,  Building Decision Support Systems in Discretionary Legal Domains  (2000) 14 International Review of Law, Computers & Technology 341 
7 
MEDIA & TECHNOLOGY 
AI in Media & Technology Sector: Opportunities, Risks, Requirements and Recommendationss 
Authors 
Jo Pierson 
Professor of Media, Innovation and Technology at Vrije Universiteit Brussel, Belgium 
Stephen Cory Robinson 
Senior Lecturer/Assistant Professor in Communication Design at Linking University, Norrkoping, Sweden 
Paula Boddington 
Senior Research Fellow, New College of the Humanities London, UK 
Patrice Chazerand 
Director at DIGITALEUROPE 
Aphra Kerr 
Professor of Sociology at Maynooth University and Maynooth lead of the ADAPT Centre for Digital Media Technology, Ireland 
Stefania Milan 
Associate Professor of New Media and Digital Culture, University of Amsterdam 
Fons Verbeek 
Full Professor in Bio-Imaging and Bio-Informatics, Leiden Insitute of Advanced Computer Science 
Cornelia Kutterer 
Senior Director, Rule of Law & Responsible Tech, European Government Affairs at Microsoft 
Evdoxia Nerantzi 
European Government Affairs at Microsoft 
Elizabeth Crossick 
Head of Government Relations at RELX 
ABSTRACT 
As AI systems increasingly pervade modern society and lead tomanifold and diverse consequences,thedevelopment ofinternationally recognized andindustry-specific frameworks focusing onlegal and ethical principles is crucial. This report aims at (a) understanding how the 7 Key Requirements for Trustworthy AI impact the Media and Technology sector (MTS) and at (b) putting forward guidelines to ensure compliance with the 7 Key Requirements. The report identifies four application areas of AI MTS, i.e. automating data capture and processing, automatingcontentgeneration,automatingcontentmediationand automating communication. Subsequently, the 7 Key Requirements are discussed within each of the four identified themes. Ultimately, recommendations are madetoensure that AI development and adoption in Media and Technology sector is compliant with the7Key Requirements. Three clustersofrecommendations are proposed: (1) addressing data power and positive obligations, (2) empowerment by design and risk assessments and (3) cooperative responsibility and stakeholder engagements. 
Keywords: 
Artificial Intelligence, Media and Technology Sector, Trustworthy AI 
1. Introduction 
AI systems are increasingly pervasive in the individual, organisational, and institutional layers of modern society. Laying the foundationsfor a Good AI Society , the multi-stakeholder initiative AI4People initiated the development of internationally recognized and industry-specific frameworks, considering ethics principles.1 2 This report examines the large-scale deployment of AI (understood as intelligent and/or autonomous systems) in the Mediaand Technology sector (MTS). Within this sector, the report lays out four centralthemes:automating data capture and processing, automating content generation, automating content mediation,andautomating communication.For each of these themes, thereport identifies overarching opportunities andrisks stemming from theuseof AI. 
In this report, the Mediaand Technology Committee  chaired by Jo Pierson  putsforward guidelinestoensure compliance withthe7Key Requirements for Trustworthy AI. The 7 Key Requirements for Trustworthy AI were originally developed by the European Commission s High-Level Expert Group on Artificial Intelligence3 and include: 
1.Human agency and oversight: Allowing humans to make informed decisions and ensuring human oversight mechanisms; 2.Technical robustness andsafety: Ensuring resilient and secure AI systems, afall back plan, accuracy, reliability and reproducibility; 3.Privacy anddatagovernance: Respecting privacy andensuringprotection, 
governance, quality of and access to data; 4.Transparency: Ensuring transparent, explainable, and traceable AI models; 5.Diversity, non-discrimination andfairness:Ensuring accessibility toallwhile 
diminishing prejudice, discrimination, and unfair bias; 6.Societal and environmental well-being: Ensuring sustainable and environmentally friendly AI systems and considering social and societal impact; 7.Accountability: Ensuringresponsibility andaccountabilityofAIsystems and their outcomes and adequate redress. 
Thisreport is structured in three main sections. After the introduction, Section 2 delineatestheMedia and Technology sectorbasedonGarnham s framework of mediationand identifiestheapplication areas ofAI.Thissectionalsoprovides the 
1 All co-authors of this paper constitute the AI4People-Automotive Committee. Floridi, L., Cowls, J., Beltrametti, M., Chatila, R., Chazerand, P., Dignum, V., Luetge, C., Madelin, R., Pagallo, U., Rossi, F., Schafer, B., Valcke, P., Vayen, E. (2018). AI4People: An Ethical Framework for aGood AI Society: Opportunities, Risks, Principles, and Recommendations. Minds and Machines 28, 689 707. 2 Hagendorff, T. (2020). The ethics of AI ethics. An evaluation of guidelines. Minds and Machines, 30: 99 120.3HLEG(2019). Ethics guidelines for trustworthy AI.Brussels: Independent High-Level ExpertGroup onArtificial Intelligence. 
definition of AI used throughout this report. Section 3 summarizes the state-of-the-art inEuropean AIgovernance. Section4isdividedintotwo parts.First, the7Key Requirements are discussedwithinthefour identifiedthemesoftheMedia and Technology sector. Second,recommendations are made to ensure that AI development and adoptionis compliant with the 7 Key Requirements in the Media and Technology sector. 
2. Conceptual framework for AI in Media and Technology Sector 
a. DefinitionMediaandTechnologySector 
This section delineates the Media and Technology sectorand identifies the application areas of AI. This is no simple matter giventhe broad field and fast evolutionof the Media and Technology sectorduetoconstant innovation. To begin with, MTS involves every form of technologically supported interactionand communicationwithinanecosystem where they intersect with specific dynamics, i.e. personalizationalgorithms. This refers to digital media, i.e. digitised traditional content media, and digital platforms which act as socio-technological intermediating architectures and infrastructures enabling and steering interaction and communicationbetweenusers through collectionand circulationof data.4 These data are collected, processed and used in MTS for many purposes, among which automated personalisationof (recommendations for) content (e.g. news) and advertising 
(e.g. targeted advertisements). We observe how especially apps are taking anincreasingly prominent place in the MTS, asalargeamount of digital media communicationtoday happens via apps, while being embedded within a wider ecosystem. 
To determinethe essential dimensions of the MTS and to situate AI, we adopt the three main components of Garnham s concept of mediation.5 
  
The first dimension includes human agents (human intermediaries)which refer to people themselves being mediators, e.g.  gate-keepers  in (citizen) journalism and news production. 

  
The second dimension includes content (systems of symbolic representation) in the form of language andsymbols, i.e. how humans produce ( encode ) text and consume ( decode ) text and what happens to the meaning when it is transported and mediated through languages and cultures. 

  
The third dimension, which includes technological systems (technological tools in media systems), prevails when it comestoAI applications in the MTS. The dimensionrefers totherole andmeaningofmediasystems andrelated technologies. 


4 Helberger, N., Pierson, J., & Poell, T. (2018). Governing online platforms: From contested to cooperative responsibility. The information society, 34(1), 1-14.5 Garnham, N. (2000). Emancipation, the media, and modernity: arguments about the media and social theory. New York: Oxford University Press. 

Figure 1:  Trustworthy AI  heptagon in the Media and Technology sector (own figure) 
Within the proposed frame, we identify the following examples of application areas of AI in the MTS which this report examines in thelight of the 7 Key Requirements of  Trustworthy AI : 
Human agents    Automating tools for journalists (Twitter analysis)  
+Technological    Data journalism tools  
systems    Newsletter andCustomer Relationship Management (CRM) tools   Social media advertising   Etc.  
Content + Technological systems    Computational journalism and robot journalism   Augmenting journalisticpractices (high level of AI autonomy): information sharingandgathering, contentgeneration (e.g. sportsandfinancial reporting), revision and distribution   Chatbots, cobots, robots   Deepfakes production and diffusion based on AI   Search engines (algorithm-based technologies)   Smart speakers, voice assistants,newforms of communication (VR/AR)   Speech and face recognition systems   Image analysis software   Marketing automation, programmatic advertising(real time bidding) and online behavioural advertising   Etc.  
Human agents + Content + Technological systems    Digital media   Journalistic practices (low ormediallevel ofAI autonomy): information sharingandgathering, content generation (e.g. sports reporting), revision and distribution   Digital intermediaries   Digital platforms   General-purpose social media platforms, e.g. Facebook, Twitter   Specific-purpose platforms, e.g.Craigslist, Upwork   News via social media by journalists, citizen journalists and people   Messaging services   Personalisation algorithms (e.g. based on inferential predictive analytics)   Video games   Etc.  

Table 1: AI application areas in the MTS 
We see the MTS is highly relevant and even exemplary for discussing opportunities, risks and requirements for Trustworthy AI. This is related to several factors. The sector ismore directly user-facing compared toothersectorssuchasenergyorautomotive sector, with, for example, social media platforms being essential for social interaction and information sharing. This meansthat people might peg the confidence they should have indigital technology to how much they cantrust social media platforms. However, at the sametime, the MTS offers the opportunity to provide AI with apromising front office, by realistically framing doom stories and possibly showcasing the advantages of cutting-edge technology. In that way, people canlearn the ropes of empowerment in an environment which is more familiar, orless forbidding than anything related to health ormobility. Consequently, the MTS lends itself very well for analysing and discussing Key Requirements for Trustworthy AI in Europe. 
b. DefinitionAI 
The AI HLEG (2019) defines Artificial Intelligence ashuman designed systems which are implemented in the digital orphysical environment in the form of software-based systems orpossibly hardware devices. Being givenacertain goal, AI collects data and assesses the information based on reasoned decision-making in order to suggest relevant actionstoachieve thegoal.Thisprocess isguidedby asetofsymbolicrulesora numeric model as well as the ability of AI to learn from their environment and previous outputs.6 TheCOM (2018)onArtificialIntelligenceinEurope emphasizes the intelligent and to a certain extent autonomous behaviour of AI systems.7 
In fact, AIcanbe definedasmachines that acquire cognitive capabilities such as learning,takingdecisions,communicatingandinteractingbasedondigitaldata. Machine learning (ML) and algorithms are two essential features of this process. An algorithm is asoftware which processes input, i.e. data, based ondescribed rules and selects the relevant information for theuser. Moreover, AI is capable of prediction-making, decision-making and problem-solving.8 
6 HLEG (2019). A Definition of AI: Main Capabilities and Disciplines. Brussels: Independent High-Level Expert Group on Artificial Intelligence.7 COM (2018). Communication from the Commission to the EuropeanParliament, the European Council, the Council, the European Economic and Social Committee and the Committee of the Regions. Artificial Intelligence for Europe. European Commission, 237 final. 8 Just, N., & Latzer, M. (2017). Governance by algorithms: reality construction by algorithmic selection onthe Internet. Media, culture & society, 39(2), 238-258. 
Examining the level of autonomy of AI, Boucher (2019) differentiates between two waves inAI development.9 The first wave, called symbolic artificial intelligence, is rather human-centered, which meansthateven though the AI system performs tasks autonomously, the decision-making process is still guided by humans (human in the loop). The system s intelligence stems from the encoding of human expertise and, hence, makes the process and output more comprehensible for humans. In the second wave, calleddata-driven machinelearning, algorithms gain more autonomy andbecome ratherindependentfrom humanexpertiseastheytrainthemselves from dataand statistics(from human-over-the-loop tohuman-out-of-the-loop).Striking abalance between data-driven and human-centred expertise and assistance is important especially within the scope of the MTS sector asautomatisation processes increasingly penetrate journalismandcommunicationactivities,acore feature ofEuropean democratic processes. 
Given that AI systems make recommendations and provide normative solutions, thenotionoftrustisimportanttobeexamined.10 According totheAIHLEG, trustworthiness shouldrepresent a prerequisite for people andsocietytodevelop, deploy and use AI .11 Hence, the MTS needs to be continuously vigilant that AI systems stay trustworthy even after having been developed, implemented and/or used. People should not be  nudged  or forced to use systems they do not trust or that do not adhere to the 7 Key Requirements. In this light, the next section briefly examines how this has been tackledby the EU to date and what this in particular means for AI applications in the MTS. 
9 Boucher, P. (2019). How artificial intelligence works. Brussels: European Parliament Research Service. 10 Ferrario, A., Loi, M., & Vigan , E. (2019). In AI We Trust Incrementally: A Multi-layer Model of Trust to Analyze Human-Artificial Intelligence Interactions. Philosophy & Technology. doi: 10.1007/s13347-019-00378-311HLEG(2019).Ethics guidelinesfor trustworthy AI.Brussels: Independent High-Level ExpertGroup onArtificial Intelligence. 
3. European AI Governance for MTS 
The EU s long-standing emphasis ondemocraticvalues and the rule of law also shapes itstechnology governance approach. Thisisespecially relevant for theMTSwhere automated-decisionmakingtechnologiesandalgorithm-dependentprocesses are becoming increasingly essential. We take acloser look at how AI governance is taking form in the EU, with a focus on MTS-related issues. For the purpose of this report the High-Level Expert Group onArtificial Intelligence (AI HLEG)12 was an important initiative, being appointed by the European Commission in June2018. Since then, the AI HLEG s work has been considered assubstantial in defininga European  governance approach centred around the concepts of  ethical  and  trustworthy  AI. The AI HLEG bases its considerations on three key requirements for AI: legal (i.e. AI should comply with the law); ethical (i.e. AI should fulfil ethical principles); androbust (i.e.AIshouldbebuiltsafely andonthehighest quality standards). InJuly 2020,theAIHLEGpublishedtheirfinalAssessmentListfor Trustworthy Artificial Intelligence (ALTAI) for allrelevant stakeholders, particularly those involved in developing and deploying AI systems, toself-assess compliance of specific AI use cases with the 7 Key Requirements for Trustworthy AI. 
The European Commission (EC) also incorporated the AI HLEG recommendations intheirlatestWhite Paper onArtificialIntelligence.13 Thedocumentsetsforth to promote anddevelop AIbasedonEuropean values, following aregulatory and investment-based approach. The EC refers tothe MTS particularly in the contextof protecting fundamental human rights and ensuring legal certainty.14 
Specifically, theEChighlightstheuseandpotential impact ofAI(1)for information selection andcontent moderation by online intermediaries; (2) in tracing people s daily habits; and (3) in creating information asymmetries by which citizens mightbeleftpowerless. TheECisparticularly concernedaboutsomepotentialAI systems  features, such as  opacity ( black box-effect ), complexity, unpredictability and partially autonomousbehaviour ,15 inoverseeing and enforcing the existing EU legal fundamental rights framework. This may be the reason for the introduction of specific rules for  high-risk  AI systems in a possible forthcoming EU regulatory framework for 
12 European Commission. (2019). High Level Expert Group onArtificial Intelligence. Retrieved onMay 20, 2020, from https://ec.europa.eu/digital-single-market/en/high-level-expert-group-artificial-intelligence 13European Commission. (2020). White Paper onArtificialIntelligence. AEuropean approach toexcellence andtrust. Retrieved onMarch 20, 2020, fromhttps://ec.europa.eu/info/sites/info/files/commission-white-paper-artificial-intelligence feb2020_en.pdf 14European Commission. (2020). White Paper onArtificialIntelligence. AEuropean approach toexcellence andtrust. Retrieved onMarch 20, 2020, fromhttps://ec.europa.eu/info/sites/info/files/commission-white-paper-artificial-intelligence feb2020_en.pdf 15European Commission. (2020). White Paper onArtificialIntelligence. AEuropean approach toexcellence andtrust. Retrieved onMarch 20, 2020, fromhttps://ec.europa.eu/info/sites/info/files/commission-white-paper-artificial-intelligence feb2020_en.pdf, 12 
AI systems. An AI system could be considered as high-risk  if  both the sector and the intendeduseinvolve significantrisks 16,particularly ifsafety, consumerrightsor fundamental rights are at stake. If an AI system meets the  high-risk  criteria, compliance with strict requirements and oversight would be mandatory. The EC explicitly sets out the protection of the following EU rights: 
  
Fundamental rights: Free expression; political freedoms; personal data protection; privacy protection; non-discrimination. 

  
Legal certainty: Safety; liability; cybersecurity. 


Thesefundamental EUrightsare relevant for theMTSsince information, communication and mediationactivitiesare allintrinsically linked andsomewhat a prerequisite for democracy and the rule of law inthe EU. Particularly relevant for the MTSisthattheWhite Paper specifically mentions  online intermediaries  and their responsibility inadequately safeguarding the abovementioned rights asrequired by EU legislation. Further, the EC underlines that citizens should clearly be aware about their interactions  withanAIsystem, and not ahumanbeing. 17According tothespecific context in which the AI applicationoperates, the EC emphasises  objective, concise and easily understandable  informationprovision. Next totheWhite Paper onArtificial Intelligence, the EuropeanCommissionprovides aninterpretationof the existing safety and liability framework specifically for Artificial Intelligence, the Internet of Things and robotics.18 Further, the documents apply inadditionto key requirements for protecting data subjects and their data, as set out by the EU data protection legislation (GDPR). 
In October 2020, the European Parliament released two legislative initiatives to develop anethicsframework for AIandacivil-oriented liabilityframework for AI causing damage. The firstinitiative callsfor alegalframework outliningtheethical principles and legal obligations for AIfollowing guiding principles suchashuman-centric and human-made AI, safety, transparency and accountability, safeguards against biasand discrimination, right toredress, social and environmental responsibility, and respect for privacy anddataprotection.19 Thesecondinitiative encouragesthe development ofacivil-oriented liabilityframework, calling for liabilityofhumans 
16European Commission. (2020). White Paper onArtificialIntelligence. AEuropean approach toexcellence andtrust. Retrieved onMarch 20, 2020, fromhttps://ec.europa.eu/info/sites/info/files/commission-white-paper-artificial-intelligence feb2020_en.pdf, 1717European Commission. (2020). White Paper onArtificialIntelligence. AEuropean approach toexcellence andtrust. Retrieved onMarch 20, 2020, fromhttps://ec.europa.eu/info/sites/info/files/commission-white-paper-artificial-intelligence feb2020_en.pdf, 2018 European Commission. (2020). Report onthe safety and liability implications of Artificial Intelligence, the Internet of Things and robotics. Retrieved on March 20, 2020, from https://eur-lex.europa.eu/legal-content/en/ TXT/?qid=1593079180383&uri=CELEX%3A52020DC0064.19 Garc a del Blanco, I. (2020). Report with recommendations tothe Commission onaframework of ethical aspects of artificial intelligence, robotics and related technologies. Retrieved onNovember 1, 2020, from https://www.europarl.europa. eu/doceo/document/A-9-2020-0186_EN.html. 
when operatingwithhigh-risk AIactivity.20 Moreover, theEuropean Parliament publishedareport onintellectualproperty rights. Thereport urgestodistinguish between AI-assisted human creations and AI-generated creations and encouragesan effective intellectual property rights system (IPR) aswell assafeguards for the EU s patent system to protect innovative developers.21 
Considering the indicatedEuropean AI policy initiatives, this report contributes to the EU AI governance process by establishing an ethical framework for AI applications in the MTS. 
4. Research questions 
The Media and Technology committee consists of 12 members representing different stakeholders of academia and media and technology industry. The goal is to establish a concerted perspective on the meaning and significance of the HLEG 7 Key Requirements for Trustworthy AIinrelation totheMTS.For this,thecommitteeheldregular gatherings to discuss implications of the requirements in their respective expertise and industry. The multistakeholder procedure for developing the main research questions, consecutive outcomes and the final report was organised as follows: 
-Discussing 7 Key Requirements and specific casesbasedoncommittee members  expertise and practical experience; 
-Asking members tosubmitcasestudies containing best and worst practiceuse cases of AI in the MTS; 
- Members submitted their cases; 
-Discussion of the submitted cases; 
- Members provided additional information, literature and explanation on cases; 
-CommitteeChair and Advisors scanned all casesfor keywords and overlapping issues and best/worst practices; 
-CommitteeChairandAdvisors grouped submitted AIcasestudiesintofour categories (themes); 
-Committee ChairandAdvisors cross-combined four themeswith7Key Requirements; 
20 Voss, A. (2020). Report with recommendations tothe Commission onacivil liability regime for artificial intelligence. Retrieved on November 1, 2020, from https://www.europarl.europa.eu/doceo/document/A-9-2020-0178_EN.html.21S journ ,S.(2020). Report onintellectualproperty rights for thedevelopment ofartificialintelligence technologies. Retrieved on November 01, 2020, from https://www.europarl.europa.eu/doceo/document/A-9-2020-0176_EN.html. 
- Committee Chair and Advisors identified tensions within the four themes; 
-Cross-combination of four themes with 7 Key Requirements was first discussed in-depth,afterwhich theviewofthecommitteewas furthervalidated and visualised through an online form and interviews among members; 
-Members proposed recommendations toensure compliance withthe7Key Requirements within the four main themes of the MTS; 
-CommitteeChairandAdvisors identifiedthree prevailing recommendation clusters. 
I. How do the 7 Key Requirements impact 
theMediaandTechnologysector? 
AI technologies are used in various MTS areas and for various purposes. Given the related manifold and diverse consequences of AI, the analysis and discussion of the 7 Key Requirements for trustworthy AIinMTSisstructured according tofour main MTS AI application and use themes: 
a)
 Automating data capture and processing; 

b)
 Automating content generation; 

c)
 Automating content mediation; 

d)
 Automating communication. 


Thefour themesare mappedinlinewiththetypical (big) datalife cycle ofdata capture, processing and interpretation, preparation and creation, and usage.22 The four MTS AI application and use themes aim (1) to be mutually inclusive and (2) to largely capture allrelevant caseswhich fall under the MTS in the scope of this report. This is madetangible inthefollowing paragraphs by focusing onconcrete examples, when discussing the Key Requirements for each individual theme. This approach allows for a content-based discussion instead of discussing various casesand impacts under each key requirement. Thisbottom-up, practically orientedmethodologyalsoallows to discuss tensions between the 7 Key Requirements. 
22 Jagadish, H. V., Gehrke, J., Labrinidis, A., Papakonstantinou, Y., Patel, J. M., Ramakrishnan, R., & Shahabi, C. (2014). Big data and its technical challenges. Communications of the ACM, 57(7), 86-94. 

Figure 2: The 7 Key Requirements for  Trustworthy AI  in relation to the four themes of the Media and Technology Sector. 
a. Theme 1: Automating data capture and processing 
Thefirsttheme,automating data capture and processing,entailsavariety ofAI technologies concerned with the systematic capture and processing of data in the MTS. Thistypically includesdatacapture andprocessing by digital media, platforms and websites for reasons ofpersonalisation,profiling, inferential predictive analytics, targeted advertising, etc. However, this type of automation also includes emotional AI in the form of facial and voice recognition systems aswell asGPS/location tracking, contact tracing apps, and VR/AR headsets.23 Within the  Trustworthy AI  heptagon (Fig. 
1)this theme is concerned with the diminishingaspect of humans asagents against augmenting impacts of technological systems on content. 
The principles human agency and oversight aswell asprivacy and data governance have a high impact on automated data capture andprocessing. First, the EU s legislative framework, in particular data and consumerprotection standards, protects individuals  fundamentalrightstomake informed andindependent choices.Thisalsoapplies in relation toautomateddatacapture andprocessing AIsystems: human agency and oversightaswell asprivacy and data governance demand that citizens should always be abletodecide ifandhow theychoosetouseacertainserviceorbeinadvertently tracked by it. Incaseof using aMTS service, there is the right to decide onwhat and how much data will be collected, what it would be used for, where itwould originate from, and how itwould be shared. Given the advertising-driven business model for a significant part of MTS, special attention is needed on how data capturing and processing takes shapewithregard toadtechandmarketing automation.Thisisparticularly relevant for online behavioural advertising (OBA), where internet users  behavioural data(website visits, clicks, mousemovements, etc.)andmetadata(browser type, location,IPaddress, etc.)are collectedandprocessed tocreate profiles usedto personaliseadsandtoimprove conversion rates.Recent events have shown that especially automatedadvertising systems ofreal-time bidding(RTB) have been capturingand processing inpossibly prohibited andunethicalways.24 RTB inad auctions is the system by which advertisers bid onthe possibility of instant targeted advertising to website visitors by using personal data that is collected through tracking and is shared with all bidders. Even advertisers who do notwin the auction receive personal datain order toascertain their interest in the auction. Some advertisers are reported to participate inthe auctions merely to enrich their data sets. The targeting is basedonprofiles of usersbuilt via the extensive and persistent tracking of online and possibly offline activities (e.g. via cookies orpixels). The profiles contain categories of users  past behaviour, but also inferred preferences and affinities, being often sensitive categories protected by the GDPR. For example, Google and several data brokers have beenaccusedofviolatingEU'sdataprotection rulesby harvesting andprocessing people's personal data to build detailed online profiles, including information on sexual orientation, health status and religious beliefs.25 Additionally, the Norwegian consumer council investigated the data traffic from popular mobile apps. This revealed anumber 
23 For example immersive mixed reality headset (i.e. Microsoft HoloLens).24 Information Commissioner s Office (2019). Update report into adtech and real time bidding, 20 June 2019. Retrieved on November 13, 2020, from https://ico.org.uk/media/about-the-ico/documents/2615156/adtech-real-time-bidding report-201906.pdf25 Scott, M., Manacourt, V. (2020). Google and data brokers accused of illegally collecting people's data: report. in: POLITICO, 21September2020.Retrieved onNovember 12,2020,from https://www.politico.eu/article/google-and-data-brokers accused-of-illegally-collecting-data-report/amp/ 
ofseriousprivacy infringementsandalargeamountofillegaldatasharingand processing.26 Academics anddataprotection practitionershave madeproposals to address these type of privacy infringements. Wachter and Mittelstadt suggest introducing the righttoreasonable inferences  by which meaningfulcontrol andchoiceover inferences and profiles are granted to data subjects.27This would be particularly relevant for high-risk inferences thatare privacy invasive orreputation damaging and have low verifiability in the sense of being predictive or opinion-based.28 Envisaged as an ex-ante mechanismtoprovide justificationfor thereasonability ofaninference, disclosing relevance ofthedatainquestion,relevance oftheinferences drawn, accuracyand statisticalreliability of the methods used, these disclosures should be accompanied by an ex-post mechanism enabling inferences to be challenged. This right should close the gap both of explainability and accountability. 
In addition, given the importance of being able to collect and process asmuchas possible (personal) datafor optimising personalisation ofcontentandadvertising, specialattention isneeded tosafeguard alevel playing fieldinMTS.Althoughall players in themedia and advertising ecosystem are affected by the GDPR, larger players may bemore resilient toregulatory interventions. Incasesmallercompetitorsdrop away, theconsolidationofpersonaldatainfewer handsmightalsoincrease, and perversely, negatively affect people s rightsandfreedoms overall. For thatreason, various initiatives have been taken, especially in smaller media markets, topool data and to process them for the benefit of different (competitive) companies at once.29 
Automated datacapture andprocessing alsotakes place inothertypesof applications (inwork, health, leisure time) aswell asdevices(VR/AR headsets30). Especially if emotion-reading and -inferring AI systems were to be adapted onalarge scale for partially abled people, the option to not use or be subjected to such AI systems shouldalways beavailable for theperson. Moreover, individuals should be aware if they are being systematically tracked, such as by websites, platforms, apps and cameras, and for which purpose, based onanopt-in regime in line with the GDPR. However, 
26 ForbrukerR det. (2020). Out of control: How consumersare exploited by the online advertising industry. Report by the Norwegian Consumer Council.27 Pop Stefanija, A. (2019, July 7-11). Algorithmic selfie: onthe right toassessalgorithmic identity and exercise right of access . Madrid, Spain: IAMCR 2019 Conference.28 Wachter, S., & Mittelstadt, B. (2019). A right to reasonable inferences: Re-thinking data protection law in the age of big data and AI. Colum. Bus. L. Rev., 494. 29van Zeeland, I., Ranaivoson, H.,Hendrickx, J., Pierson,J., Van denBroeck, W. &van derBank,J. (2019).Salvaging European media diversity while protecting personal data. Brussels, Belgium: SMIT Policy Brief #23, Report for Chair  Data Protection on the Ground  (Media Sector).30 As such, Microsoft HoloLens -immersive mixed reality headset -canhelp people who are blind and with low vision learn who is where in their social environment. Roach, J. (2020). Using AI, people who are blindare able to find familiar faces in aroom. Retrieved onMay 2, 2020, from https://news.microsoft.com/innovation-stories/project-tokyo/?utm_source=pre-amp 
there canalso conditions where certain types of capture are required for overriding purposes (e.g. tracking potential terrorists, in caseofasubstantiated suspicion). The key principle to safeguard human agency and oversight also implies that citizens must be aware iftheir information orfaceisbeingrecorded, especially ifpersonaldatais tracked. Giving their consent in context of AI applications often involves aweighing of benefitsandharmsofnot opting in, resulting inaratherreluctant agreement than 
31 32
genuine willingness. Given that tracking of data has become an essential part of many platforms and services inMTS,notoptingintotheconditionsoftenleadstosubstantialdisadvantages for users.A swift implementation of automated systems for data capture and processing during emergency situationsmay however leadtolower standards ofaccuracyand ethical oversight (e.g. in the case of COVID-19 contact tracing apps).33 In addition, the wideadoption oftracking appsmay leadtochillingeffects insurveillance offree movement and/or individual behaviour. 
Moreover, users should have agency over their data when they visit MTS websites oruseappswhich record certain information onpurpose. Users  given consent needs to be purpose-limited and context-specific. The key principle transparency would enable increased human agency and oversight if AI uses systematic tracking and tracing features. Transparent and explainable automated data capture AI can build human oversight and trust in technologies. However, a challenge to becoming transparent poses the complex nature of AI itself, such asmachine learning. It seemsto be adifficult matter to agree onwhat a transparent  explanation ofthesystem mustcontainandtocometoan understanding of what sort of information is enough for all types of individuals subject to the data capture and processing. Linking this to consent, the specific requirements of transparency needed to obtain genuine consent could vary from domain to domain. 
The principle of diversity, non-discrimination and fairness would impact the MTS inaway that algorithmic data capture and processing AI should not discriminate and/ or be biased, and promote a stated conception of fairness. Conceptions of what is a fair 
31 Apps for coronavirus contact tracing could trace the spread of disease, to understand infection pathways for risk individuals andcommunities, and could help indelivering resources towhere needed. However, thesametechnology could be used for widersurveillanceof populations and for very punitive consequences insomesocieties, especially in combination with other technology such as facial recognition to monitor citizens  behaviour. Surveillance measures may outlast the need. Prasso, S. (2020). Corona Virus surveillance helps, but the programs are hard tostop. Retrieved onApril 20, 2020, from https://www.bloomberg.com/news/articles/2020-04-06/coronavirus-surveillance-helps-but-the-programs-are-hard-to-stop32 Gershgorn, D. (2020). We mapped how the Coronavirus is driving new surveillance programs around the world. Retrieved onApril 20, 2020from https://onezero.medium.com/the-pandemic-is-a-trojan-horse-for-surveillance-programs-around-the world-887fa6f12ec9 33 Van Zeeland, I. & Pierson, J. (2020). Contact tracing apps and solutionism. Position statement for the Future of Privacy Forum's Privacy & Pandemics: Responsible Uses of Technology and Health Data During Times of Crisis -An International Tech and Data Conference . 
distribution of anything in society differs. Subsequently, what asystem developer may deem as  fair  should be explicitly stated, as well as promoted already in the data capture stage (e.g. donotonly process dataonyoung people s news preferences ifthisis further used for providingrecommendations to seniors). In relation to automating data capture, the link between technical robustness and safety is paramount because automated data capture systems needtofully represent all potential usersand other individuals who will be affected by the systems  outcomes, and not only a certain (biased) part of its dataset. In the MTS, those key requirements are particularly relevant for content-related platforms, like search engines (e.g. Google, Bing), socialnetwork sites(e.g. Facebook, Twitter) and video sharing services (e.g YouTube, Vimeo). Equally important are to uphold the key requirements diversity, non-discrimination and fairness to avoid the simplistic classificationofemotions,which couldresult inunwanted socialsorting. More generally, cultural norms in emotions are not yet fully researched, and psychological research would besuitedtoinform technologydevelopers aboutthesocialnorms behind public display of emotions. Furthermore, if the AI system could appropriately adoptculturalnorms,itwould require considerationifreinforcing certaincultural normsis desirable ornot. It is, more fundamentally, worth considering which ethically legitimate purposes could be served by processing data on human emotions at scale. 
Technical robustness and safety are alsohighly important tonotmarket any AI systems for which theimpactisnotwell-researched, andwhich are notyet fully developed, based onthe precautionary principle. The risk is to release it too early. As such,the misreading  ofemotionscouldcreate seriousdamagetobothusersand corporatereputation. This is also why the auditing of automated and algorithmic data capture and processing AI systems is key: accountability enablesamore comprehensive assessmentofthe purpose, development and deployment ofautomated data capture andprocessing AIandadditionally enhancetransparency.Finally, accountability can further be improved through multi-stakeholder deliberation, maintenance and oversight, where also citizens and civil society organisations are represented in meaningful way. 
Prospectively, automatingdatacapture andprocessing technology couldallow more immersive interactionswithsurrounding environments by meansofVirtual Reality (VR) or Augmented Reality (AR) technologies.34 Large amounts of data could determinethelarge-scale nudging by  recommendations , assuchfor onlinemaps, servicesorproducts. Automating data capture and processing could enabletargeted consumerchoicebutlikewise decrease human agency and oversight.More and better datasets by automating data capture and processing could create powerful nudges based 
34Pollock, D. (2019). Digital billboards open-upadvertising toblockchain, artificialintelligence,andcryptocurrency. Retrieved onApril 20, 2020, form https://www.forbes.com/sites/darrynpollock/2019/04/18/delving-into-digital-advertising as-blockchain-cryptocurrency-iot-ar-and-ai-enter-the-frame/ 
onemotional appeals which oneis unable to rationally and cognitively process. This is whyhuman agency and oversight are key requirements aslongasAI technologiesfor widespread automatingdatacapture andprocessing progress. At thesametime, prediction basedonautomating datacapture andprocessing requires considering accountability.People should be abletoknow who is providing the information and what databasetheprediction isbasedon,suchasmodelsofotherpeople orpast behaviour. Several principles cometogether, asaccountability towards usersand supervisory authorities, to enable effective independent oversight, requires transparency for datasetstodeterminewhether thecaptured andprocessed dataindeedsupports diversity, non-discrimination, and fairness. 
b. Theme 2: Automating content generation 
The second theme, automating content generation, refers to online content produced either fully by automated systems or partly in combination with human agents. Examples ofcommonAIusesincontentgenerationare text-based news reporting apps (based on user preferences)35 and translation tools, and-in a malign way -disinformation and deepfakes on online platforms. The question remains how much of this type of content is fully automated. The automated element is perhaps more prevalent in the diffusion andamplification ofthecontentratherthantheproduction ofit.Inaddition,an emergingAIapplication area are creative industries,suchasthemusicandgames industry, andcreative AI/computing.36 Consideringthe  Trustworthy AI  heptagon (Fig. 1),strong links between technological systems andcontentbecome evident. In sum, increasing automation incontentgenerationmay provoke animbalance disfavouring the role of human agents in content generation. 
Ashumanagentslike journalistsplay amajorrole inproviding trustworthy information, theaspectsofhuman oversight, accountability,andtechnical robustness are highly important.AI-driven toolsare already employed injournalisticcontent generation,which relates totheprinciple ofhumanagencyandoversight. Indata journalism, for instance, AI helps to identify patterns in large datasets. AI-driven tools can suggest titles and photos, help to find a new topic angle, and produce draft versions of articles. Automated systems assist the journalist in writing the story, but the journalist is still the main storyteller.37Thus, a high level of editorial input and human oversight remains; atthesametime, publishing articles becomes more efficient. The increasing 
35 For example Google News, Apple News, Reddit, Digg, and Flipboard.36 Amato, G., Behrmann, M., Bimbot, F., Caramiaux, B., Falchi, F., Garcia, A., & Koenitz, H. (2019). AI in the media and creative industries. arXiv preprint arXiv:1905.04175.37 Willens, M. (2019). Forbes is building more AI tools for itsreporters. Retrieved onMarch 4, 2020, from https://digiday. com/media/forbes-built-a-robot-to-pre-write-articles-for-its-contributors/. 
pace and efficiency of news production triggered by automation canput pressure on smaller newsrooms whichusually do not dispose large datasets and robust AI-systems.38 Insome news genres, especially thosethatare ratherfact-based, theautomationin news generation is higher. For instance, specific natural language processing tools can generate sports articles and financial reporting,39 while recent projects even involve videoreporting40 Higherautomationofcontentgenerationcaneventually leadto transitions in working opportunities and possible job loss, impacting societal well-being.41 42 In addition, content produced by AI systems is often not flagged assuch to the user and this, hence, links to the importance oftransparency. 
Technical robustness of AI-driven tools in content generation is essential to manage largeamountsofdata.Datajournalismrequires robust AIsystems toanalyse data correctly andtoextract relevant  information. Akey issueisasthedefinitionof  relevant  information today. New forms of news/ information coupled with commercial pressures on the internet are shaping what is presented as  news  and how it is presented, 
e.g.clickbait (content whose main purpose is to attract attention and encourage visitors toclickonalinktoaparticularweb page). How andwhat information AI systems extract can, ultimately, shape how thereader understands the information, e.g. positive ornegative attitudetoward asubject. This canbe linked tothe principle of diversity, non-discrimination, and fairness. 
Anotherexample ofthesignificanceoftechnically robust AIsystems isin preserving practices ofEuropean culturalandarchitectural heritage.AIsystems are capable of digitising high volumes of informationwhich is stored in physical form in archives andmuseum;43 for instance,IVOW s  Culturally Sensitive Deep Learning model cancreate captions for photos generated by naturallanguageprocessing algorithms.44 
38 Helberger, N., Eskens, S. J., van Drunen, M. Z., Bastian, M. B., & M ller, J. E. (2019). Implications of AI-driven tools in the media for freedom of expression.39 Peiser, J. (2019). The Rise of the Robot Reporter (Published 2019). Retrieved on November 13, 2020, from https://www.nytimes.com/2019/02/05/business/media/artificial-intelligence-journalism-robots.html40 Chandler, S. (2020). Reuters usesAI to prototype first ever automated video reports. Retrieved onMay 10, 2020, from https://www.forbes.com/sites/simonchandler/2020/02/07/reuters-uses-ai-to-prototype-first-ever-automated-video reports/#7eb6a99f7a2a41 Lind n, C.-G., Tuulonen, H. (Eds.) (2019). News Automation. The rewards, risks and realities of  machine journalism . Frankfurt: WAN-IFRA. Retrieved November 22, 2020, from http://immersiveautomation.com/wp-content/uploads/2019/06/WAN-IFRA_News_Automation-FINAL.pdf. sws42 Srnicek, N. (2017). Platform Capitalism. Polity Press.43 Ibaraki, S. (2019). Artificial Intelligence For Good: Preserving Our Cultural Heritage. Retrieved onMarch 6, 2020, from https://www.forbes.com/sites/cognitiveworld/2019/03/28/artificial-intelligence-for-good-preserving-our-cultural heritage/#200a70094e96https://www.forbes.com/sites/cognitiveworld/2019/03/28/artificial-intelligence-for-good preserving-our-cultural-heritage/#200a70094e9644 IVOW. (2020).An AI and Storytelling Startup. Retrieved on May 10, 2020, from https://www.ivow.ai 
Technical robustness isalsohighly relevant inproducing andtranslatingtexts. Automated translationrisksreplication biases(e.g. stereotypes, genderandracial biases) and errors from training datasets. This canaffect the principle of diversity, non discrimination, and fairness.Given the linguistic diversity in the European Union, robust automated translation is highly important. It preserves linguistic and cultural plurality. For example theADAPTresearch center45 inIreland aimstodevelop datasetsand intelligentmodelsthatautomatically translateonlinecontentfor native speakers of low-resource languages,andmake importantcontentavailable topeople intheir language of choice. Projects have focused ondeveloping resources for Irish, Serbian, Basque,andnon-European languages including Hindi.Theirapproach istoemploy both, AI and human, rather than fully automated systems.46 
Onsocialmediaplatforms, deepfakes generatedby AI-driven toolsgrow in popularity. These formats simulateaspeechoranaction, usually of apublic persona (such aspoliticians,celebritiesandactors), where thegeneratedcontentdoesnot correspond toreality butreveals strikingresemblance. Thisishighly problematic becausesuchfalseinformation isoftengenerated withouttheknowledge ofthe individuals inquestion and viewers may be unaware the video was tampered with. This applies to the principle ofhuman agency andsocietal wellbeing.It canfoster the spread of contentious content like  fake news , disinformation, hate speech and harmful content. One of the mostpopular videos that went viral in 2019 portrays Marc Zuckerberg claiming to conquer the world. A recent study shows that 72 percent of people reading anAI-generated news story thought it was credible.47Another example is the Chinese app Zao which allows people to seamlessly swap themselves into famous movie scenes.48 Generating deepfakes andproducing disinformation challengesmediaintegrity. In addition, it can severely harm individuals through inappropriate and false representation aswell asharassment, for example by malign actions like revenge-porn, affecting not just public figures, but also regular, commonpeople. Forms ofredress totackle these issues seem to be underrepresented or do not guarantee general accessibility to citizens. Hence, it canalso be linked to the principle of diversity, non-discrimination, and fairness. 
45 Transforming Global Content. (2020). Retrieved on May 5, 2020, from https://www.adaptcentre.ie/research/transforming global-content/46 For example, they have developed a high-quality Irish-English system called Tapad ir to translate documents into Irish for the Irish government. From 2021 all European documents will also have to be translated into Irish and much of this will be done using these automated systems supplemented by Irish language native speakers and translators.47 Leibowicz, C. (2019). On AI & Media Integrity: Insights from the Deepfake Detection Challenge. Retrieved onApril 20, 2020, from https://www.partnershiponai.org/on-ai-media-integrity-insights-from-the-deepfake-detection-challenge/48 Kambhampati, S. (2019). Perception won t be reality, onceAIcanmanipulatewhat we see.Retrieved onApril 20, 2020, from https://thehill.com/opinion/cybersecurity/470826-perception-wont-be-reality-once-ai-can-manipulate-what-we-see 
In the music sector, deploying AI-driven tools links to the principles of human agency, societal wellbeing, and diversity, non-discrimination, and fairness.While AI may be beneficial for musiciansasitcouldenhancemusiceducation andcomposition,italsocauses concernsabout,for instance, replacing humancreativity andremoving thepersonal aspect of music creation. Furthermore, the human agency in question may affect societal wellbeing in hampering the development of human talent. This canresult in reducing opportunities for live music and canproduce acycle by which music is generated and experienced online and remotely, with an impact on human social life.49 Ultimately, the music sector does not represent anurgent human demand to be complemented by AI systems, to justify replacing human labour. 
c. Theme 3: Automating content mediation 
The third theme, automating content mediation, involves automated filtering systems in the distribution and moderation of online content and advertising. AI technologies incontentdistributionoccurin the form ofrecommender systems for entertainment andsocialmediacontent,onlinenews aggregators, andprogrammatic advertising (including RTB) which provide user-specific andcontext-conform content.A further setof AI systems is employed tomoderatecontent todetect and tackle contentious content like fake news, mis- and disinformation50,and harmful content.51 Linking this to the three components in the  Trustworthy AI  heptagon (Fig. 1) reveals that online content isincreasingly processed by technological systems eitherfully automatedor assisting human agents. 
Employing automated filtering systems in online content and advertising mediation tasksrequires acareful consideration of the principles diversity, non-discrimination, and fairness and human agency and oversight.For example, we observe how years after the initialresearch into discrimination in online employment ads, higher salary positions are stilladvertised topredominantly (assumed)maleusers.52 AsAItechnologies somehow occupy the newrole of traditional gatekeepers and doing agenda-setting in the online sphere, they canalso co-determine what people seeornotseeaswell as what content users can generate online. This could affect freedom of expression, media diversity andplurality of voices.53 In the case of algorithmic content distribution, it can constrain access to a diversity of information and create  filter bubbles  leading to  echo 
49 Castro, A. (2019). We ve beenwarned about AI and music for over 50 years, but noone s prepared. Retrieved onMay 1, 2020, from https://www.theverge.com/2019/4/17/18299563/ai-algorithm-music-law-copyright-human50 EPRS (2019). Regulating disinformation with artificial intelligence. EPRS (2019). Automated tackling of disinformation.51 Lacoma, T. (2020). League of Legends Survey Reveals Nearly Every Player Has Been Harassed. Retrieved on May 1, 2020 from https://screenrant.com/league-legends-survey-harassment-toxicity-riot-games-everyone/52 Datta, A., Tschantz, M.C., Datta, A. (2015). Automated Experiments onAd Privacy Settings: A Tale of Opacity, Choice, and Discrimination. arXiv. Retrieved on November 12, 2020 from https://arxiv.org/pdf/1408.6491.pdf 
chambers , i.e. personalised content.Especially online platform recommender systems tend to magnify hyperactive users  interests and content, while passive users' interests andcontentbecomemore invisible.54 55 Hence, politicalmicrotargeting andopinion formation couldbecomesubjectto(un)intentional algorithmicmanipulation. Furthermore, thedatasetsaswell asdeveloped standards aboutfairnessandnon discrimination for algorithmic filtering systems might contain bias and could leverage discrimination and social sorting. With regards todiversity, non-discrimination, and fairnessinthemediasector, mediarecommendation algorithmsmay worsen the position of smaller countries and their cultural values in media creation. 
These issues raise the importance of human agency and oversight in online content mediation. Algorithmic filtering systems constrain human agency as users are hampered in choosing which contenttheyreceive orif they want tobe exposed toalgorithmic recommendations atall.Furthermore, humanoversight iscrucialindetectingand tacklingdisinformation andharmfulcontent,alsoinrelation toprogrammatic advertising withadvertisers beingworried aboutbrandsafety withtheiradsbeing placed besidescontentiouscontentondigitalplatforms. Take, for instance,the  infodemic  orlargecirculation ofdisinformation andmisleadingadsduringthe Covid-19 pandemic (e.g. drinkingmore water would cure anindividualfrom the disease).57 58 59Especially in the context of health crises, correct information and reliable sources are particularly importantandthelackofitcanhave severe, even fatal consequences. This casereveals the importance of human oversight in fact-checking thecontentby professionals, such ashealth advice. Nevertheless, algorithmic filtering systems are required tomasterthe high volume and fast-paced production of online content.When it comestocontentmoderation, AI systems are crucial assistants for augmenting human agents in their demanding work of evaluating harmful content such aschildabuse, racism, andharassment.Thishasimmediateeffects onthephysical, mental and societal well-being of human contentmoderators. AI systems canfacilitate and support contentmoderation for humans60 by flagging harmful content, blurring outareas that are particularly harmful, orengaging in  visual question answering , i.e. humans moderationscanaskquestionstotheAItoolaboutthecontentwithout 
53 Helberger, N. Karppinen, K., D Acunto, L. (2018). Exposure diversity asadesign principle for recommender systems. In: Information, Communication & Society, 21:2, 191-207.54 Content Personalisation Network. (2020). Retrieved on May 1, 2020, from https://www.projectcpn.eu55 Papakyriakopoulos, O., Serrano, J.C.M., Hegelich, S. (2020). Political communication on social media: A tale of hyperactive users andbias in recommender systems. Online Social Networks and Media, 15. https://doi.org/10.1016/j.osnem.2019.10005856 WFA and platforms make majorprogress toaddress harmfulcontent. (2020). Retrieved onNovember 13, 2020, from https://wfanet.org/knowledge/item/2020/09/23/WFA-and-platforms-make-major-progress-to-address-harmful-content57 Stolton, S. (2020). EU Rapid Alert System used amid coronavirus disinformation campaign. Retrieved onMay 1, 2020, from  https://www.euractiv.com/section/digital/news/eu-alert-triggered-after-coronavirus-disinformation-campaign/58 Mozilla Insights. (2020). When Content Moderation Hurts. Retrieved onMay 4, 2020, from https://foundation.mozilla. org/en/blog/when-content-moderation-hurts59 Ofcom. (2020). Half of UK adults exposed tofalse claims about coronavirus. Retrieved onMay 1, 2020, from https://www.ofcom.org.uk/about-ofcom/latest/features-and-news/half-of-uk-adults-exposed-to-false-claims-about-coronavirus60 Ofcom. (2019). Use of AI in Online Content Moderation. Cambridge Consultants. 
actually seeing it. The actual efficiency of these techniques also depends onthe human response timetoreview theproposed content.OtherAI-driven methodstotackle malicious online behaviour are toaddress the online audience directly in community management.Such AI nudging  techniques involve notificationsorcommentsby chatbotsthatmake theuseraware thatthepostcontainsharmfulcontent,orthe technologycancauseashort delay in the postingprocess which could encourage the usertorethink hisorhermessage.61 AIsystemscanalsoprovide alternative, more positively expressed content suggestionswhich still resemble the original message. In both instances, the human agent, namely content moderator oruser, takes the ultimate decision. 
The complexity of onlinecontent challenges the technical robustness of AI systems incontentmoderation. First, AI systems facelimitations due tothe large variety of contentformats, suchastext, image, video, andaudiowhich canalsoappearina combination of different formats, such asin GIFs, memes, and emojis in combination with text. Advanced content types such asdeepfakes and live video streams represent a considerablechallengefor humanandalgorithmic contentmoderation.62 Second, contentmoderationoftenrequires evaluation beyond thecontent:itmusttake into account contextual understanding, e.g. societal, cultural, historical, and political aspects, and  metadata , i.e. surrounding online information such as the number of followers and platform activities. Third, the variety of languages and nuances, e.g. sarcasm, represent challenges.Thesepointscreate achallengefor both, algorithmic systems aswell as human agents. However, usershave higher expectations and less tolerance for mistakes inAIratherthanhumanperformance.63 Apoor algorithmic performance canhave direct impact onthe trust of humans in machines. To increase thetechnical robustness, training the AI systems requires large, suitable, and high-quality diverse datasets and constantupdating,which is, however, challenged by complex contextual nuances.In particular, smallernewsrooms face difficulties in keeping up with big tech companies. Data and trained engineers for machine learning tend tobe underrepresented and/or being insufficiently diverse, e.g. on gender, cultural background. In addition, training AI systems substantially affects environmental well-being.An AI training process is highly energyintensive and, hence, incursconsiderableenvironmental costs.Thisleadsto 
64 65
significant sustainability issues. 
61 Statt, N. (2020). Twitter tests awarning message that tells userstorethink offensive replies. Retrieved onMay 5, 2020, from https://www.theverge.com/2020/5/5/21248201/twitter-reply-warning-harmful-language-revise-tweet-moderation62 Ofcom. (2019). Use of AI in Online Content Moderation. Cambridge Consultants.63 Ofcom. (2019). Use of AI in Online Content Moderation. Cambridge Consultants.64 Hao, K. (2019). Training asingle AI model canemitasmuch carbon asfive carsin their lifetimes. Deep learning has a terriblecarbonfootprint. Retrieved onMay 31,2020,from https://www.technologyreview.com/2019/06/06/239031/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes/65 Matheson, R. (2020). Reducing the carbon footpring of artificial intelligence. MIT system cutsthe energy required for training and running neutral networks. Retrieved onMay 31, 2020, from http://news.mit.edu/2020/artificial-intelligence-ai carbon-footprint-0423. bon footpring o 
Transparency andaccountability are two major principles totracealgorithmic decision-making and to counter potential abuse. First, the highly complex architecture ofwell performing AI moderating tools makes itdifficult toanalyse andreveal their decisions-making process.66 Algorithmicstandards thatare notclearly defined and articulated can result in leaving  negative  content online and/or removing  appropriate  content.In this regard, itmustbe transparent who is towhat extentaccountable for algorithmic decisions andjudgments and towhom it mustbe disclosed, e.g. general public,certainsectors,humanagencies and/or oversight bodies.At thesametime, transparency of algorithmic moderating tools towards userscanincrease therelation oftrust between humans and machines. Second, alack of transparency aswell asthe algorithmic system itselfcanbe abused in political online campaigns during election periods, such as it often remains unclear who is paying for it, how much is being spent, and how audiencesare segmented and targeted, e.g. through ads and chatbots.67An abuse of algorithmic filtering systems could further result in censorship which would violatedemocraticprinciples. Inthisregard Tracking Exposed68 andAlgorithms Exposed (ALEX)69 introduced open-source software asalgorithmic auditing methods to tackle the consequences of personalisation algorithms onsocial media and shopping platforms. Their goal is to empower both advanced users and low-skill users in the data extraction and enhance data literacy. 
d. Theme 4: Automating communication The fourth theme, automating communication, includes all forms of interaction and communicative actionsandinfrastructure enabledby AI.Assuch, chatbots, smart speakers, voice assistants, automated marketing communication belong tothis theme. Everything from AI systems that simulate aproper conversation aswell asencoding anddecoding conversational messagesanddatafrom usersfallsunderthistheme. Referring to the  Trustworthy AI  heptagon (Fig. 1), it is expected that the AI technology in this theme further diminishes aspect of human agents and is in favour ofcontent generated by technological systems. 
Themost important key requirements for the automated communication theme are human agency and oversight, diversity, non-discrimination and fairness aswell as transparency.First, transparency would require allAI-empowered communication channels to lay open or make auditable to specialists much of their data infrastructure and thus also how information and output is compiled. Transparency would also enable userstounderstand better how theirconversation data is being used and evaluated. Especially inthe field of automated marketing and communication, userscanfall prey 
66 EPRS. (2019). Understanding algorithmic decision-making: Opportunities and challenges.67 Transparent Referendum Initiative. (2020). Retrieved on May 1, 2020, from http://tref.ie/68 Tracking Exposed. (2020). Retrieved on May 1, 2020, https://tracking.exposed69 Algorithms Exposed. (2020). Retrieved on May 4, 2020, https://algorithms.exposed 
to misleading messages orbiased information, which could be avoided if transparency in marketing practices would be made mandatory. More open and transparent automated communicationtechnologies would ultimately give usersgreater reassurance while open infrastructures and datasets could enable research and generate public interest value. As such, open-source anonymous data may benefit for example the AI-driven development oftranslationservicesfor low-resource languages.Thetransparency principle is therefore closely linked to maintaining privacy and data governance.At the sametime, the trade-off by enhanced transparency could result in abacklash against dataprotection civilsocietygroups advocating for betterprotection ofaggregated datasets. In any case, ensuring multi-stakeholder governance of data and robust privacy measures is also relevantin relation to the data collection and purposes around voice and emotional AI, since it mustbe clear tousershow the information is stored and used in a long-term perspective. 
Referring todiversity, non-discrimination and fairness in automated communication AI systems, open unbiased datasets would not only favour users  communication experience butare also key to not distortacertainconversation orflow of information between humansandmachines.Further, consideringtheEU s linguisticdiversity, automated communicationsystems canalready discriminateordisadvantage certainlinguistic minorities. Finally, usersshould be able to choose whether they want to interact with achatbotorwithahuman being, reflected in the principle human agency and oversight. This also links to the principle of accountability as far as imprecise or wrong information given by aconsumer-oriented chatbot, e.g. for abank,cancauseharmordamage.70As such, the redressing of automated decisions by chatbots should be considered when discussing accountability in automated communication. 
Also,technical robustness is relevant in that regard because the key principle would encouragemore testing and development of automated communication systems prior tomarket themasasolution, by the creators tothe potential customers.Thiswould allow for abetteruserexperienceaswell asmore trustinautomatedAI-enabled communications. 
Thesocietalandenvironmental wellbeing principle alsodemands emphasis on theoverall decision whether it is suitable, viable, sensible, considerate, and sustainable toadaptautomatedcommunicationAIfor acertaincase.Assuch,beneficialcases include automated descriptions of visual content by using object recognition technology for theblindandvision-loss community.71Likewise, automatedcommunicationAI tools develop datasets and intelligent models that automatically translate online content 
70 However, this also applies to wrong information from a human employee, and in both cases the bank is liable anyway.71 Facebook automated alternative text. (2016). Retrieved onApril 20,2020, from https://www.facebook.com/accessibility/ videos/1082033931840331/ 
for native speakers oflow-resource languages72,thereby making important content accessibletolinguistically diverse communities.However, deploying already existing data risks replicating biases and errors from training datasets, e.g. stereotypes, gender and racial biases, especially for fully automated translation AI interfaces. The fact that employment opportunities for translatorsare significantly diminishedby automated communication AI technologies threatens thesocietal wellbeing principle, according to which automated communication AI companies were required tomitigate the impact of their technologies on the traditional job sector. 
Ultimately, automatedcommunicationshouldenhance humanwork which is achieved if the communication flows are still subject tohumanoversight. Delegating the decision to the AI system without human oversight should be avoided. 
To improve the validity and emphasise the significance of the 7 Key Requirements inthefour themesidentified above, thefigure below represents theviewofthe committee. The members assessed the significance of each requirement for Trustworthy AI within thecontext of the four themes of the MTS, i.e. automating data capture and processing, automatingcontentgeneration,automatingcontentmediationand automating communication. While the colours dark red and light red indicate ahigher significance for the corresponding theme, orange, yellow and white reveal slightly lower significance. The view of the committee reveals that each of the 7 Key Requirements hasahigh significance throughout the MTS. This figure identifieshuman agency and oversight, transparency,andaccountability asprevailing requirements throughout the MTS. 
72Assuch,anIrishresearch centre deploys automatedtranslationand naturallanguage processing AIfor low-resource languagestopreserve linguisticandculturalplurality intheEU: ADAPT center. Transforming GlobalContent.(2020). Retrieved on April 20, 2020, from https://www.adaptcentre.ie/research/transforming-global-content/ 

Figure 3: View of the committee onthe significance of the 7 Key Requirements for  Trustworthy AI  in relation 
to the four themes of the Media and Technology Sector 
e. Possible tensions among the 7 Key Requirements for Trustworthy AI 
Technical robustness andsafety; Diversity, non-discrimination andfairness;Societal and environmental well-being 
Possible tensions could arise between technical robustness and safety; diversity, non discrimination and fairness; societal and environmental well-being because the large-scale implementation of AI tools such asholo-lenses for blind people asindicated in theme 1 (automating data capture and processing) still requires more extensive research and testing in order tobedeployed onalargescale. Given thecostsof development, it seemsvery hard toachieve non-discrimination in early adoption. Inorder toavoid longer term discrimination, care should be taken to ensure that products being developed are trialled ondiverse groups. In addition, currently, such advanced AI systems are not yet available for mostoftheblindpopulation, especially for socioeconomically disadvantaged populations. 
Technical robustness andsafety; Human agency andoversight; Accountability; Transparency 
Maximizing efficiency through technical robustness of AI systems incontent generation can create tensions withinthe key requirements. Technical robust AI systems indata analysis can reduce human agency and oversight, accountability, and transparency. Furthermore, technical robust AI systems canbecome so efficient and well-advanced that they, ultimately, replace humans intasks for which it is not necessary ordesirable. This is at odds with diversity, non-discrimination, and fairness and societal wellbeing. 
Transparency; Technical robustness and safety; Privacy and data governance; Societal and environmental wellbeing 
Particularly inalgorithmiccontentmediation,tensions canappearbetween transparency, technical robustness, privacy and data governance, and environmental wellbeing. A good moderating performance of AI systems might be based onacomplex design of AI systems which, eventually, hampers explainability and transparency. Moreover, large datasets including alot of userinformation are applied toincrease the accuracy and efficiency of algorithmic systems. These data setscontain comprehensive information suchaslocation,consumerpreferences, political interests, education and workplace, relationship status,etc.,which underlinesonceagaintheimportanceofprivacy protection and data governance. Moreover, improving the accuracy of AI operations through well-trained system occurs at the expense of environmental wellbeing. 
II. What must the Media and Technology sector do to be compliant with the 7 Key 
Requirements? 
Thissectionsetsoutguidelinesfor theimplementation ofAIintheMedia and Technology sector. Specifically, it recommends how to adhere to the 7 Key Requirements, the Trustworthy AI  heptagon,within the four identified MTS themes automating data capture and processing, automating content generation, automating mediation, and automating communication.Three clusters of recommendations are proposed: addressing data power and positive obligations (oriented mainly at people), empowerment by design and risk assessments(oriented mainly atinfrastructure) andcooperative responsibility and stakeholder engagement (oriented mainly at stakeholders). 
a) Addressing data power and positive obligations 
Key requirements: Privacy and data governance; Human agency and oversight; Transparency 
Aforementioned issues of consent are legitimate, particularly regarding the theme ofautomating data capture and processing.Do customers know when their personal data is being collected by AI-enabled systems? This relates to furthering  data literacy  and  data agency , which meansstimulatingawareness, buildingattitudes,enhancing capabilities and adjusting behaviour among usersregarding (personal) data collection, processing and (re)use inthe area of digital media and technologies.73However, at the sametime, it should be avoided toputtoomuch of the burden onthe shoulders of relatively powerless citizens. It isfirstandforemost thetaskofdatacontrollers to meaningfully explain what is happening with the data. Some usersmay never be fully digitally literate, yet data controllers also need to make clear to them what is going on. Thisrequires more investigation intoexplaining well andmeaningfully thedata capturing, processing and (re)use. This could also meanapositive obligation for AI-driven business to conduct such research onan ongoing basis, ashas been suggested in the past by WP29 in their guidelines onvalid consent, which were recently updated by the European Data Protection Board.74 
Positive dataobligations also enable citizens to act with agency in the face of data power.75Automated datacollectionby AIsystems happensinthebackground, particularly in remote biometric identification datasets and emotion detection AI. This raises, for example, the question if people should be able todecide if and how their emotionscanbe tracked, profiled, and re-used for specific purposes in order to avoid potentially harmful effects. For instance, Spotify s data analytics team conducts studies into musical preferences to profile users, not only to present them with better musical advice. One of Spotify s data analytics goals is to target advertising at users depending onthe mood they are in, which is aplay atmanipulation using people s unconscious vulnerabilities.76 
The meaningful, intentional and informed consent might erode in the presence of AI in the MTS. Users should, therefore, be informed when their volunteered, observed orinferred personal data is being used to train machine learning algorithms, and based onthat decide whether toopt in, which could be described aspositive obligations. 
73 Pierson, J. (forthcoming) Media and Communication Studies, Privacy and Public Values: Future Challenges. In: Gonz lez-Fuster, G., van Brakel, R. and De Hert, P. (eds.) Research Handbook onPrivacy and Data Protection Law: Values, Norms and Global Politics, Cheltenham: Edward Elgar Publishing.74 EDPB (2020). Guidelines 05/2020 on consent under Regulation 2016/679, adopted on May 4, 2020.75 Kennedy, H., Poell, T. & van Dijck, J. (2015) Data and agency. In: Big Data & Society, July-December, 1-7.76 See e.g. https://mitpress.mit.edu/books/spotify-teardown 
Therefore, the Committee recommends ensuring clear and strong consent (opt in) and transparency obligations for algorithmic training and testing with userdata in MTS. This canbe operationalised by for example setting-up algorithmic registries, as done by the cities of Amsterdam and Helsinki.77On top of providing understandable and easily accessible information onautomating data capture and processing to users, the lattermustalso be able tocontactahumantoprovide further information about the aforementioned aspects and users must be guaranteed satisfactory and effective remedies if they have been negatively affected by decisions of AI systems.78The Committee, therefore, recommends responsive redress mechanisms. 
Disclosure of personal data should be ahuman-consented transaction, notone enticedor(unconsciously) demanded by technology. Data minimisation by design as required by theGDPR shouldbeclearly implemented andenforced intheMTS. Companies should be obliged toundergoregular data reviews toensure theyare not  casting their nets fartherthannecessary. Lastly, dataanonymisation oratleast pseudonymisation by design should become a key principle. More research investments by the MTS sector are needed in this field. Pseudonymising data is not only favourable for users, but further mitigates risks arising from data breaches, systemic surveillance and cybercrime. 
Explainability is a complex, nuanced problem, considering the variety of European citizens. Research and funding for increasing AI transparency and explainability should be pursued and prioritized. This should be combined with (co-)regulatory efforts for establishing more transparency from digital platforms vis- -vis independent regulators, onmatterslike internalprocesses for handlingharmfulandillegalcontentthrough algorithms and AI. In that way we canbetter address andregulate the behaviour of platform-specific architectural amplifiers of contentious content, e.g. in recommendation engines,search enginefeatures (such asautocomplete), features like  trending , and other mechanisms that predict what we want toseenext.This approach fits in with suggestions being madeonexanteprinciples-based co-regulatory approaches for addressing online harms asakey operational objective of digital platforms, in away which is reflective of their reach, their technical architecture, their resources, and the risk such content is likely to pose.79Hence, the Committee recommends strengthening research, process-based (co-)regulation and oversight on AI transparency and explainability,especially withregards toarchitectural elementsfor algorithmic amplification. 
77 Moltzau, A. (2020). Algorithm Registries in Amsterdam and Helsinki. Retrieved onNovember 13, 2020, from https://alexmoltzau.medium.com/algorithm-registries-in-amsterdam-and-helsinki-c1364b70ca678 Fanni,R., Steinkogler, V. E., Zampedri, G., & Pierson, J. (2020, September). Active Human Agency in Artificial Intelligence Mediation. In Proceedings of the 6th EAI International Conference onSmart Objects and Technologies for Social Good (pp. 84-89).79 Vermeulen, M. (2019). Online Content: To Regulate or not to Regulate-Is that the Question?. Vermeulen, Mathias, Online content: to regulate or not-is that the question. 
Anticipatory data management policy should be a future priority in EU legislation. Privacy is a moving target, and new categories of personal data will be utilized, collected andcreated. Therefore, it is imperative that GDPR and the ePrivacy directive update consider emerging sensitive AI-related personal identifiers, whether emotional data or even predicted behaviour AI systems foresee an individual taking. 
Individual consent decisions will not prevent all types of societal harms stemming from abusive usesofautomatedpersonaldataprocessing. While individualsmay consent to the useof information about e.g. their emotions, political affiliation, health orsexualorientation,thismay have large-scaleeffects beyond asingle citizen, for which individual choices cannotbearresponsibility. Political microtargeting offers an example: individual users may consent to the use of data about their political preferences andemotional statesonaplatform, butinaggregated form, dataonattitudesand emotions linked to political preferences may be used to automatically manipulate voting behaviour of other citizens with potentially major societal effects, asthe Cambridge Analytica scandalhasillustrated.80 Prevention ofsuchmalignantapplications of automateddataprocessing cannotrest onanindividual s shouldersandshouldbe addressed with regulation based onaninterdisciplinary, multi-stakeholder engagement to uphold public values. 
The Committee recommends multi-stakeholder processes for investigating how predictive analytics, sentiment analysis and emotional AI threaten the integrity and autonomy of digital media users, especially in online behavioural advertising and synthetic content production.Thisapproach isinlinewiththe remit of Art. 22 GDPR ( Automated individual decision-making, including profiling ). 
b) Empowerment by design and risk assessments Key requirements: Human agency and oversight; Transparency; Diversity, non-discrimination and fairness; Societal and environmental well-being; Technical robustness and safety 
AItechnologies being usedfor activitieslike profiling, contentpersonalisation and targeted advertising canpose threats to human agency,totransparency,todiversity, non-discrimination and fairness, to societal well-being, and to technical robustness and safety. 
Therefore, it is important that comprehensive solutions are being investigated and developed to address these threats. This fits inwith the idea of  empowerment by design , 
i.e.building infrastructures and systems insuchaway that(organised) citizens have agency to safeguard and strengthen their fundamental rights and the public interest.81 
80 The Guardian,The Cambridge Analytica Files. Retrieved onNovember 13, 2020, from https://www.theguardian.com/ news/series/cambridge-analytica-files81Pierson,J. andMilan,S.(2017) Empowerment by design: Configuring theagencyofcitizensandactivistsindigitalinfrastructure. Presentation at Communication Policy & Technology section for IAMCR Conference  Transforming Culture, Politics & Communication: New media, new territories, new discourses , 17 July 2017, Cartagena, Colombia. 
The targeted advertising industry in MTS is acomplex and multi-sided market withamultitude of actors, many of whom intermediaries, such asnetworks of third parties with tracking technology, intermediary data brokers, and exchanges all competing in the market of RTB andautomated auctions.82 Sensitive information about individuals canbe inferred and used, e.g. ethnicity, gender, sexual orientation, religious beliefs, for online behavioural advertising and affinity profiling, i.e. grouping people according to their assumed interests rather than their personal traits. Several scholars and digital rights organisations have made suggestions for empowering consumers in case of illegal orunethicalautomatedcapturingandprocessing oftheirpersonaldata.Hence the Committee recommends investigating comprehensive solutions for addressing legalandethicalrisksofautomateddecision-makingandprofiling,likethe  right to reasonable inferences . 
Besides issues of profilingin digital marketing, AI is also used in emotion detection and sentiment analysis in MTS. This canhave positive uses, but it also bears risks to manipulating human behaviour. Thesesystems could powerfully  nudge  people into taking certain behavioural actions; used to infer belief and attitude; and incentivise use orconcealmentofcertainemotionalexpressions. Emotiondetectioncouldlikewise exacerbateexistingbiasesspecifically for vulnerable groups ofthesociety. Asetof actionscouldhelptomitigatetherisksposedby emotiondetectionAI.First, users should have to opt-in if any of their data is being used to detect emotions. The consent by usersshould be mandatory for MTS business, asrequired by EU data protection law. However, consenting to the data collection does not suffice, asthe issue lies with how theresults of data analysis are applied, e.g. avoiding that citizens are manipulated at scale. The (dynamic) consent should be reviewed and renewed onarecurring basis withfulldisclosure over thepurposeandscopeoftheemotionAIimplementation areas, and only for soundreasons suchashealthorsafety. Those developing sentiment analysis and emotion detection AI need tobeurgedtofull transparency and public discussionwithrelevant expertssuchassociologists, psychologists, anthropologists, media scholars and psychiatrists. Overall, the Committee recommends designing an EU-wide, dynamic, and mandatory high-risk assessment scheme for AI systems detecting sentiments fromtheir users, leading to empowerment by design for citizens and society. 
82 Binns, R., Zhao, J., Kleek, M. V., & Shadbolt, N. (2018). Measuring Third-party Tracker Power Across Web and Mobile. ACM Trans. Internet Technol., 18(4), 52:1 52:22. https:// doi.org/10.1145/3176246 
More largely, high-risks assessmentschemes also need toconsider the value of theAI-enabled system(s) against therisks.Thelatteralsorefers tominimising unintentional and unexpected harm, and preventing unacceptable harm, which is related to the principle of technical robustness and safety. Simply put, the value of the service enabled/provided mustoutweigh theriskofthedatacollected.Thus,atheoretical continuum exists where risks associated with disclosure of personal data and reward or value ofreceived product orserviceare balancedcognitively.83 84 Thisapplies in scenarioswhere humans interact with AI systems, such asin the first theme. The AI HLEG Assessment List for Trustworthy Artificial Intelligence (ALTAI) already provides atooltoself-assess compliance of specific AI usecaseswith the 7 Key Requirements for Trustworthy AI.TheCommitteerecommends thattheEU-wide, dynamic, and mandatory high-risk assessment scheme should be coherent with the ALTAI, specifically focussing on the potential risks and societal impacts arising in the MTS.85 
c) Cooperative responsibility and stakeholder engagement Key requirements: Accountability; Societal and environmental well-being; Diversity, non discrimination, and fairness; Transparency 
Many concerns that arise in this sector can only be tackled by means and resources beyond the sector. For instance, social media has enabled targeted harassment of private individuals,which may evade current attemptstoregulate, andsavvy abuserscan readily avoid penalty. Accountability issuescanarise if companies fail to catch up with technology, if the technology orservice provided is ineffective, orif services available only topeople with plenty of resources. Yet, effective legalremedies against abusive individuals could be oneway of helping to prevent blanket social media policies which may have more draconianeffects on freedom of expression. Targeted online harassment ofindividuals needstobetaken more seriously especially consideringtheEU fundamentalhumanrightsframework andlegal obligations. Thesepoliciesshould consider the context since it is vital to communication and hence, apolicy that works in one context in social media could be disastrous in another. 
ICT blurred borders between media production, consumption and literacy. The mosteffective way tosecure societal and environmental well-being shouldbeashared responsibility between civilsociety (users), industry (platforms) andgovernments (education remit). This type of  cooperative responsibility  requires that digital media platforms, policy makers, usersand possible other actors develop adivision of labour 
83 Robinson, C. (2017). Disclosure of personal data in ecommerce: A cross-national comparison of Estonia and the United States. Telematics and Informatics, 34(2), 569-582.84 Petronio, S. (2002). Boundaries of privacy: Dialectics of disclosure. Suny Press.85European Commission. (2020). AssessmentListfor Trustworthy ArtificialIntelligence (ALTAI) for self-assessment. Retrieved onJuly 17, 2020, from https://ec.europa.eu/digital-single-market/en/news/assessment-list-trustworthy-artificial intelligence-altai-self-assessment. 
onhow to manage their responsibility for theirrole regarding public values.86 The EU preliminary principle demands that the MTS canonly be  compliant  in presence ofan oversight body including atransparent system of compliance, anappeal (redress) and a complaintsprocedure. Any such system would also have to acknowledge and interface somehow with legacy governance structures in the MTS. Given the legal obligations in the EU, the Committee recommends setting up an advisory body with all relevant stakeholders involved for feedback and evidence on EU technology policy. 
Providing anoutlook,theNew European Media Initiative (NEM)87isakey European technologyplatform organisationfor theMTS,that sinceFramework Programme 7  is intensely involved in the EU research, thereby driving the future of digital experience. In their  Vision Paper 2030  Towards afuture media ecosystem , NEMaimstounitetheMTSwithEuropean core values, drivers andgoals.Acting ethical,transparent andaccountable,beinghuman-centricandsustainable,and encouraginganempowered and critical society are the main ambitions.88 In line with the 7 Key Requirements for AI in the MTS, the Committee recommends fostering exchanges and best practices with other institutions, network organisations and multi-stakeholder initiatives,for example, NEM,Forum onInformation& Democracy,89 Re-Imagine Europe,90 and the Council of Europe. 
Furthermore, impacts onhumancreativity and societal wellbeing in the media andcreative industry could be serious, e.g. in caseof music creation by AI. Remedies could, for example, include tax channelled to live music venues/music schools, regulators toremove barrierstolive music performance, encouragement of music tuition atall levels of schooling, and open provision of software to educational establishments. This also includes broader support for public service media and creative industry to safeguard creativity and wellbeing. Therefore, the Committee recommends allocating funding to the most severely impacted creative media industries in the EU,especially on cultural and public service/information grounds. 
The MTS and online intermediaries in particular should be encouraged more to setupanappropriate architecture for empowering users.More standardised methodologies and deliberation fora tofacilitate ongoing exchange with the specific user community should be put in place. Also, media production cycles such as designing websites (access, monitoring and dissemination) should involve multiple stakeholders. Likewise, thesamestakeholders shouldbetaughttheessentialsofdiversity, non 
86 Helberger, N., Pierson, J. and Poell, T. (2018) Governing online platforms: from contested to cooperative responsibility. In: The Information Society, 34 (1), 1-14.87 https://nem-initiative.org88 Adzic, J., D Andria, F., Behrmann, M. Boi, S., Castillo, P., Clarke, J., Danet, P-Y., Delaere, S., Fernandez, S. Hrasnica, H. Lippold, S., Matton, M., Men ndez, J.M., De Rosa, S. (2020) NEM Vision 2030: Towards afuture media ecosystem, NEM 
  New European Media, April 2020, 18.89 https://informationdemocracy.org90 https://reimagine-europa.eu 
discrimination, fairness and human rights,asthe ISFE-Council of Europe guidelines to onlinegamedevelopers did.91 The Committee recommends incentivizing and developing educational trajectories, guidelines, training, materials and tools for professional and technical staff (e.g. via online courses or curriculum changes in higher education) tobetter understand and engage with EU fundamental human rights and the principle of trustworthy human-centered AI. 
Example: The PEGI Case 
To present therecommendations inapplied context,thePan European Game Information (PEGI) System demonstrateshow avoluntary regulatory system can work in practice. The system recommends content andage policies for video games. It is pan-European, interacts with other regional systems in Asia and North America, and sits ontop of national governance systems. PEGI is advised by national councils andanexpert advisory board madeupofrepresentatives (e.g. academics, parent bodies, film rating bodies) from around Europe. These all meet with PEGI staff face tofaceonceayear andonlineinbetween theannual meetings.Thecommittee membernamesare published online, which provides transparency. PEGI andits North AmericanandAsianequivalents are working togethertodevelop an International Age Rating Coalition (IACR). 
PEGI is asystem thatresults in information notices onthe back of physical boxed media products and now also in the online app and other stores. Publishers fill outaquestionnaire and send it to PEGI before agame is released. PEGI canrefuse togive arating toagame, ask for clarifications and it canincrease ordecrease a rating on appeal. It also takes complaints directly from the general public. 
The system works reasonably well in termsofahigh level of accountability, but it also has weaknesses. Some online platforms do not participate. How games are ratedandonwhat grounds canbeopaquetothoseoutsideoftheorganisation. Further, the system does not have legislative backing and thus cannot take punitive actions against game companies like the game rating systems for example in Germany and theUK do. Thus, whileunder national legislationit is illegal to sell anover 18 gametoaminor in the UK, this is amatterof national legislation. The system is highly focused on protecting children but less onnegative impactsorprocedures for adultsorother vulnerable populations. Further, it is unclear what impact the system has in practice intermsof purchasing behaviour andgame playing. PEGI is aco-regulatory system, with afocus on educating  consumersbut especially protecting children. For a critical discussion see Felini (2015). 
91 DG of Human Rights and Legal Affairs. (2008). Human rights guidelines for online game providers. Developed by the Council of Europe in co-operation with the Interactive Software Federation in Europe. Retrieved onJune 1, 2020, from https://rm.coe.int/16805a39d3. 
Any system that might emerge may want to consider the rather stronger role and stance taken in some countries in relation to the  traditional media  industries including for example the Press Councils and Press Ombudsman in Ireland which operates to oversee both print and online only news media92 and the communications regulation bodieslike Ofcom intheUKwhich oversee telecomsandbroadcast media.93 Any governance system might also need towork with established worker unions like the National Union of Journalists, both in terms of training and educating journalists, and in terms of whistleblowing and worker rights. In sum, the Committee recommends strengthening workers  rights and public interest values in the media as new AI systems evolve and emerge. 
Public information campaigns and initiatives about the functioning and possible risks of newAI initiatives should be promoted. Assuch, the Media Literacy Initiative94 involves public,commercial andnotfor profit/community organisations tocounter mis- and disinformation around Covid-19 and is running across online and traditional media channels.95 Similar information andpublic communication initiatives are taken at European level including of course Safer Internet Day.96 The Committee recommends extending existing publicly supported media, data and AI literacy programmes to include information and public awareness of AI applications, services and impacts. 
5. Conclusion 
Artificial intelligence systems have a substantial impact on various areas of the European mediaandtechnologysector(MTS). Thisreport identifiedfour themesofAI applications intheMTS:automating data capture and processing, automating content generation, automating content mediation, and automating communication.Thisreport analysed thecore opportunitiesandrisksofAIapplications withintheseproposed themes.The7Key Requirements for Trustworthy AIdeveloped by theEuropean CommissionHigh-Level ExpertGroup onAIwere atthecentre ofdiscussion.The report addresses itsrecommendations to all stakeholders involved in the development, deployment, use, and governance of AI systems in the MTS. 
92PressCouncilofIreland. OfficeofthePress Ombudsman(2020). Retrieved onJune 1, 2020, from https://www. presscouncil.ie/.93 Ofcom. (2020). TV, radio and on-demand. Retrieved onJune 1, 2020, from https://www.ofcom.org.uk/tv-radio-and-on demand. 94 Be smart media. An Initiative of Media Literacy Ireland. (2020). Members. Retrieved on June 1, 2020, from https://www. bemediasmart.ie/members.95 Be smartmedia. An Initiative of Media Literacy Ireland. (2020). About. Retrieved onJune 1, 2020, from https://www. bemediasmart.ie/about.96 Be smart media. An Initiative of Media Literacy Ireland. (2020). Members. Retrieved on June 1, 2020, from https://www. bemediasmart.ie/members. 
Recommendation cluster 1: Addressing data power and positive obligations 
  
Ensuringclearandstrong consent(opt-in) andtransparency obligationsfor algorithmic training and testing with user data in MTS. 

  
Establishingresponsive redress mechanisms,sothatuserscancontact humans to provide understandableandeasily accessibleinformation onautomatingdata capture andprocessing, andhave satisfactoryandeffective remedies when negatively affected by AI decisions. 

  
Strengthening research, process-based (co-)regulation andoversight onAI transparency andexplainability,especially withregards architectural elements for algorithmic amplification. 

  
Ensuringamulti-stakeholder process for investigating how predictive analytics, sentimentanalysis andemotionalAIthreaten theintegrityandautonomy of digitalmediausers,especially inonlinebehavioural advertising andsynthetic content production. 


Recommendation cluster 2: Empowerment by design and risk assessments 
  
Investigating comprehensive solutionsfor addressing legal andethicalrisksof automated decision-making and profiling, like the  right to reasonable inferences . 

  
DesigninganEU-wide, dynamic, andmandatory high-risk assessmentscheme for AI systems detecting sentiments from their users, leading toempowerment by design for citizens and society. 

  
The EU-wide, dynamic, and mandatory high-risk assessmentscheme should be coherent with the ALTAI, specifically focussing on the potential risks and societal impacts arising in the MTS. 


Recommendation cluster 3: Cooperative responsibility and stakeholder engagement 
  
Setting up anadvisory body with all relevant stakeholdersinvolved for feedback and evidence on EU technology policy. 

  
Fostering exchanges andbestpracticeswithotherinstitutions,network organisations and multi-stakeholder initiatives. 

  
Allocating funding to the most severely impacted creative media industries in the EU, especially on cultural and public service/information grounds. 

  
Incentivizinganddeveloping educationaltrajectories,guidelines,training, materials and toolsfor professional and technical staff to better understand and engagewithEUfundamentalhumanrightsandtheprinciple oftrustworthy human-centered AI. 

  
Facilitating and strengthening workers  rights and public interest values in the media as new AI systems evolve and emerge. 

  
Extending existing publicly supported media, data and AI literacy programmes toincludeinformation andpublicawareness ofAIapplications, servicesand impacts. 


Thereport concludes by emphasising the involvement of public, private, scientific and civil society stakeholders in order to achieve a holistic AI governance framework across the EU. 
Thisreport and especially the proposed recommendations aim to tackle concernsthat arise due to the proliferation of AI systems in the MTS, thereby ensuring an ethical and sustainable AI implementation throughout this sector. As such, public, private and civil society organisations representing the media and technology sector in Europe, aswell asotherinstitutionsinEurope are encouragedtoconsultthisreport andactively implement the proposed recommendations. 
Acknowledgements 
We thank Valerie Eveline Steinkogler and Rosanna Fanni for their assistance with the development of the project and for contributingtothe draft report, aswell asGiulia Zampedrifor thesupportinthefinalisationofthereport. Inaddition,we are also grateful to Ana Pop Stefanija and Ine van Zeeland for their revisions and input, in their capacityasPhDresearchers for respectively theFWO Research Project DELICIOS (Delegation of Decision-Making toAutonomous Agents in Socio-Technical Systems) (https://coast.uni.lu/delicios) andtheVUBResearch Chair DataProtection onthe Ground  (www.dataprotectionontheground.be). 

Atomium-European Institute for Science, Media and Democracy (EISMD),convenesleadingEuropean universities, media, businesses,governments and policymakers to increase theexchange of information and interdisciplinary collaboration, to develop innovative collaborative initiatives and to encourage frontier thinking about science, media and democracy. 
Atomium-EISMD was launched publicly by the former PresidentofFrance Val ry Giscard d Estaing, MichelangeloBaracchiBonviciniandbytheleaders of the institutions engaged during the first conference onthe 27 November 2009 attheEuropeanParliamentinBrussels. 

With the contribution of: 





