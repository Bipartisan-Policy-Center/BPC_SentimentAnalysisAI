

TABLE OF CONTENTS 

SUMMARY OF UPDATES ................................................................................... 4 
FOREWORD .......................................................................................................... 7 
1.
 PREAMBLE ........................................................................................................ 9 

2.
 INTRODUCTION ............................................................................................... 12 Objectives ............................................................................................................... 13 Guiding Principles for the Model Framework ....................................................... 15 Assumptions ........................................................................................................... 17 


Definitions ....... ........................................................................................................  18 
3. MODEL AI GOVERNANCE FRAMEWORK .................................................. 19 Internal Governance Structures and Measures  ...................................................... 21 
Determining the Level of Human Involvement in AI-augmented Decision-making ..... 28 
Operations Management .......................................................................................  35 Stakeholder Interaction and Communication ........................................................ 53 
ANNEX A 
For Reference: A Compilation of Existing AI Ethical Principles ............................. 64 
ANNEX B 
Algorithm Audits ...................................................................................................... 67 
ACKNOWLEDGEMENTS ..................................................................................... 68 


SUMMARY OF UPDATES 

DATE 
EDITION SUMMARY 
RELEASED 
FIRST  23 January  Released the Model AI Governance Framework (First  
2019  Edition) at the 2019 World Economic Forum Annual  
Meeting in Davos, Switzerland.  

SECOND  21 January 2020  Released the Model AI Governance Framework (Second Edition) at the 2020 World Economic Forum Annual Meeting in Davos, Switzerland.  
The key changes include: • Addition of industry examples in each section to illustrate how organisations have implemented AI governance practices in that section; • Updating the titles of two sections to accurately reflect their content:  
» “Determining AI Decision-Making Model” to “Determining the level of human involvement in AI-augmented decision-making”; » “Customer Relationship Management” to “Stakeholder interaction and communication”.  
Section-specific changes include the following:  
Determining the level of human involvement in AI- 
augmented decision-making • Clarified the “human-over-the-loop” approach by explaining the human’s supervisory role in AI-augmented decision-making. • Clarified that organisations can consider other factors such as the nature and reversibility of harm and operational feasibility in determining the level of human involvement in an organisation’s decision-making process involving AI.  

DATE 
EDITION SUMMARY 
RELEASED 
SECOND  21 January 2020  Operations management • Provided guidance to organisations to adopt a risk- 
based approach when implementing measures by:  
» Identifying features or functionalities with the  
greatest impact on stakeholders;  
» Considering which measure would be most  
effective in building trust with stakeholders. • Provided guidance on the necessity and relevance  
of the various measures:  
» Clarified that datasets used for building AI  
models may include both personal and non- 
personal data;  
» Included new measures such as robustness,  
reproducibility and auditability and provided  
examples of helpful practices for these  
measures.  
Stakeholder interaction and communication  
• Highlighted the importance of communication  
with various internal and external stakeholders.  
• Highlighted the need to consider the purpose  
and context when interacting with the various  
stakeholders.  
• Provided suggestions on the level of information  
to be provided when interacting with various  
stakeholders.  
Annex A – For reference: a compilation of existing  
AI ethical principles (Annex A) • Clarified that the list of AI ethical principles  
provided is a compilation of existing AI principles  
that is for reference only. Not all listed principles  
are addressed in the Model AI Governance  
Framework. Organisations could consider  
incorporating other principles in Annex A into  
their own corporate principles.  


DATE 
EDITION SUMMARY 
RELEASED 
SECOND 21 January Annex B – Algorithm Audits 2020 • Clarified that an algorithm audit is to be conducted only if it is necessary to discover the actual operations of algorithms comprised in models, and only at the request of a regulator (as part of a forensic investigation).  
Annex C – Use Case • Annex C has been removed. Instead, a separate Compendium of Use Cases has been published (go.gov.sg/ai-gov-use-cases).  


FOREWORD 

In 2019, the world saw significant advances in the sophistication and pervasive use of artificial intelligence (“AI”). For instance, we witnessed the emergence of next-generation AI-powered natural text generators like GPT-2, which can 
generate passages that are difficult to distinguish from human writing. We 
also saw the development of Dactyl, a robotic hand, which uses reinforcement learning to grasp and manipulate common household objects with human-like dexterity. These examples attest to the speed of AI’s advancement and how it will become ubiquitous in our daily lives. 
The discourse on AI ethics and governance has also moved forward. Over the last two years, governments and international organisations have begun issuing principles, frameworks and recommendations on AI ethics and governance. In January 2019, Singapore launched our Model AI Governance Framework (“Model Framework”) at the World Economic Forum in Davos. The Model Framework’s unique contribution to the global discourse on AI ethics lies in translating ethical principles into practical recommendations that organisations could readily adopt to deploy AI responsibly. We are heartened by the diversity of organisations that have adopted the practices outlined in the Model Framework, which underscores its ease-of-use and relevance. 
Singapore is proud to launch the second edition of the Model Framework. This edition incorporates the experiences of organisations that have adopted AI, and feedback from our participation in leading international platforms, such as the European Commission’s High-Level Expert Group and the OECD Expert Group on AI. Such input has enabled us to provide clearer and effective guidance for organisations to implement AI responsibly. 

Singapore’s Info-communications Media Development Authority (“IMDA”) and Personal Data Protection Commission (“PDPC”) have also partnered the World Economic Forum Centre for the Fourth Industrial Revolution to develop an Implementation and Self-Assessment Guide for Organisations (“ISAGO”). The ISAGO complements the Model Framework by allowing organisations to assess the alignment of their AI governance practices with the Model Framework, while providing useful industry examples and practices. We are also publishing a Compendium of Use Cases, which features real-world examples of how organisations have implemented or aligned their AI governance practices with the Model Framework. Together, these initiatives 
enable any organisation to establish and refine its AI governance practices in 
concrete and practical ways. 
These initiatives play a critical role in Singapore’s National AI Strategy. They epitomise our plans to develop a human-centric approach towards 
AI governance that builds and sustains public trust. They also reflect our 
emphasis on co-creating an AI ecosystem in a collaborative and inclusive manner. The Model Framework and ISAGO will pave the way for future developments, such as the training of professionals on ethical AI deployment, and laying the groundwork for Singapore, and the world, to better address AI’s impact on society. 
The steps we take today will leave an indelible imprint on our collective future. 
The Model Framework has been recognised as a firm foundation for the 
responsible use of AI and its future evolution. We will build on this momentum to advance a human-centric approach to AI – one that facilitates innovation and safeguards public trust – to ensure AI’s positive impact on the world for generations to come. 
S Iswaran 
Minister for Communications and Information Singapore January 2020 



1. PREAMBLE 


1.1 The Model Framework focuses primarily on four broad areas: internal governance structures and measures, human involvement in AI-augmented decision-making, operations management and stakeholder interaction and communication. 
While the Model Framework is certainly not limited in ambition, it is ultimately limited by form, purpose and practical considerations of scope. With that in mind, several caveats bear mentioning. The Model Framework is – 

1.2 It is recognised that there are a number of issues that are closely interrelated to the ethical use and deployment of AI. This Model Framework does not focus on these specific issues, which are often sufficient in scope to warrant separate study and treatment. 
Examples of these issues include: 




2. INTRODUCTION 

OBJECTIVES 
2.1 The exponential growth in data and computing power has fuelled the advancement of data-driven technologies such as AI. AI can be used by organisations to provide new goods and services, boost productivity, enhance competitiveness, ultimately leading to economic growth and a better quality of life. As with any new technology, however, AI also introduces new ethical, legal and governance challenges. These include risks of unintended discrimination potentially leading to unfair outcomes, as well as issues relating to consumers’ knowledge about how AI is involved in making significant or sensitive decisions about them. 
2.2 The PDPC,1 with advice from the Advisory Council, proposes this second edition of the living and voluntary Model Framework as a general, ready-to-use tool to enable organisations that are deploying AI solutions at scale to do so in a responsible manner. This Model Framework is not intended for organisations that are deploying updated commercial off-the-shelf software packages that happen to now incorporate AI in their feature set. 
2.3 This voluntary Model Framework provides guidance on the key issues to be considered and measures that can be implemented. Adopting this Model Framework will require tailoring the measures to address the risks identified for the implementing organisation. The Model Framework is intended to assist organisations to achieve the following objectives: 


1  Under section 5 of Singapore’s Personal Data Protection Act 2012, the IMDA is designated as the PDPC. 

2.4 To assist organisations in implementing the Model Framework, the PDPC has also prepared a complementary ISAGO. The ISAGO helps organisations assess the alignment of their AI governance practices and processes with the Model Framework. It also provides additional useful industry references and examples that further clarify the recommendations set out in this Model Framework. 
2.5 The extent to which organisations adopt the recommendations in this Model Framework depends on several factors, including the nature and complexity of the AI used by organisations, the extent to which AI is employed in the organisations’ decision-making, and the severity and probability of the impact of the autonomous decision on individuals. 
2.6 To elaborate: AI technologies may be used to augment a human decision-maker or to autonomously make a decision. For instance, the impact of an autonomous decision in medical diagnosis is arguably greater than that in a product recommendation. The commercial risks of AI deployment is therefore proportionate to the impact on individuals. Generally, where the cost of implementing AI technologies in an ethical manner outweighs the expected benefits, organisations should consider whether alternative non-AI solutions should be adopted. The considerations and recommendations set out in this Framework are intended to guide organisations that have decided to deploy AI technologies at scale. 


GUIDING PRINCIPLES 
2.7 The Model Framework is based on two high-level guiding principles that promote trust in AI and understanding of the use of AI technologies: 



Organisations should ensure that AI decision-making processes are explainable, transparent and fair, while AI solutions should be human-centric. 



2.8 Like other technologies, AI aims to increase human productivity. However, unlike earlier technologies, some aspects of autonomous predictions or decisions made by AI may not be fully explainable. As AI technologies can make decisions that affect individuals, or have a significant impact on society, markets or economies, organisations should consider using this Model Framework to guide their deployment of AI. 
2.9 Organisations should detail a set of ethical principles when they embark on deployment of AI at scale within their processes or to empower their products and/or services. Where necessary, organisations may wish to refer to the compilation of AI ethical principles in Annex A. As far as possible, organisations should also review their existing corporate values and incorporate the ethical principles that they have articulated. Some of the ethical principles (e.g. safety) may be articulated as risks that can be incorporated into the corporate risk management framework. The Model Framework is designed to assist organisations by incorporating ethical principles into familiar and pre-existing corporate governance structures, and thereby aid in guiding the adoption of AI in an organisation. 



ASSUMPTIONS 
2.10 The Model Framework aims to discuss good data management practices in general. The Model Framework is mainly applicable to machine learning models (as compared to pure decision tree-driven AI models). 
2.11 The Model Framework does not address the risk of catastrophic failure due to cyber-attacks on an organisation heavily dependent on AI. Organisations remain responsible for ensuring the availability, reliability, quality and safety of their products and services, regardless of whether AI technologies are used. 
2.12 Adopting this voluntary Model Framework will not absolve organisations from compliance with current laws and regulations. However, as this is an accountability-based framework, adopting it will assist organisations in demonstrating that they had implemented accountability-based practices in data management and protection, e.g. the PDPA and OECD Privacy Principles. 
2.13 Further, it should be noted that certain industry sectors (such as in the finance, healthcare, and legal sectors) may be regulated by existing sector-specific laws, regulations or guidelines relevant to the sector. For example, the Monetary Authority of Singapore published the Principles to Promote Fairness, Ethics, Accountability and Transparency in the Use of Artificial Intelligence and Data Analytics in Singapore’s Financial Sector (the “FEAT Principles”) to provide guidance to firms that use AI and data analytics to offer financial products and services.2 Organisations are advised to remain mindful of such laws, regulations and guidelines, as adopting the Model Framework does not mean that organisations are in compliance with such sector-specific laws, regulations or guidelines. 
2  Monetary Authority of Singapore, “Principles to Promote Fairness, Ethics, Accountability and Transparency (FEAT) in the Use of Artificial Intelligence and Data Analytics in Singapore’s Financial Sector” (12 November 2018) <https://www.mas.gov.sg/publications/monographs    or-information-paper/2018/FEAT>. 




DEFINITIONS 
2.14 The following simplified diagram depicts the key stakeholders in an AI adoption process discussed in the Model Framework. The adoption process does not distinguish between business-to-consumer (“B2C”), business-to-business (“B2B”), and business-to-business-to-consumer (“B2B2C”) relationships. 

AI Solution Providers Organisations Individuals 

2.15 Some terms used in AI may have different definitions depending on context and use. The definitions of some key terms used in 
this Model Framework are as follows: 






3. MODEL AI 
GOVERNANCE FRAMEWORK 



MODEL AI GOVERNANCE FRAMEWORK 
3.1 This Model Framework comprises guidance on measures promoting the responsible use of AI that organisations should adopt in the following key areas: 

a. Internal governance structures and measures Adapting existing or setting up internal governance structure and measures to incorporate values, risks, and responsibilities relating to algorithmic decision-making. 
b. Determining the level of human involvement in AI-augmented decision-making 
A methodology to aid organisations in setting its risk appetite for use of AI, i.e. determining acceptable risks and identifying an appropriate level of human involvement in AI-augmented decision-making. 

c. Operations management Issues to be considered when developing, selecting and maintaining AI models, including data management. 
d. Stakeholder interaction and communication Strategies for communicating with an organisation’s stakeholders, and the management of relationships with them. 

3.2 Organisations adopting this Model Framework may find that not 
all elements are relevant. This Model Framework is meant to be 
flexible, and organisations can adapt the Model Framework to 
suit their needs and adopting those elements that are relevant. 

3.3 To help organisations better understand the Model Framework, we have included (in each section) illustrations demonstrating how real-world companies have implemented certain practices described in that specific section. In addition, the PDPC has also released a Compendium of Use Cases that illustrates how various local and international organisations have put in place AI governance practices that are aligned to all sections of the Model Framework. 



INTERNAL GOVERNANCE STRUCTURES AND MEASURES 
3.4 This section is intended to guide organisations in developing appropriate internal governance structures that allow organisations to have appropriate oversight over how AI technologies are brought into their operations and/or products and services. 
3.5 Internal governance structures and measures help to ensure robust oversight over an organisation’s use of AI. The organisation’s existing internal governance structures can be adapted, and/or new structures can be implemented if necessary. For example, risks associated with the use of AI can be managed within the enterprise risk management structure, while ethical considerations can be introduced as corporate values and managed through ethics review boards or similar structures. 


Ethical considerations can be introduced as corporate values and managed through ethics review boards or similar structures. 


3.6 Organisations may also consider determining the appropriate features in their internal governance structures. For example, when relying completely on a centralised governance mechanism is not optimal, a de-centralised one could be considered to incorporate ethical considerations into day-to-day decision-making at the operational level, if necessary. The sponsorship, support and participation of the organisation’s top management and its board of directors in the organisation’s AI governance are crucial. 


3.7 Organisations may wish to consider including features that are relevant to the development of their internal governance structure, such as: 
1. 
Clear roles and responsibilities for the ethical deployment of AI 

2. 
Risk management and internal controls 

















DETERMINING THE LEVEL OF HUMAN INVOLVEMENT IN AI­AUGMENTED DECISION-MAKING 
3.8 This section is intended to help organisations determine the appropriate extent of human oversight in AI-augmented decision-making. 
3.9 Having clarity on the objective of using AI is a key first step in determining the extent of human oversight. Organisations can start by deciding on their commercial objectives of using AI (e.g. ensuring consistency in decision-making, improving operational efficiency and reducing costs, or introducing new product features to increase consumer choice). These commercial objectives can then be weighed against the risks of using AI in the organisation’s decision-making. This assessment should be guided by organisations’ corporate values, which in turn, could reflect the societal norms or expectations of the territories in which the organisations operate. 


Before deploying AI solutions, organisations should decide on their commercial objectives of using AI, and then weigh them against the risks of using AI in the organisation’s decision-making. 


3.10 It is also desirable for organisations operating in multiple countries to consider the differences in societal norms, values and/or expectations, where possible. For example, gaming advertisements may be acceptable in one country but not in another. Even within a country, risks may vary significantly depending on where AI is deployed. For example, risks to individuals associated with recommendation engines that promote products in an online mall or automating the approval of online applications for travel insurance may be lower than the risks associated with algorithmic trading facilities offered to sophisticated investors. 
3.11 Some risks to individuals may only manifest at group level. For example, widespread adoption of a stock recommendation algorithm might cause herding behaviour, increasing overall market volatility if sufficiently large numbers of individuals make similar decisions at the same time. In addition to risks to individuals, other types of risks may also be identified (e.g. risk to an organisation’s commercial reputation). 
3.12 Organisations’ weighing of their commercial objectives against the risks of using AI should ideally be guided by their corporate values. Organisations can assess if the intended AI deployment and the selected model for algorithmic decision-making are consistent with their own core values. Any inconsistencies and deviations should be conscious decisions made by organisations with a clearly defined and documented rationale. 
3.13 As identifying commercial objectives, risks and determining the appropriate level of human involvement in AI-augmented decision-making is an iterative and ongoing process, it is desirable for organisations to continually identify and review risks relevant to their technology solutions, mitigate those risks, and maintain a response plan should mitigation fail. Documenting this process through a periodically reviewed risk impact assessment helps organisations develop clarity and confidence in using the AI solutions. It will also help organisations respond to potential challenges from individuals, other organisations or businesses, and regulators. 





WHAT ARE THE THREE BROAD APPROACHES OF HUMAN INVOLVEMENT IN AI-AUGMENTED DECISION-MAKING? 
3.14 Based on the risk management approach described above, the Model Framework 
identifies three broad approaches to classify the various degrees of human oversight 
in the decision-making process: 

3.15 The Model Framework also proposes a design framework (structured as a matrix) to help organisations determine the level of human involvement required in AI-augmented decision-making. This design framework is structured along two axes: the (a) probability; and (b) severity of harm to an individual (or organisation) as a result of the decision made by an organisation about that individual (or organisation). 
3.16 The definition of “harm” and the computation of probability and severity will depend on the context and vary from sector to sector. For example, the considerations of a hospital regarding the harm associated with a wrong diagnosis of a patient’s medical condition will differ from the considerations of a clothing store’s regarding the harm associated with a wrong product recommendation for apparels. 


3.17 The matrix, however, should not be taken to imply that the probability of harm and severity of harm are the only factors to be considered in determining the level of human oversight in an organisation’s decision-making process involving AI (although they are generally two of the more important factors).4 
3.18 For safety-critical systems, it would be prudent for organisations 
to ensure that a person be allowed to assume control, with the 
AI system providing sufficient information for that person to make 
meaningful decisions or to safely shut down the system where human control is not possible. 

4 Other factors that organisations in various contexts may consider relevant, could also include: 
(a) the nature of harm (i.e. whether the harm is physical or intangible in nature); (b) the reversibility of harm, and as a corollary to this, the ability for individuals to obtain recourse; and (c) whether it is operationally feasible or meaningful for a human to be involved in a decision-making process (e.g. having a human-in-the-loop would be unfeasible in high-
speed financial trading, and be impractical in the case of driverless vehicles). 
USING THE PROBABILITY-SEVERITY OF HARM MATRIX 
An online retail store wishes to use AI to fully automate the recommendation of food products to individuals based 
HIGHLY RECOMMENDED! 

on their browsing behaviours and purchase histories. The automation will meet the organisation’s commercial 
objective of operational efficiency. 
Probability-severity assessment 
The definition of harm can be the impact of making product recommendations that do not address the perceived needs of the individuals. The severityof harm in making the wrong product recommendations to individuals may be low since individuals ultimately decide whether to make the purchase. The 
probability of harm may be high or low depending on the efficiency and efficacy of the AI solution. 
Degree of human intervention in decision-making process 
Given the low severity of harm, the assessment points to an approach that requires no human intervention (i.e. human-out-of-the-loop). 
High severity 
High severity Low probability 
High probability 

Low severity 
Low severity Low probability 
High probability 
Human-out­of-the-loop 
Probability of Harm 
Regular review 

The organisation regularly reviews its approach (i.e. human-out-of-the-loop)  to re-assess the severity and probability of harm, and as societal norms and values evolve. 
Note:This is a simple illustration using bright-line norms and values. Organisations can consider testing this method of determining the AI decision-making model against cases with more challenging and complex ethical dilemmas. 
Severity of Harm 







OPERATIONS MANAGEMENT 


3.19 This section is intended to help organisations adopt responsible measures in the operations aspect of their AI adoption process. A reference AI adoption process is set out in order to provide a context for the recommendations for good governance in respect of the organisation’s data, algorithm and AI model. 
3.20 The Model Framework uses the following generalised AI model development and deployment process to describe phases in implementing an AI solution by an organisation.5 It should be noted that this process is not always uni-directional – it can, and usually is, a continuous process of learning. 

Stage 1: Stage 2: Stage 3: Raw data is formatted Models are trained on The chosen model is and cleansed so the dataset and used to produce conclusions can be algorithms may be probability scores that drawn accurately. applied. This includes can be incorporated Generally, accuracy and statistical or machine into applications to insights increase with learning models offer predictions, make relevance and the including decision trees decisions, solve amount of data. and neural networks. The problems and trigger 
results are examined and actions. 
models are iterated until 
the most appropriate 
model emerges. 
Iterate until data is ready Iterate for most appropriate model 


5 

Adapted from “Machine learning at scale” Microsoft Azure (2 December 2018) <https:// docs.microsoft.com/en-us/azure/architecture/data-guide/big-data/machine-learning-at­
scale> (accessed December 2019). 


3.21 During deployment, algorithms such as linear regression algorithms, decision trees, or neural networks are applied for analysis on training datasets. The resulting algorithmic models are examined and algorithms are iterated until a model that produces the most appropriate results for the use case emerges. This model and its results are then incorporated into applications to offer predictions, make decisions, solve problems and trigger actions. The intimate interaction between data and algorithm/ model is the focus of this part of the Model Framework. 
DATA FOR MODEL DEVELOPMENT 
3.22 Datasets used for building models may come from multiple sources, and could include both personal and non-personal data. The quality and selection of data from each of these sources are critical to the success of an AI solution. If a model is built using biased, inaccurate or non-representative data, the risks of unintended discriminatory decisions from the model will increase. 


To ensure the effectiveness of an AI solution, relevant departments within the organisation with responsibilities over quality of data, model training and model selection must work together to put in place good data accountability practices. 


3.23 The persons who are involved in training and selecting models for deployment may be internal staff or external service providers. It is ideal for the models deployed in an intelligent system to have an internal departmental owner, who will be the one making decisions on which models to deploy. To ensure the effectiveness of an AI solution, it would be helpful for relevant departments within the organisation with responsibilities over quality of data, model training and model selection to work together to put in place good data accountability practices. These include the following: 






3.24 Even if only non-personal data are used for the training of AI models (including personal data that has been anonymised), the good data accountability practices above remain relevant. 







ALGORITHM AND MODEL 
3.25 AI systems may have numerous features or functionalities enabled through algorithms in AI models. Measures such as explainability, repeatability, robustness, regular tuning, reproducibility, traceability, and auditability can enhance the transparency of algorithms found in AI models. It may not be feasible or cost-effective to implement even the most essential of these measures for all algorithms. 
Organisations are encouraged to take a risk-based approach in making a two-fold assessment. First, identify the subset of features or functionalities that have the greatest impact on stakeholders for which such measures are relevant. Second, identify which of these measures will be most effective in building trust with their stakeholders. Some of these measures like explainability (or repeatability, when using models that are not 
easily explained), robustness and regular tuning are sufficiently 
essential that they could, to varying extents, be incorporated as part of the organisation’s AI deployment process. Other measures, such as reproducibility, traceability and auditability 
are more resource-intensive and may be relevant for specific features or in specific scenarios. 


Explainability 
3.26 Explainability is achieved by explaining how deployed AI models’ algorithms function and/or how the decision-making process incorporates model predictions. The purpose of being able to explain predictions made by AI is to build understanding and trust. An algorithm deployed in an AI solution is said to be explainable if how it functions and how it arrives at a particular prediction can be explained. When an algorithm cannot be explained, understanding and trust can still be built by explaining how predictions play a role in the decision-making process. 
3.27 Organisations deploying AI solutions are recommended to adopt the following practices: 



3.28 Technical explainability may not always be enlightening, especially to the man on the street. Implicit explanations of how the AI models’ algorithms function may be more useful than explicit descriptions of the models’ logic. For example, providing an individual with counterfactuals (such as “you would have been approved if your average debt was 15% lower”) and/or comparisons (such as “these are users with similar profiles to yours that received a similar decision”) can be a powerful type of explanation that organisations could consider. 
3.29 Nevertheless, there may be scenarios where it might not be practical or reasonable to provide information in relation to an algorithm. For example, disclosing algorithms deployed for anti-money laundering detection, information security, and fraud prevention may allow bad actors to avoid detection; likewise, providing detailed information about proprietary algorithms or the decisions made by the algorithms may expose confidential business information. 
These tools are known as “supplementary” as there is at present no single comprehensive technical solution for making AI models explainable. These tools thus play a supplementary role in providing some level of interpretability on an AI model’s operation. Examples of these tools include the use of surrogate models, partial dependence plots, global variable importance/interaction, sensitivity analysis, counterfactual explanations, or Self-Explaining and Attention-Based Systems. 


Repeatability 
3.30 Where explainability cannot practicably be achieved given the state of technology, organisations can consider documenting the repeatability of results produced by the AI model. Repeatability refers to the ability to consistently perform an action or make a decision, given the same scenario. While repeatability (of results) is not equivalent to explainability (of algorithm), some degree of assurance of consistency in performance could provide AI 
users with a larger degree of confidence. Helpful practices include: 


7 James Manyika, Jake Sitberg, and Brittany Presten, “What Do We Do About the Biases in AI?” Harvard Business Review (25 October 2019) <https://hbr.org/2019/10/what-do-we­do-about-the-biases-in-ai> (accessed 31 October 2019). 
Robustness 
3.31 Robustness refers to the ability of a computer system to cope with errors during execution and erroneous input, and is assessed by the degree to which a system or component can function correctly in the presence of invalid input or stressful environmental conditions. Ensuring that deployed models are sufficiently robust will contribute towards building trust in the AI system. 
3.32 The concept of robustness arises because it is not possible for models to be able to enumerate and set out all preconditions and consequences for an action. This creates the possibility of models producing insensible or unexpected results even with minor modifications to input data (that may not even be perceptible to humans). Testing for robustness can be achieved through scenario-based testing for foreseeable erroneous input.8 To ensure that models are more robust, organisations can consider working with AI developers to conduct adversarial testing on their models to ensure that their models are able to handle a broader range of unexpected input variables (especially for public-facing AI systems). As this is a resource-intensive exercise, organisations can take a risk-based approach towards identifying the subset of AI-powered features in their products or services that requires adversarial testing. 
3.33 No model can be perfectly robust as it is not possible to detect 
all possible modifications to a set of input data. For this reason, 
organisations intending to use continual learning (i.e. where the 
learned parameters of a machine learning model are not fixed 
but the model continues to change its learned parameters after being deployed into production) are encouraged to be aware of the risks of doing so, should the continual learning model behave in an unpredictable manner. 

This is distinct from user acceptance testing (“UAT”), which is a process where actual software users test a piece of software to ensure that it can handle required tasks in real-
world scenarios, based on specifications. UAT is often a critical step that is taken before 
a newly-developed software is released to the market. 


Regular tuning 
3.34 Establishing an internal policy and process to perform regular model tuning is effective for ensuring that deployed models cater for changes to customer behaviour over time. This allows organisations to refresh models based on updated training datasets that incorporate new input data. Model tuning may also be necessary when commercial objectives, risks, or corporate values change. 
3.35 Wherever possible, testing should reflect the dynamism of the planned production environment. To ensure safety, testing may need to assess the degree to which an AI solution generalises well and fails gracefully. For example, a warehouse robot tasked with avoiding obstacles to complete a task (e.g. picking up packages) could be tested with different types of obstacles and realistically varied internal environments (e.g. workers wearing a variety of different coloured shirts). Otherwise, models risk learning regularities in the environment that do not reflect actual production environment conditions (e.g. assuming that all humans that it must avoid will be wearing white lab coats). Once AI models are deployed in the real-world environment, active monitoring, review and tuning are advised. 
Traceability 
3.36 An AI model is considered to be traceable if (a) its decisions, and 
(b) the datasets and processes that yield the AI model’s decision (including those of data gathering, data labelling and the algorithms used), are documented in an easily understandable way. The former refers to traceability of AI-augmented decisions, while the latter refers to traceability in model training. Traceability facilitates transparency and explainability, and is also helpful for other reasons. First, the information might also be useful for troubleshooting, or for an investigation into how the model was functioning or why a particular prediction was made. Second, the traceability record (in the form of an audit log) can be a source of input data that can be used as a training dataset in the future. 

3.37 Practices that organisations may consider to promote traceability include: 
a. 
Building an audit trail to document the model training and AI-augmented decision. 

b. 
Implementing a black box recorder that captures all input data streams. For example, a black box recorder in a self-driving car tracks the vehicle’s position and records when and where the self-driving system takes control of the vehicle, suffers a technical problem or requests the driver to take over the control of the vehicle.9 

c. 
Ensuring that data relevant to traceability are stored appropriately to avoid degradation or alteration, and retained for durations relevant to the industry. 



3.38 As traceability measures may lead to an accumulation of a large volume of activity data, organisations can consider which of their product features require traceability and which traceability measures might be sufficient for their needs, bearing in mind the resources needed to document the AI model’s decisions, datasets and processes. Organisations could assess this based on several factors, including: 
a. 
Their assessment of the probability and/or severity of harm arising from the use of the AI system; 

b. 
The extent to which the AI model had previously been trialled or used; and 

c. 
The regulatory needs of their industry. 



It should be noted that a black box recorder does not refer to a “black box” in the AI 
model sense (i.e. where the decision-making process of an AI model is inherently difficult 
to interpret and explain). 


Reproducibility 
3.39 While repeatability refers to the internal repetition of results within one’s organisation, reproducibility refers to the ability of an independent verification team to produce the same results using the same AI method based on the documentation made by the organisation. Reproducibility can influence the trustworthiness of the AI product and the organisation deploying the AI model. As implementing reproducibility entails the involvement of external parties, organisations can take a risk-based approach towards identifying the subset of AI-powered features in their products or services that requires external reproducibility testing. 
3.40 The following practices contribute towards reproducibility: 


Auditability 
3.41 Auditability refers to the readiness of an AI system to undergo an assessment of its algorithms, data and design processes. The evaluation of the AI system by internal or external auditors (and the availability of evaluation reports) can contribute to the trustworthiness of the AI system as it demonstrates the responsibility of design and practices and the justifiability of outcomes. It should, however, be noted that auditability does not necessarily entail making information about business models or intellectual property related to the AI system publicly available. 
3.42 Implementing auditability not only entails the involvement of external parties but requires disclosure of commercially sensitive information to the auditors, who may be external. Organisations can take a risk-based approach towards identifying the subset of AI-powered features in their products or services for which implementing auditability is necessary, or where implementing auditability is necessary for an organisation to align itself with regulatory requirements or industry practice. 
3.43 To facilitate auditability, organisations can consider keeping a comprehensive record of data provenance, procurement, pre­processing, lineage, storage and security. The record could also include qualitative input about data representations, data sufficiency, source integrity, data timelines, data relevance, and unforeseen data issues encountered across the workflow. 
3.44 Organisations may also wish to centralise such information digitally in a process log. This would enable the organisation to make available, in one place, information that may assist in demonstrating to concerned parties and affected decision subjects both the responsibility of design and practices and the justifiability of the outcomes of your system’s processing behaviour. Such a log would also enable better organisation of the accessibility and presentation of information yielded, assist in the curation and protection of data that should be kept unavailable from public view, and increase the organisation’s capacity to cater the presentation of results to different tiers of stakeholders with different interests and levels of expertise. 







STAKEHOLDER INTERACTION AND COMMUNICATION 
3.45 This section is intended to help organisations take appropriate steps to build trust in the stakeholder relationship strategies when deploying AI. 
General disclosure 
3.46 Organisations are encouraged to provide general information on whether AI is used in their products and/or services. Where appropriate, this could include information on what AI is, how AI is used in decision-making in relation to consumers, what are its benefits, why your organisation has decided to use AI, how your organisation has taken steps to mitigate risks, and the role and extent that AI plays in the decision-making process. For example, an online portal may inform its users that they are interacting with an AI-powered chatbot and not a human customer service agent. 
3.47 Organisations can consider disclosing the manner in which an AI decision may affect an individual consumer, and whether the decision is reversible. For example, an organisation may inform the individuals that their credit ratings may lead to a loan refusal not only from this organisation but also from other similar organisations, while also informing them that such a decision is reversible if individuals can provide more evidence on their credit worthiness. 
Policy for explanation 
3.48 Organisations are encouraged to develop a policy on what explanations to provide to individuals and when to provide them. Such policies help ensure consistency in communication, and clearly sets out roles and responsibilities of different members of your organisation. These can include explanations on how AI works in an AI-augmented decision-making process, how a specific decision was made and the reasons behind that decision, and the impact and consequence of the decision. The explanation can be provided as part of general communication. It can also be information in respect of a specific decision upon request. In this regard, the principle of equivalence can provide some guidance such that the same standards of disclosure for human-driven decisions is applied to decisions that have been made or augmented by an AI system. 


Bringing explainability and transparency together in a meaningful way 
3.49 Appropriate interaction and communication inspire trust and 
confidence as they build and maintain open relationships between 
organisations and individuals (including employees). Stakeholder relationship strategies should also not remain static. Companies are encouraged to test, evaluate and review their strategies for effectiveness. Further, the extent and mode of implementation of these factors could vary from scenario to scenario. 

3.50 As different stakeholders have different information needs, an organisation can start by first identifying its audience (i.e. its external and internal stakeholders). An organisation’s external stakeholders may include consumers, regulators, other organisations it does business with, and society at large. Its internal stakeholders may include the organisation’s board, management and employees. An organisation can also consider the purpose and the context of the interaction with its stakeholders. For the purposes of illustration, this Model Framework provides considerations for interacting with consumers and other organisations. 

As different stakeholders have 
different information needs, an 
organisation can start by first 

identifying its audience and considering the purpose and the context of the interaction. 


Interacting with consumers 
3.51 Organisations are encouraged to consider the information needs of consumers as they go through the journey of interacting with AI, from considering whether to use an AI solution, to understanding how the AI solution works as they use it, to requesting for reviews on the decisions made by the AI solution. A typical consumer journey may entail meeting the following information needs of consumers: 




Option to opt-out 
3.52 Organisations may wish to consider carefully when deciding whether to provide individuals with the option to opt out from the use of the AI product or service, and whether this option should be offered by default or only upon request. Relevant considerations include: 
a. 
Degree of risk/harm to the individuals; 

b. 
Reversibility of the decision made; 

c. 
Availability of alternative decision-making mechanisms; 

d. 
Cost or trade-offs of alternative mechanisms; 

e. 
Complexity and inefficiency of maintaining parallel systems; and 

f. 
Technical feasibility. 



3.53 Where an organisation has weighed the factors above and decided not to provide an option to opt out, it is prudent for the organisation to consider providing modes of recourse to the consumer such as providing a channel for reviewing the decision. Where appropriate, organisations may also wish to keep a history of chatbot conversations when facing complaints or seeking recourse from consumers. 
Communication channels 
3.54 Organisations are encouraged to put in place the following communications channels for their customers: 


This channel could be used for 
customers to raise feedback or raise queries. It could be managed by an organisation’s Data Protection Officer (“DPO”) if this is appropriate. Where customers find inaccuracies in their personal data which has 
been used for decisions affecting them, this channel can also allow them to correct their data. Such correction and feedback, in turn, maintain data veracity. It could also be managed by an organisation’s Quality Service Manager (“QSM”) if stakeholders wish to raise feedback and queries on material inferences made about them. 
Apart from existing review obligations, organisations can consider providing an avenue 
for individuals (such as an aggrieved consumer) to request a review of material AI decisions that have affected them. Where the effect of a fully-autonomous decision on a consumer may be material, it would be reasonable to provide an opportunity for the decision to be reviewed by a human. 

Testing the user interface 
3.55 Organisations are encouraged to test user interfaces and address usability problems before deployment, so that the user interface serves its intended purposes. If applicable, organisations are also encouraged to inform individuals that their responses would be used to train the AI system (e.g. a chatbot). Organisations should be aware of the risks of using such responses as some individuals may intentionally use “bad language” or “random replies” which would affect the training of the AI system. 
Easy-to-understand communications 
3.56 Organisations are encouraged to communicate in an easy-to­understand manner to increase transparency. There are existing tools to measure readability, such as the Fry readability graph, the Gunning Fog Index, the Flesch-Kincaid readability tests, etc. It would be helpful for decisions with higher impact to be communicated in an easy-to-understand manner, with the need to be transparent about the technology being used. Besides textual communications, organisations can also consider using visualisation tools, graphical representations, summary tables, or a combination of these. The priority is to convey your information, such as an explanation or interpretation, in a way that is understandable by an organisation’s consumers and other stakeholders. 


Acceptable user policies 
3.57 In certain cases, organisations may be implementing AI-powered solutions that are also trained on real-life input data (i.e. active learning). These organisations may wish to consider setting out certain acceptable user policies (“AUPs”) to ensure that users do not maliciously introduce input data that unacceptably manipulates the performance and/or results of the solution’s model. This is pertinent, given past examples of AI chatbot systems that have been unduly manipulated to issue publicly-unacceptable responses. 
3.58 In this regard, AUPs serve to set broad boundaries for the interactions that individuals can perform with the AI system, such as restrictions with regard to intentional actions or attempts to reverse engineer, disable, interfere or disrupt the functionality, integrity or performance of the AI-powered service. 
Interacting with other organisations 
3.59 Some of the approaches and methodologies described in the preceding section are also relevant when organisations interact with AI solution providers (such as procuring AI solutions and obtaining regulatory approval), or other organisations (such as facilitating industry collaboration, enabling interoperability of systems). Organisations would thus need to obtain sufficient information from AI solution providers to help them meet their business objectives (for example, this could be a back-to-back arrangement for providing the information described in paragraph 3.51). This could be as straightforward as obtaining the AI solution providers’ support to provide the information10 and to build the features11 necessary such that the deploying organisation can align itself to the Model Framework. 
10 For example, information related to lineage of the training dataset and documenting the key steps in the model training and selection process. 
11 For example, the user-facing interactions providing information about the expected behavior of an AI-powered feature and building the function to allow users to manage preference 
settings that influence how the AI-powered feature will perform for them in future. 
3.60 Organisations may have to consider the level of support and detailed information that they may need to obtain from AI solution providers pertaining to: 
a. 
Data (e.g. types and range of data used in training the algorithm, source and quality of external training data); 

b. 
Model training and selection (e.g. features and variables used and weights of the commercial models supplied, documenting the key decisions made with respect to the model training and selection); 

c. 
Human elements (e.g. nature of human involvement in developing the algorithm, or in the decision-making process); 

d. 
Inferences (e.g. predictions made by the algorithm and how these are incorporated into product features or decision-making); 

e. 
Algorithmic presence (e.g. where in the solution that an algorithm is used); and 

f. 
Measures and safeguards in place to mitigate biases in data and algorithms. 



3.61 Depending on the purpose and context, the type and level of detail of information required may be different. For example, a regulator may require a regulated entity to demonstrate that its model development and selection process is sufficiently rigorous, and the AI solution provider may be required to provide more information and be involved in the clarification process with the regulator. An industry collaborator, on the hand, may be more concerned with factors pertaining to compatibility and interoperability. 
Ethical evaluation 
3.62 Finally, as ethical standards governing the development and use of AI evolve, organisations are encouraged to evaluate whether their AI governance practices and processes are in line with evolving AI standards, and make available the outcome of such evaluations to relevant stakeholders. 











CONCLUSION 
3.63 This Model Framework is by no means complete or exhaustive and remains a document open to feedback. As AI technologies evolve, so would the related ethical and governance issues. It is the PDPC’s aim to update this Model Framework periodically with the feedback received, to ensure that it remains relevant and useful to organisations deploying AI solutions. 

Appropriate communication regarding the use of AI inspires trust as it builds and maintains open relationships between organisations and individuals. 





ANNEX A 

FOR REFERENCE: A COMPILATION OF EXISTING AI ETHICAL PRINCIPLES 
This annex comprises a collection of foundational AI ethical principles, distilled from various sources.12 Not all are included or addressed in the Model Framework. Organisations may consider incorporating these principles into their own corporate principles, where relevant and desired. 
1. 
Accountability: Ensure that AI actors are responsible and accountable for the proper functioning of AI systems and for the respect of AI ethics and principles, based on their roles, the context, and consistency with the state of art. 

2. 
Accuracy: Identify, log, and articulate sources of error and uncertainty throughout the algorithm and its data sources so that expected and worst-case implications can be understood and can inform mitigation procedures. 

3. 
Auditability: Enable interested third parties to probe, understand, and review the behaviour of the algorithm through disclosure of information that enables monitoring, checking or criticism. 

4. 
Explainability: Ensure that automated and algorithmic decisions and any associated data driving those decisions can be explained to end-users and other stakeholders in non-technical terms. 

5. 
Fairness: 


a. 
Ensure that algorithmic decisions do not create discriminatory or unjust impacts across different demographic lines (e.g. race, sex, etc.). 

b. 
To  develop and include monitoring and accounting mechanisms to avoid unintentional discrimination when implementing decision-making systems. 

c. 
To consult a diversity of voices and demographics when developing systems, applications and algorithms. 




12 These include the Institute of Electrical and Electronics Engineers (“IEEE”) Standards Association’s Ethically Aligned Design (https://standards.ieee.org/industry-connections/ec/ead-v1.html), Software and Information Industry Association’s 
Ethical Principles for Artificial Intelligence and Data Analytics (https://www.siia.net/Portals/0/pdf/Policy/Ethical%20 Principles%20for%20Artificial%20Intelligence%20and%20Data%20Analytics%20SIIA%20Issue%20Brief. 
pdf?ver=2017-11-06-160346-990) and Fairness, Accountability and Transparency in Machine Learning’s Principles for Accountable Algorithms and a Social Impact Statement for Algorithms (http://www.fatml.org/resources/principles-for­accountable-algorithms). There is also the European Commission’s Communication From The Commission To The European Parliament, The Council, The European Economic And Social Committee And The Committee Of The Regions - Building 
Trust in Human-Centric Artificial Intelligence (https://ec.europa.eu/newsroom/dae/document.cfm?doc_id=58496), and the OECD’s Recommendation of the Council on Artificial Intelligence (https://legalinstruments.oecd.org/en/instruments/ 
OECD-LEGAL-0449). They also include principles raised through consultation feedback from the industry. 
6. Human Centricity and Well-being: 
a. 
To aim for an equitable distribution of the benefits of data practices and avoid data practices that disproportionately disadvantage vulnerable groups. 

b. 
To aim to create the greatest possible benefit from the use of data and advanced modelling techniques. 

c. 
Engage in data practices that encourage the practice of virtues that contribute to 


human flourishing, human dignity and human autonomy. 
d. 
To give weight to the considered judgements of people or communities affected by data practices and to be aligned with the values and ethical principles of the people or communities affected. 

e. 
To make decisions that should cause no foreseeable harm to the individual, or should at least minimise such harm (in necessary circumstances, when weighed against the greater good). 

f. 
To allow users to maintain control over the data being used, the context such data is being used in and the ability to modify that use and context. 

g. 
To ensure that the overall well-being of the user should be central to the AI system’s functionality. 


7. 
Human rights alignment: Ensure that the design, development and implementation of technologies do not infringe internationally recognised human rights. 

8. 
Inclusivity: Ensure that AI is accessible to all. 

9. 
Progressiveness: Favour implementations where the value created is materially better than not engaging in that project. 




10.  Responsibility, accountability and transparency: 
a. 
Build trust by ensuring that designers and operators are responsible and accountable for their systems, applications and algorithms, and to ensure that such systems, applications and algorithms operate in a transparent and fair manner. 

b. 
To make available externally visible and impartial avenues of redress for adverse individual or societal effects of an algorithmic decision system, and to designate a 


role to a person or office who is responsible for the timely remedy of such issues. 
c. 
Incorporate downstream measures and processes for users or stakeholders to verify how and when AI technology is being applied. 

d. 
To keep detailed records of design processes and decision-making. 


11. 
Robustness and Security: AI systems should be safe and secure, not vulnerable to tampering or compromising the data they are trained on. 

12. 
Sustainability: Favour implementations that effectively predict future behaviour and 


generate beneficial insights over a reasonable period of time. 


ANNEX B 

ALGORITHM AUDITS 

1. Algorithm audits are conducted if it is necessary to discover the actual operations of algorithms comprised in models. This would have to be carried out at the request of a regulator (as part of a forensic investigation) having jurisdiction over the organisation or by an AI technology provider to assist its customer organisation which has to respond to a regulator’s request. Conducting an algorithm audit requires technical expertise which may require engaging external experts. The audit report may be beyond the understanding of most individuals and organisations. The expense and time required to conduct an algorithm audit should be weighed against the expected benefits obtained from the audit report. Ultimately, algorithm audits should normally be used when it is reasonably clear that such an audit will yield clear benefits for an investigation. 
2. The following factors may be relevant when considering an algorithm audit: 
a. The purpose for conducting an algorithm audit. The Model Framework promotes the provision of information about how AI models function as part of explainable AI. Before embarking on an algorithm audit, it is advisable to consider whether the information that has already been made available to individuals, other organisations 
or businesses, and regulators is sufficient and credible (e.g. product or service descriptions, system technical specifications, model training and selection records, 
data provenance record, audit trail). 
b. Target audience of audit results. This refers to the expertise required of the target audience to effectively understand the data, algorithm and/or models. The information required by different audience vary. When the audience consists of individuals, providing information on the decision-making process and/or how the individuals’ data is used in such processes will achieve the objective of explainable AI more efficaciously. When the audience consists of regulators, information relating to data accountability and the functioning of algorithms should be examined first. An algorithm 
audit can prove how an AI model operates if there is reason to doubt the veracity or completeness of information about its operation. 
c. 
General data accountability. Organisations can provide information on how general data accountability is achieved within the organisations. This includes all the good data practices described in the Model Framework under Data for Model Development section such as maintaining data lineage through keeping a data provenance record, ensuring data accuracy, minimising inherent bias in data, splitting data for different purposes, determining data veracity and reviewing and updating data regularly. 

d. 
Algorithms in AI models can be commercially valuable information that can affect market competitiveness. For example, the algorithm may be a trade secret or may embody business rules that are trade secrets. If a technical audit is contemplated, corresponding mitigation measures should also be considered (e.g. non-disclosure agreements). 






ACKNOWLEDGEMENTS 
The PDPC expresses its sincere appreciation to the following individuals and organisations for their valuable feedback to the Model Framework (in alphabetical order): 
A*STAR Accenture 

AIG Asia Pacific Insurance Pte. Ltd. 
Apple Asia Cloud Computing Association AsiaDPO BSA | The Software Alliance Cambrian AI CUJO.AI Data Synergies DBS Element AI Emerging Technologies Policy Forum Facebook Fountain Court Chambers Google Grab Great Eastern GSK 
IBM Asia Pacific 
LawTech.Asia Mastercard Microsoft Asia MSD International GmBH (Singapore branch) 
Non-Profit Working Group on AI 
OCBC Bank PwC pymetrics Salesforce Singtel Standard Chartered Bank Suade Labs Symphony AyasdiAI Telenor Group Temasek International Tookitaki UCARE.AI Untangle AI 


#SGDIGITAL 

Singapore Digital (SG:D) gives Singapore’s digitalisation efforts a face, identifying our digital programmes and initiatives with one set of visuals, and speaking to our local and international audiences in the same language. 
The SG:D logo is made up of rounded fonts that evolve from the expressive dot that is red. SG stands for Singapore and :D refers to our digital economy. The :D 
smiley face icon also signifies the optimism of Singaporeans 
moving into a digital economy. As we progress into the digital economy, it’s all about the people - empathy and assurance will be at the heart of all that we do. 

www.imda.gov.sg www.pdpc.gov.sg 
Copyright 2020 – Info-communications Media Development Authority (IMDA) and Personal Data Protection Commission (PDPC) 
This publication is intended to foster responsible development and adoption of 
Artificial Intelligence. The contents herein are not intended to be an authoritative 
statement of the law or a substitute for legal or other professional advice. The PDPC 
and its members, officers and employees shall not be responsible for any inaccuracy, 
error or omission in this publication or liable for any damage or loss of any kind as a result of any use of or reliance on this publication. 
The contents of this publication are protected by copyright, trademark or other forms of proprietary rights and may not be reproduced, republished or transmitted in any form or by any means, in whole or in part, without written permission. 



